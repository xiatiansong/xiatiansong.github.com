<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Hadoop集群部署权限总结 - 天松的个人博客</title>
  <meta name="description" content="这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项，包括 Hadoop 各个组件集成 kerberos、openldap 和 sentry 的过程。如果你想了解详细的过程，请参考本博客中其他的文章。">
  <meta name="keywords" content="java, hadoop, hive, hbase, spark, linux, scala, python, mysql">
  <meta name="author" content="天松">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
  <meta name="sogou_site_verification" content="ofwXWFdthV"/>

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" >

  <link rel="canonical" href="http://blog.xiatiansong.com/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop">
  <link rel="stylesheet" href="/static/css/bootstrap.min.css" media="all">
  <link rel="stylesheet" href="/static/css/style.css" media="all">
  <link rel="stylesheet" href="/static/css/pygments.css" media="all">
  <link rel="stylesheet" href="/static/css/font-awesome.css" media="all">

  <!-- atom & rss feed -->
  <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="天松的个人博客 RSS Feed">
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="天松的个人博客 ATOM Feed">
</head>


  <body>
    <div class="container">
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<header class="header">
  <div class="title"><a title="天松的个人博客" href="/">天松的个人博客</a></div>
  <ul class="nav navbar-nav navbar-right visible-lg visible-md">
    <li>
    <form id="search-form" class="form-group has-success visible-lg" role="form">
      <input type="text" class="form-control input-sm" placeholder="Search" id="query" style="width: 160px;">
    </form>
    </li>
    <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
    <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
    <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
    <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
    
    <li><a href="https://github.com/xiatiansong" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
    
    
    
    
    
    <li><a href="http://weibo.com/xiaotian120" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
    

    <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
  </ul>
</header>


      <div class="wrapper">
        <div class="row">
          <div id="search-loader" style="display:none;text-align:center">
            <img src="http://xiaotian120.qiniudn.com/images/loading.gif">
          </div>
          <div class="col-md-12">
  <article class="news-item">

      <h2  class="news-item"> Hadoop集群部署权限总结  
        <time class="small">2014.11.25</time>
      </h2>

      <div><p>这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项，包括 Hadoop 各个组件集成 kerberos、openldap 和 sentry 的过程。如果你想了解详细的过程，请参考本博客中其他的文章。</p>

<h1 id="1.-开始之前">1. 开始之前</h1>

<p>hadoop 集群一共有三个节点，每个节点的 ip、hostname、角色如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">192.168.56.121 cdh1 NameNode、kerberos-server、ldap-server、sentry-store
192.168.56.122 cdh2 DataNode、yarn、hive、impala
192.168.56.123 cdh3 DataNode、yarn、hive、impala
</code></pre></div>
<p>一些注意事项：</p>

<ul>
<li>操作系统为 CentOs6.2</li>
<li>Hadoop 版本为 CDH5.2</li>
<li><strong>hostname 请使用小写</strong>，因为 kerberos 中区分大小写，而 hadoop 中会使用 hostname 的小写替换 <code class="prettyprint">_HOST</code>，impala 直接使用 hostname 替换 <code class="prettyprint">_HOST</code>。</li>
<li>开始之前，请确认 hadoop 集群部署安装成功，不管是否配置 HA，请规划好每个节点的角色。我这里为了简单，以三个节点的集群为例做说明，你可以参考本文并结合你的实际情况做调整。</li>
<li>请确认防火墙关闭，以及集群内和 kerberos 以及 ldap 服务器保持<strong>时钟同步</strong>。</li>
<li>cdh1 为管理节点，故需要做好 cdh1 到集群所有节点的<strong>无密码登陆</strong>，包括其本身。</li>
</ul>

<p>集群中每个节点的 hosts 如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat /etc/hosts
127.0.0.1       localhost

192.168.56.121 cdh1
192.168.56.122 cdh2
192.168.56.123 cdh3
</code></pre></div>
<p>为了方便管理集群，使用 cdh1 作为管理节点，并在 /opt/shell 目录编写了几脚本，/opt/shell/cmd.sh 用于批量执行命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat /opt/shell/cmd.sh

<span class="c">#!/bin/sh</span>

<span class="k">for </span>node in 121 122 123<span class="p">;</span><span class="k">do</span>
<span class="k">    </span><span class="nb">echo</span> <span class="s2">&quot;===============&quot;</span>192.168.56.<span class="nv">$node</span><span class="s2">&quot;===============&quot;</span>
    ssh 192.168.56.<span class="nv">$node</span> <span class="nv">$1</span>
<span class="k">done</span>
</code></pre></div>
<p>/opt/shell/cmd.sh 用于批量执行命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat /opt/shell/syn.sh

<span class="c">#!/bin/sh</span>

<span class="k">for </span>node in 121 122 123<span class="p">;</span><span class="k">do</span>
<span class="k">    </span><span class="nb">echo</span> <span class="s2">&quot;===============&quot;</span>192.168.56.<span class="nv">$node</span><span class="s2">&quot;===============&quot;</span>
    scp -r <span class="nv">$1</span> 192.168.56.<span class="nv">$node</span>:<span class="nv">$2</span>
<span class="k">done</span>
</code></pre></div>
<p>/opt/shell/cluster.sh 用于批量维护集群各个服务：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat /opt/shell/cluster.sh
<span class="c">#!/bin/sh</span>
<span class="k">for </span>node in 121 122 123<span class="p">;</span><span class="k">do</span>
<span class="k">    </span><span class="nb">echo</span> <span class="s2">&quot;===============&quot;</span>192.168.56.<span class="nv">$node</span><span class="s2">&quot;===============&quot;</span>
    ssh 192.168.56.<span class="nv">$node</span> <span class="s1">&#39;for src in `ls /etc/init.d|grep &#39;</span><span class="nv">$1</span><span class="s1">&#39;`;do service $src &#39;</span><span class="nv">$2</span><span class="s1">&#39;; done&#39;</span>
<span class="k">done</span>
</code></pre></div>
<h1 id="2.-安装-kerberos">2. 安装 kerberos</h1>

<p>在 cdh1 节点修改 /etc/krb5.conf 如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>logging<span class="o">]</span>
<span class="nv">default</span> <span class="o">=</span> FILE:/var/log/krb5libs.log
<span class="nv">kdc</span> <span class="o">=</span> FILE:/var/log/krb5kdc.log
<span class="nv">admin_server</span> <span class="o">=</span> FILE:/var/log/kadmind.log

<span class="o">[</span>libdefaults<span class="o">]</span>
<span class="nv">default_realm</span> <span class="o">=</span> JAVACHEN.COM
<span class="nv">dns_lookup_realm</span> <span class="o">=</span> <span class="nb">false</span>
<span class="nv">dns_lookup_kdc</span> <span class="o">=</span> <span class="nb">false</span>
<span class="nv">clockskew</span> <span class="o">=</span> 120
<span class="nv">ticket_lifetime</span> <span class="o">=</span> 24h
<span class="nv">renew_lifetime</span> <span class="o">=</span> 7d
<span class="nv">forwardable</span> <span class="o">=</span> <span class="nb">true</span>
<span class="nv">renewable</span> <span class="o">=</span> <span class="nb">true</span>
<span class="nv">udp_preference_limit</span> <span class="o">=</span> 1
<span class="nv">default_tgs_enctypes</span> <span class="o">=</span> arcfour-hmac
<span class="nv">default_tkt_enctypes</span> <span class="o">=</span> arcfour-hmac

<span class="o">[</span>realms<span class="o">]</span>
JAVACHEN.COM <span class="o">=</span> <span class="o">{</span>
 <span class="nv">kdc</span> <span class="o">=</span> cdh1:88
 <span class="nv">admin_server</span> <span class="o">=</span> cdh1:749
<span class="o">}</span>

<span class="o">[</span>domain_realm<span class="o">]</span>
.javachen.com <span class="o">=</span> JAVACHEN.COM
javachen.com <span class="o">=</span> JAVACHEN.COM

<span class="o">[</span>kdc<span class="o">]</span>
<span class="nv">profile</span><span class="o">=</span>/var/kerberos/krb5kdc/kdc.conf
</code></pre></div>
<p>修改/var/kerberos/krb5kdc/kdc.conf 如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>kdcdefaults<span class="o">]</span>
 <span class="nv">v4_mode</span> <span class="o">=</span> nopreauth
 <span class="nv">kdc_ports</span> <span class="o">=</span> 88
 <span class="nv">kdc_tcp_ports</span> <span class="o">=</span> 88

<span class="o">[</span>realms<span class="o">]</span>
 JAVACHEN.COM <span class="o">=</span> <span class="o">{</span>
  <span class="c">#master_key_type = aes256-cts</span>
  <span class="nv">acl_file</span> <span class="o">=</span> /var/kerberos/krb5kdc/kadm5.acl
  <span class="nv">dict_file</span> <span class="o">=</span> /usr/share/dict/words
  <span class="nv">admin_keytab</span> <span class="o">=</span> /var/kerberos/krb5kdc/kadm5.keytab
  <span class="nv">supported_enctypes</span> <span class="o">=</span>  des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal des-cbc-crc:v4 des-cbc-crc:afs3
  <span class="nv">max_life</span> <span class="o">=</span> 24h
  <span class="nv">max_renewable_life</span> <span class="o">=</span> 7d
  <span class="nv">default_principal_flags</span> <span class="o">=</span> +renewable, +forwardable
 <span class="o">}</span>
</code></pre></div>
<p>修改 /var/kerberos/krb5kdc/kadm5.acl  如下：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">*/admin@JAVACHEN.COM  *
</code></pre></div>
<p>将 cdh1 上的 /etc/krb5.conf 同步到集群各个节点上：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sh /opt/shell/syn.sh /etc/krb5.conf /etc/krb5.conf
</code></pre></div>
<p>在 kerberos 服务器节点上使用下面脚本初始化 kerberos：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">yum install krb5-server krb5-libs krb5-auth-dialog krb5-workstation  -y

rm -rf /var/kerberos/krb5kdc/*.keytab /var/kerberos/krb5kdc/prin*

kdb5_util create -r JAVACHEN.COM -s

chkconfig --level 35 krb5kdc on
chkconfig --level 35 kadmin on
service krb5kdc restart
service kadmin restart

<span class="nb">echo</span> -e <span class="s2">&quot;root\nroot&quot;</span> <span class="p">|</span> kadmin.local -q <span class="s2">&quot;addprinc root/admin&quot;</span>

<span class="nv">DNS</span><span class="o">=</span>JAVACHEN.COM
<span class="nv">hostname</span><span class="o">=</span><span class="sb">`</span>hostname -i<span class="sb">`</span>

<span class="c">#读取/etc/host文件中ip为 192.168.56 开头的机器名称并排除自己（kerberos 服务器）</span>
<span class="k">for </span>host in  <span class="sb">`</span>cat /etc/hosts<span class="p">|</span>grep 192.168.56<span class="p">|</span>grep -v <span class="nv">$hostname</span><span class="p">|</span>awk <span class="s1">&#39;{print $2}&#39;</span><span class="sb">`</span> <span class="p">;</span><span class="k">do</span>
<span class="k">    for </span>user in hdfs<span class="p">;</span> <span class="k">do</span>
<span class="k">        </span>kadmin.local -q <span class="s2">&quot;addprinc -randkey $user/$host@$DNS&quot;</span>
        kadmin.local -q <span class="s2">&quot;xst -k /var/kerberos/krb5kdc/$user-un.keytab $user/$host@$DNS&quot;</span>
    <span class="k">done</span>
<span class="k">    for </span>user in HTTP hive yarn mapred impala zookeeper sentry llama zkcli <span class="p">;</span> <span class="k">do</span>
<span class="k">        </span>kadmin.local -q <span class="s2">&quot;addprinc -randkey $user/$host@$DNS&quot;</span>
        kadmin.local -q <span class="s2">&quot;xst -k /var/kerberos/krb5kdc/$user.keytab $user/$host@$DNS&quot;</span>
    <span class="k">done</span>
<span class="k">done</span>

<span class="nb">cd</span> /var/kerberos/krb5kdc/
<span class="nb">echo</span> -e <span class="s2">&quot;rkt hdfs-un.keytab\nrkt HTTP.keytab\nwkt hdfs.keytab&quot;</span> <span class="p">|</span> ktutil

<span class="c">#kerberos 重新初始化之后，还需要添加下面代码用于集成 ldap</span>

kadmin.local -q <span class="s2">&quot;addprinc ldapadmin@JAVACHEN.COM&quot;</span>
kadmin.local -q <span class="s2">&quot;addprinc -randkey ldap/cdh1@JAVACHEN.COM&quot;</span>
kadmin.local -q <span class="s2">&quot;ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM&quot;</span>

/etc/init.d/slapd restart

<span class="c">#测试 ldap 是否可以正常使用</span>
ldapsearch -x -b <span class="s1">&#39;dc=javachen,dc=com&#39;</span>
</code></pre></div>
<p>将其保存为 /root/init_kerberos.sh，然后运行：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sh /root/init_kerberos.sh
</code></pre></div>
<p>将上面生成的 keytab 同步到其他节点并设置权限：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sh /opt/shell/syn.sh /opt/keytab/hdfs.keytab /etc/hadoop/conf/
sh /opt/shell/syn.sh /opt/keytab/mapred.keytab /etc/hadoop/conf/
sh /opt/shell/syn.sh /opt/keytab/yarn.keytab /etc/hadoop/conf/
sh /opt/shell/syn.sh /opt/keytab/hive.keytab /etc/hive/conf/
sh /opt/shell/syn.sh /opt/keytab/impala.keytab /etc/impala/conf/
sh /opt/shell/syn.sh /opt/keytab/zookeeper.keytab /etc/zookeeper/conf/
sh /opt/shell/syn.sh /opt/keytab/zkcli.keytab /etc/zookeeper/conf/
sh /opt/shell/syn.sh /opt/keytab/sentry.keytab /etc/sentry/conf/

sh /opt/shell/cmd.sh <span class="s2">&quot;chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/*.keytab&quot;</span>
sh /opt/shell/cmd.sh <span class="s2">&quot;chown mapred:hadoop /etc/hadoop/conf/mapred.keytab ;chmod 400 /etc/hadoop/conf/*.keytab&quot;</span>
sh /opt/shell/cmd.sh <span class="s2">&quot;chown yarn:hadoop /etc/hadoop/conf/yarn.keytab ;chmod 400 /etc/hadoop/conf/*.keytab&quot;</span>
sh /opt/shell/cmd.sh <span class="s2">&quot;chown hive:hadoop /etc/hive/conf/hive.keytab ;chmod 400 /etc/hive/conf/*.keytab&quot;</span>
sh /opt/shell/cmd.sh <span class="s2">&quot;chown impala:hadoop /etc/impala/conf/impala.keytab ;chmod 400 /etc/impala/conf/*.keytab&quot;</span>
sh /opt/shell/cmd.sh <span class="s2">&quot;chown zookeeper:hadoop /etc/zookeeper/conf/*.keytab ;chmod 400 /etc/zookeeper/conf/*.keytab&quot;</span>

<span class="c"># sentry 只安装在 cdh1 节点</span>
chown sentry:hadoop /etc/sentry/conf/*.keytab <span class="p">;</span>chmod 400 /etc/sentry/conf/*.keytab
</code></pre></div>
<p>在集群中每个节点安装 kerberos 客户端：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sh /opt/shell/cmd.sh <span class="s2">&quot;yum install krb5-devel krb5-workstation -y&quot;</span>
</code></pre></div>
<p>批量获取 root/admin 用户的 ticket</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sh /opt/shell/cmd.sh <span class="s2">&quot;echo root|kinit root/admin&quot;</span>
</code></pre></div>
<h1 id="3.-hadoop-集成-kerberos">3. hadoop 集成 kerberos</h1>

<p>更新每个节点上的 JCE 文件并修改 /etc/default/hadoop-hdfs-datanode，并且修改 hdfs、yarn、mapred、hive 的配置文件。</p>

<p>如果配置了 HA，则先配置 zookeeper 集成 kerberos。</p>

<p>同步配置文件：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sh /opt/shell/syn.sh /etc/hadoop/conf /etc/hadoop
sh /opt/shell/syn.sh /etc/zookeeper/conf /etc/zookeeper

sh /opt/shell/cmd.sh <span class="s2">&quot;cd /etc/hadoop/conf/; chown root:yarn container-executor.cfg ; chmod 400 container-executor.cfg&quot;</span>

sh /opt/shell/syn.sh /etc/hive/conf /etc/hive
</code></pre></div>
<p>接下来就是依次获取每个服务对应的 ticket 并启动对应的服务，我创建了一个脚本 /opt/shell/manager_cluster.sh 来做这件事：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/bash</span>

<span class="nv">role</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">dir</span><span class="o">=</span><span class="nv">$role</span>
<span class="nb">command</span><span class="o">=</span><span class="nv">$2</span>

<span class="k">if</span> <span class="o">[</span> X<span class="s2">&quot;$role&quot;</span> <span class="o">==</span> X<span class="s2">&quot;hdfs&quot;</span> <span class="o">]</span><span class="p">;</span><span class="k">then</span>
<span class="k">    </span><span class="nv">dir</span><span class="o">=</span>hadoop
<span class="k">fi</span>

<span class="k">if</span> <span class="o">[</span> X<span class="s2">&quot;$role&quot;</span> <span class="o">==</span> X<span class="s2">&quot;yarn&quot;</span> <span class="o">]</span><span class="p">;</span><span class="k">then</span>
<span class="k">        </span><span class="nv">dir</span><span class="o">=</span>hadoop
<span class="k">fi</span>

<span class="k">if</span> <span class="o">[</span> X<span class="s2">&quot;$role&quot;</span> <span class="o">==</span> X<span class="s2">&quot;mapred&quot;</span> <span class="o">]</span><span class="p">;</span><span class="k">then</span>
<span class="k">        </span><span class="nv">dir</span><span class="o">=</span>hadoop
<span class="k">fi</span>

<span class="nb">echo</span> <span class="nv">$dir</span> <span class="nv">$role</span> <span class="nv">$command</span>
<span class="k">for </span>node in 121 122 123 <span class="p">;</span><span class="k">do</span>
<span class="k">    </span><span class="nb">echo</span> <span class="s2">&quot;========192.168.56.$node========&quot;</span>
    ssh 192.168.56.<span class="nv">$node</span> <span class="s1">&#39;</span>
<span class="s1">        host=`hostname -f| tr &quot;[:upper:]&quot; &quot;[:lower:]&quot;`</span>
<span class="s1">        path=&quot;&#39;</span><span class="nv">$role</span><span class="s1">&#39;/$host&quot;</span>
<span class="s1">        #echo $path</span>
<span class="s1">        principal=`klist -k /etc/&#39;</span><span class="nv">$dir</span><span class="s1">&#39;/conf/&#39;</span><span class="nv">$role</span><span class="s1">&#39;.keytab | grep $path | head -n1 | cut -d &quot; &quot; -f5`</span>
<span class="s1">        echo $principal</span>
<span class="s1">        if [ X&quot;$principal&quot; == X ]; then</span>
<span class="s1">            principal=`klist -k /etc/&#39;</span><span class="nv">$dir</span><span class="s1">&#39;/conf/&#39;</span><span class="nv">$role</span><span class="s1">&#39;.keytab | grep $path | head -n1 | cut -d &quot; &quot; -f4`</span>
<span class="s1">            echo $principal</span>
<span class="s1">            if [ X&quot;$principal&quot; == X ]; then</span>
<span class="s1">                    echo &quot;Failed to get hdfs Kerberos principal&quot;</span>
<span class="s1">                    exit 1</span>
<span class="s1">            fi</span>
<span class="s1">        fi</span>
<span class="s1">        kinit -kt /etc/&#39;</span><span class="nv">$dir</span><span class="s1">&#39;/conf/&#39;</span><span class="nv">$role</span><span class="s1">&#39;.keytab $principal</span>
<span class="s1">        if [ $? -ne 0 ]; then</span>
<span class="s1">                echo &quot;Failed to login as hdfs by kinit command&quot;</span>
<span class="s1">                exit 1</span>
<span class="s1">        fi</span>
<span class="s1">        kinit -R</span>
<span class="s1">        for src in `ls /etc/init.d|grep &#39;</span><span class="nv">$role</span><span class="s1">&#39;`;do service $src &#39;</span><span class="nv">$command</span><span class="s1">&#39;; done</span>

<span class="s1">    &#39;</span>
<span class="k">done</span>
</code></pre></div>
<p>启动命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 启动 zookeeper</span>
sh /opt/shell/manager_cluster.sh zookeeper restart

<span class="c"># 获取 hdfs 服务的 ticket</span>
sh /opt/shell/manager_cluster.sh hdfs status

<span class="c"># 使用普通脚本依次启动 hadoop-hdfs-zkfc、hadoop-hdfs-journalnode、hadoop-hdfs-namenode、hadoop-hdfs-datanode</span>
sh /opt/shell/cluster.sh hadoop-hdfs-zkfc restart
sh /opt/shell/cluster.sh hadoop-hdfs-journalnode restart
sh /opt/shell/cluster.sh hadoop-hdfs-namenode restart
sh /opt/shell/cluster.sh hadoop-hdfs-datanode restart

sh /opt/shell/manager_cluster.sh yarn restart
sh /opt/shell/manager_cluster.sh mapred restart

sh /opt/shell/manager_cluster.sh hive restart
</code></pre></div>
<p>修改 impala 配置文件并同步到其他节点，然后启动 impala 服务：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="se">\c</span>p /etc/hadoop/conf/core-site.xml /etc/impala/conf/
<span class="se">\c</span>p /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf/
<span class="se">\c</span>p /etc/hive/conf/hive-site.xml /etc/impala/conf/

sh /opt/shell/syn.sh /etc/impala/conf /etc/impala/
sh /opt/shell/syn.sh /etc/default/impala /etc/default/impala
sh /opt/shell/manager_cluster.sh impala restart
</code></pre></div>
<p>到此，集群应该启动成功了。</p>

<h1 id="3.-安装-ldap">3. 安装 ldap</h1>

<p>使用下面命令在 cdh1 节点快速安装 ldap-server：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">yum install db4 db4-utils db4-devel cyrus-sasl* krb5-server-ldap -y
yum install openldap openldap-servers openldap-clients openldap-devel compat-openldap -y

<span class="c"># 更新配置库：</span>
rm -rf /var/lib/ldap/*
cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG
chown -R ldap.ldap /var/lib/ldap

<span class="c"># 备份原来的 slapd-conf</span>
cp -rf /etc/openldap/slapd.d /etc/openldap/slapd.d.bak

cp /usr/share/doc/krb5-server-ldap-1.10.3/kerberos.schema /etc/openldap/schema/
touch /etc/openldap/slapd.conf

<span class="nb">echo</span> <span class="s2">&quot;include /etc/openldap/schema/corba.schema</span>
<span class="s2">include /etc/openldap/schema/core.schema</span>
<span class="s2">include /etc/openldap/schema/cosine.schema</span>
<span class="s2">include /etc/openldap/schema/duaconf.schema</span>
<span class="s2">include /etc/openldap/schema/dyngroup.schema</span>
<span class="s2">include /etc/openldap/schema/inetorgperson.schema</span>
<span class="s2">include /etc/openldap/schema/java.schema</span>
<span class="s2">include /etc/openldap/schema/misc.schema</span>
<span class="s2">include /etc/openldap/schema/nis.schema</span>
<span class="s2">include /etc/openldap/schema/openldap.schema</span>
<span class="s2">include /etc/openldap/schema/ppolicy.schema</span>
<span class="s2">include /etc/openldap/schema/collective.schema</span>
<span class="s2">include /etc/openldap/schema/kerberos.schema&quot;</span> &gt; /etc/openldap/slapd.conf

<span class="nb">echo</span> -e <span class="s2">&quot;pidfile /var/run/openldap/slapd.pid\nargsfile /var/run/openldap/slapd.args&quot;</span> &gt;&gt; /etc/openldap/slapd.conf
slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d
chown -R ldap:ldap /etc/openldap/slapd.d <span class="o">&amp;&amp;</span> chmod -R 700 /etc/openldap/slapd.d

<span class="c">#重启服务</span>
chkconfig --add slapd
chkconfig --level 345 slapd on

/etc/init.d/slapd restart
</code></pre></div>
<p>集成 kerberos：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 创建管理员用户</span>
kadmin.local -q <span class="s2">&quot;addprinc ldapadmin@JAVACHEN.COM&quot;</span>
kadmin.local -q <span class="s2">&quot;addprinc -randkey ldap/cdh1@JAVACHEN.COM&quot;</span>

rm -rf /etc/openldap/ldap.keytab
kadmin.local -q <span class="s2">&quot;ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM&quot;</span>

chown -R ldap:ldap /etc/openldap/ldap.keytab
/etc/init.d/slapd restart
</code></pre></div>
<p>创建 modify.ldif 文件用于更新数据库：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">dn: olcDatabase={2}bdb,cn=config
changetype: modify
replace: olcSuffix
olcSuffix: dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
replace: olcRootDN
# Temporary lines to allow initial setup
olcRootDN: uid=ldapadmin,ou=people,dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
add: olcRootPW
olcRootPW: secret

dn: cn=config
changetype: modify
add: olcAuthzRegexp
olcAuthzRegexp: uid=([^,]*),cn=GSSAPI,cn=auth uid=$1,ou=people,dc=javachen,dc=com

dn: olcDatabase={2}bdb,cn=config
changetype: modify
add: olcAccess
# Everyone can read everything
olcAccess: {0}to dn.base=&quot;&quot; by * read
# The ldapadm dn has full write access
olcAccess: {1}to * by dn=&quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; write by * read
</code></pre></div>
<p>运行下面命令更新数据库：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">ldapmodify -Y EXTERNAL -H ldapi:/// -f modify.ldif
</code></pre></div>
<p>添加用户和组，创建 setup.ldif 如下：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">dn: dc=javachen,dc=com
objectClass: top
objectClass: dcObject
objectclass: organization
o: javachen com
dc: javachen

dn: ou=people,dc=javachen,dc=com
objectclass: organizationalUnit
ou: people
description: Users

dn: ou=group,dc=javachen,dc=com
objectClass: organizationalUnit
ou: group

dn: uid=ldapadmin,ou=people,dc=javachen,dc=com
objectClass: inetOrgPerson
objectClass: posixAccount
objectClass: shadowAccount
cn: LDAP admin account
uid: ldapadmin
sn: ldapadmin
uidNumber: 1001
gidNumber: 100
homeDirectory: /home/ldap
loginShell: /bin/bash
</code></pre></div>
<p>运行下面命令导入到数据库：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">ldapadd -x -D <span class="s2">&quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot;</span> -w secret -f setup.ldif
</code></pre></div>
<p>接下来，可以在 ldap 服务器上创建一些本地系统用户，然后将这些用户导入到 ldap 服务中。</p>

<p>先安装 migrationtools 然后修改 /usr/share/migrationtools/migrate_common.ph 文件中的 defalut DNS domain 和 defalut base。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># 创建 admin 组</span>
groupadd admin

<span class="c"># 创建 test 和 hive 用户，用于后面测试 sentry</span>
useradd <span class="nb">test </span>hive
usermod -G admin <span class="nb">test</span>
usermod -G admin hive

<span class="c"># 将关键用户导入到 ldap</span>
grep -E <span class="s2">&quot;bi_|hive|test&quot;</span> /etc/passwd  &gt;/opt/passwd.txt
/usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldif
ldapadd -x -D <span class="s2">&quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot;</span> -w secret -f /opt/passwd.ldif

<span class="c"># 将 admin 组导入到 ldap</span>
grep -E <span class="s2">&quot;admin&quot;</span> /etc/group  &gt;/opt/group.txt
/usr/share/migrationtools/migrate_group.pl /opt/group.txt /opt/group.ldif
ldapadd -x -D <span class="s2">&quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot;</span> -w secret -f /opt/group.ldif
</code></pre></div>
<p>然后，你可以依次为每个用户设置密码，使用下面命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">ldappasswd -x -D <span class="s1">&#39;uid=ldapadmin,ou=people,dc=javachen,dc=com&#39;</span> -w secret <span class="s2">&quot;uid=hive,ou=people,dc=javachen,dc=com&quot;</span> -S
</code></pre></div>
<p>另外，这些用户和组都是存在于 ldap 服务器上的，需要将其远程挂载到 hadoop 的每个节点上，否则，你需要在每个节点创建对应的用户和组（目前，测试是这样的）。</p>

<h1 id="4.-集成-sentry">4. 集成 sentry</h1>

<p>这部分建议使用数据库的方式存储规则，不建议生产环境使用文件保存方式。</p>

<p>详细的配置，请参考 <a href="/2014/11/14/config-impala-and-hive-with-sentry/">Impala和Hive集成Sentry</a></p>

<p>通过 beeline 使用 <code class="prettyprint">hive/cdh1@JAVACHEN.COM</code> 连接 hive-server2 创建一些角色和组：</p>
<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">create</span> <span class="k">role</span> <span class="n">admin_role</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">ALL</span> <span class="k">ON</span> <span class="n">SERVER</span> <span class="n">server1</span> <span class="k">TO</span> <span class="k">ROLE</span> <span class="n">admin_role</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">ROLE</span> <span class="n">admin_role</span> <span class="k">TO</span> <span class="k">GROUP</span> <span class="k">admin</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">ROLE</span> <span class="n">admin_role</span> <span class="k">TO</span> <span class="k">GROUP</span> <span class="n">hive</span><span class="p">;</span>

<span class="k">create</span> <span class="k">role</span> <span class="n">test_role</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">ALL</span> <span class="k">ON</span> <span class="k">DATABASE</span> <span class="n">testdb</span> <span class="k">TO</span> <span class="k">ROLE</span> <span class="n">test_role</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">ALL</span> <span class="k">ON</span> <span class="k">DATABASE</span> <span class="k">default</span> <span class="k">TO</span> <span class="k">ROLE</span> <span class="n">test_role</span><span class="p">;</span>
<span class="k">GRANT</span> <span class="k">ROLE</span> <span class="n">test_role</span> <span class="k">TO</span> <span class="k">GROUP</span> <span class="n">test</span><span class="p">;</span>
</code></pre></div>
<p>上面 amdin 和 hive 组具有所有数据库的管理员权限，而 test 组只有 testdb 和 default 库的读写权限。</p>

<p>在 impala-shell 中通过 ldap 的方式传入不同的用户，可以测试读写权限。</p>

<h1 id="5.-如何添加新用户并设置权限？">5. 如何添加新用户并设置权限？</h1>

<p>TODO</p>

<p>Enjoy it !</p>
</div>

      <!--
      <div id="pay" style="text-align:center;">
        ----EOF-----
        <br>
        <section>
  <h4>Sponsor</h4>
	<img src="http://xiaotian120.qiniudn.com/images/alipay.png" width="150/">
  <p class="small">觉得此博客对你有帮助，支付宝扫码赞助吧</p>
</section>

      </div>
      -->
      <p class="meta">
      	
            Categories:
      	    
          	<a class="btn btn-default btn-xs" href="/categories.html#hadoop">hadoop</a>
          
      	

      	
            Tags:
      	    
          	<a class="btn btn-default btn-xs" href="/tags.html#hadoop">hadoop</a>
          
          	<a class="btn btn-default btn-xs" href="/tags.html#kerberos">kerberos</a>
          
          	<a class="btn btn-default btn-xs" href="/tags.html#ldap">ldap</a>
          
          	<a class="btn btn-default btn-xs" href="/tags.html#sentry">sentry</a>
          
      	
      </p>
	</article>

	<ul class="pager">
	  
	  <li class="previous"><a class="btn btn-xs" href="/2014/11/24/spring-with-jpa2" title="Spring集成JPA2.0">&larr; Prev</a></li>
	  
	  
	  <li class="next"><a class="btn btn-xs" href="/2014/12/02/some-usages-of-jpa" title="JPA的使用">Next &rarr;</a></li>
	  
	</ul>

  
<div id="comments">
  <div class="ds-thread" data-thread-key="/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop"  data-title="Hadoop集群部署权限总结 - 天松的个人博客"></div>
</div>



</div>

        </div>
      </div>

      <footer class="footer text-center">
  <p>&copy; 2009-2015 <a href="/" target="_blank" title="86后，工作在深圳；Java程序员、Hadoop工程师；主要关注Java、Scala、Hadoop、Kettle、关注大数据处理技术。">天松</a>. With Help from <a href="//jekyllrb.com/" target="_blank" title="Transform your plain text into static websites and blogs.">Jekyll</a> and <a href="//getbootstrap.com/" target="_blank" title="Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.">Bootstrap</a>, theme from <a href="http://havee.me/" target="_blank" title="http://havee.me/">Havee</a>.

  <span style="float:right;"><a href="/about.html">About</a></span>

	
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




  </p>
  <div id="toTop" style="display: block;">
    <a href="#">▲</a><a href="#footer">▼</a>
  </div>
</footer>

    </div>

    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/core.js"></script>

    
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




    
    <!-- duoshuo Begin -->
    <script type="text/javascript">
      var duoshuoQuery = {short_name:"sunshine1987"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script>
    <!-- duoshuo End -->
    
  </body>
</html>
