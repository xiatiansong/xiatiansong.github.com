<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Sqoop导入关系数据库到Hive - 天松的个人博客</title>
  <meta name="description" content="Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。">
  <meta name="keywords" content="Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。">
  <meta name="author" content="天松">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
  <meta name="sogou_site_verification" content="ofwXWFdthV"/>

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" >

  <link rel="canonical" href="http://blog.xiatiansong.com/2014/08/04/import-data-to-hive-with-sqoop">
  <link rel="stylesheet" href="/static/css/bootstrap.min.css" media="all">
  <link rel="stylesheet" href="/static/css/style.css" media="all">
  <link rel="stylesheet" href="/static/css/pygments.css" media="all">
  <link rel="stylesheet" href="/static/css/font-awesome.css" media="all">

  <!-- atom & rss feed -->
  <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="天松的个人博客 RSS Feed">
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="天松的个人博客 ATOM Feed">
</head>


  <body>
    <div class="container">
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<header class="header">
  <div class="title"><a title="天松的个人博客" href="/">天松的个人博客</a></div>
  <ul class="nav navbar-nav navbar-right visible-lg visible-md">
    <li>
    <form id="search-form" class="form-group has-success visible-lg" role="form">
      <input type="text" class="form-control input-sm" placeholder="Search" id="query" style="width: 160px;">
    </form>
    </li>
    <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
    <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
    <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
    <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
    
    <li><a href="https://github.com/xiatiansong" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
    
    
    
    
    
    <li><a href="http://weibo.com/xiaotian120" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
    

    <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
  </ul>
</header>


      <div class="wrapper">
        <div class="row">
          <div id="search-loader" style="display:none;text-align:center">
            <img src="http://javachen-rs.qiniudn.com/images/loading.gif">
          </div>
          <div class="col-md-12">
  <article class="news-item">

      <h2  class="news-item"> Sqoop导入关系数据库到Hive  
        <time class="small">2014.08.04</time>
      </h2>

      <div><p>Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。</p>

<h1 id="1.-安装-sqoop">1. 安装 Sqoop</h1>

<p>使用 rpm 安装即可。</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">yum install sqoop sqoop-metastore -y
</code></pre></div>
<blockquote>
<p>安装完之后需要下载 mysql jar 包到 sqoop 的 lib 目录。</p>
</blockquote>

<p>这里使用 hive 的 metastore 的 mysql 数据库作为关系数据库，以 TBLS 表为例，该表结构和数据如下：</p>
<div class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="n">mysql</span><span class="o">&gt;</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">TBLS</span> <span class="k">limit</span> <span class="mi">3</span><span class="p">;</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+</span>
<span class="o">|</span><span class="n">TBL_ID</span><span class="o">|</span><span class="n">CREATE_TIME</span><span class="o">|</span><span class="n">DB_ID</span><span class="o">|</span><span class="n">LAST_ACCESS_TIME</span><span class="o">|</span><span class="k">OWNER</span><span class="o">|</span><span class="n">RETENTI</span> <span class="o">|</span> <span class="n">SD_ID</span><span class="o">|</span> <span class="n">TBL_NAME</span><span class="o">|</span> <span class="n">TBL_TYPE</span>       <span class="o">|</span><span class="n">VIEW_EXPANDED_TEXT</span><span class="o">|</span> <span class="n">VIEW_ORIGINAL_TEXT</span><span class="o">|</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+</span>
<span class="o">|</span>    <span class="mi">34</span><span class="o">|</span><span class="mi">1406784308</span> <span class="o">|</span>    <span class="mi">8</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">45</span><span class="o">|</span> <span class="n">test1</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">|</span>    <span class="mi">40</span><span class="o">|</span><span class="mi">1406797005</span> <span class="o">|</span>    <span class="mi">9</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">52</span><span class="o">|</span> <span class="n">test2</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">|</span>    <span class="mi">42</span><span class="o">|</span><span class="mi">1407122307</span> <span class="o">|</span>    <span class="mi">7</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">59</span><span class="o">|</span> <span class="n">test3</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+</span>
</code></pre></div>
<h1 id="2.-使用">2. 使用</h1>

<h2 id="2.1-命令说明">2.1 命令说明</h2>

<p>查看 sqoop 命令说明：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop <span class="nb">help</span>
usage: sqoop COMMAND <span class="o">[</span>ARGS<span class="o">]</span>

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  <span class="nb">eval               </span>Evaluate a SQL statement and display the results
  <span class="nb">export             </span>Export an HDFS directory to a database table
  <span class="nb">help               </span>List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  version            Display version information

See <span class="s1">&#39;sqoop help COMMAND&#39;</span> <span class="k">for </span>information on a specific command.
</code></pre></div>
<p>你也可以查看某一个命令的使用说明：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --help
<span class="nv">$ </span>sqoop <span class="nb">help </span>import
</code></pre></div>
<p>你也可以使用别名来代替 <code class="prettyprint">sqoop (toolname)</code>：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop-import
</code></pre></div>
<p>sqoop import 的一个示例如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS
</code></pre></div>
<p>你还可以使用 <code class="prettyprint">--options-file</code> 来传入一个文件，使用这种方式可以重用一些配置参数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop --options-file /users/homer/work/import.txt --table TEST
</code></pre></div>
<p>/users/homer/work/import.txt 文件内容如下：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">import
--connect
jdbc:mysql://192.168.56.121:3306/metastore
--username
hiveuser
--password 
redhat
</code></pre></div>
<h1 id="2.2-导入数据到-hdfs">2.2 导入数据到 hdfs</h1>

<p>使用 sqoop-import 命令可以从关系数据库导入数据到 hdfs。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --target-dir /user/hive/result
</code></pre></div>
<p>注意：</p>

<ul>
<li>mysql jdbc url 请使用 ip 地址</li>
<li>如果重复执行，会提示目录已经存在，可以手动删除</li>
<li>如果不指定 <code class="prettyprint">--target-dir</code>，导入到用户家目录下的 TBLS 目录</li>
</ul>

<p>你还可以指定其他的参数：</p>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--append</code></td>
<td style="text-align: left">将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--as-avrodatafile</code></td>
<td style="text-align: left">将数据导入到一个Avro数据文件中</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--as-sequencefile</code></td>
<td style="text-align: left">将数据导入到一个sequence文件中</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--as-textfile</code></td>
<td style="text-align: left">将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--boundary-query &lt;statement&gt;</code></td>
<td style="text-align: left">边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：<code class="prettyprint">--boundary-query &#39;select id,no from t where id = 3&#39;</code>，表示导入的数据为id=3的记录，或者 <code class="prettyprint">select min(&lt;split-by&gt;), max(&lt;split-by&gt;) from &lt;table name&gt;</code>，注意查询的字段中不能有数据类型为字符串的字段，否则会报错</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--columns&lt;col,col&gt;</code></td>
<td style="text-align: left">指定要导入的字段值，格式如：<code class="prettyprint">--columns id,username</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--direct</code></td>
<td style="text-align: left">直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--direct-split-size</code></td>
<td style="text-align: left">在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--inline-lob-limit</code></td>
<td style="text-align: left">设定大对象数据类型的最大值</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-m,--num-mappers</code></td>
<td style="text-align: left">启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--query，-e &lt;sql&gt;</code></td>
<td style="text-align: left">从查询结果中导入数据，该参数使用时必须指定<code class="prettyprint">–target-dir</code>、<code class="prettyprint">–hive-table</code>，在查询语句中一定要有where条件且在where条件中需要包含 <code class="prettyprint">\$CONDITIONS</code>，示例：<code class="prettyprint">--query &#39;select * from t where \$CONDITIONS &#39; --target-dir /tmp/t –hive-table t</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--split-by &lt;column&gt;</code></td>
<td style="text-align: left">表的列名，用来切分工作单元，一般后面跟主键ID</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--table &lt;table-name&gt;</code></td>
<td style="text-align: left">关系数据库表名，数据从该表中获取</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--delete-target-dir</code></td>
<td style="text-align: left">删除目标目录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--target-dir &lt;dir&gt;</code></td>
<td style="text-align: left">指定hdfs路径</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--warehouse-dir &lt;dir&gt;</code></td>
<td style="text-align: left">与 <code class="prettyprint">--target-dir</code> 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--where</code></td>
<td style="text-align: left">从关系数据库导入数据时的查询条件，示例：<code class="prettyprint">--where &quot;id = 2&quot;</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">-z,--compress</code></td>
<td style="text-align: left">压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--compression-codec</code></td>
<td style="text-align: left">Hadoop压缩编码，默认是gzip</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--null-string &lt;null-string&gt;</code></td>
<td style="text-align: left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--null-non-string &lt;null-string&gt;</code></td>
<td style="text-align: left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>
</tbody></table>

<p>示例程序：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --columns <span class="s2">&quot;tbl_id,create_time&quot;</span> --where <span class="s2">&quot;tbl_id &gt; 1&quot;</span> --target-dir /user/hive/result
</code></pre></div>
<h3 id="使用-sql-语句">使用 sql 语句</h3>

<p>参照上表，使用 sql 语句查询时，需要指定 <code class="prettyprint">$CONDITIONS</code></p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --query <span class="s1">&#39;SELECT * from TBLS where \$CONDITIONS &#39;</span> --split-by tbl_id -m 4 --target-dir /user/hive/result
</code></pre></div>
<p>上面命令通过 <code class="prettyprint">-m 1</code> 控制并发的 map 数。</p>

<h3 id="使用-direct-模式：">使用 direct 模式：</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --delete-target-dir --direct --default-character-set UTF-8 --target-dir /user/hive/result
</code></pre></div>
<h3 id="指定文件输出格式：">指定文件输出格式：</h3>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --delete-target-dir  --target-dir /user/hive/result
</code></pre></div>
<p>这时候查看 hdfs 中数据(观察分隔符是否为制表符)：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hadoop fs -ls result
Found 5 items
-rw-r--r--   3 root hadoop          0 2014-08-04 16:07 result/_SUCCESS
-rw-r--r--   3 root hadoop         69 2014-08-04 16:07 result/part-m-00000
-rw-r--r--   3 root hadoop          0 2014-08-04 16:07 result/part-m-00001
-rw-r--r--   3 root hadoop        142 2014-08-04 16:07 result/part-m-00002
-rw-r--r--   3 root hadoop         62 2014-08-04 16:07 result/part-m-00003

<span class="nv">$ </span>hadoop fs -cat result/part-m-00000
34  1406784308  8   0   root    0   45  test1   EXTERNAL_TABLE  null    null    null

<span class="nv">$ </span>hadoop fs -cat result/part-m-00002
40  1406797005  9   0   root    0   52  test2   EXTERNAL_TABLE  null    null    null
42  1407122307  7   0   root    0   59  test3   EXTERNAL_TABLE  null    null    null
</code></pre></div>
<p>指定空字符串：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --delete-target-dir --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --target-dir /user/hive/result
</code></pre></div>
<p>如果需要指定压缩：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --delete-target-dir --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --compression-codec <span class="s2">&quot;com.hadoop.compression.lzo.LzopCodec&quot;</span> --target-dir /user/hive/result
</code></pre></div>
<p>附：可选的文件参数如下表。</p>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--enclosed-by &lt;char&gt;</code></td>
<td style="text-align: left">给字段值前后加上指定的字符，比如双引号，示例：<code class="prettyprint">--enclosed-by &#39;\&quot;&#39;</code>，显示例子：&quot;3&quot;,&quot;jimsss&quot;,&quot;<a href="mailto:dd@dd.com">dd@dd.com</a>&quot;</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--escaped-by &lt;char&gt;</code></td>
<td style="text-align: left">给双引号作转义处理，如字段值为&quot;测试&quot;，经过 <code class="prettyprint">--escaped-by &quot;\\&quot;</code> 处理后，在hdfs中的显示值为：<code class="prettyprint">\&quot;测试\&quot;</code>，对单引号无效</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--fields-terminated-by &lt;char&gt;</code></td>
<td style="text-align: left">设定每个字段是以什么符号作为结束的，默认是逗号，也可以改为其它符号，如句号<code class="prettyprint">.</code>，示例如：<code class="prettyprint">--fields-terminated-by</code></td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--lines-terminated-by &lt;char&gt;</code></td>
<td style="text-align: left">设定每条记录行之间的分隔符，默认是换行串，但也可以设定自己所需要的字符串，示例如：<code class="prettyprint">--lines-terminated-by &quot;#&quot;</code> 以#号分隔</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--mysql-delimiters</code></td>
<td style="text-align: left">Mysql默认的分隔符设置，字段之间以<code class="prettyprint">,</code>隔开，行之间以换行<code class="prettyprint">\n</code>隔开，默认转义符号是<code class="prettyprint">\</code>，字段值以单引号<code class="prettyprint">&#39;</code>包含起来。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--optionally-enclosed-by &lt;char&gt;</code></td>
<td style="text-align: left">enclosed-by是强制给每个字段值前后都加上指定的符号，而<code class="prettyprint">--optionally-enclosed-by</code>只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的</td>
</tr>
</tbody></table>

<h1 id="2.3-创建-hive-表">2.3 创建 hive 表</h1>

<p>生成与关系数据库表的表结构对应的HIVE表：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop create-hive-table --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS 
</code></pre></div>
<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--hive-home &lt;dir&gt;</code></td>
<td style="text-align: left">Hive的安装目录，可以通过该参数覆盖掉默认的hive目录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--hive-overwrite</code></td>
<td style="text-align: left">覆盖掉在hive表中已经存在的数据</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--create-hive-table</code></td>
<td style="text-align: left">默认是false，如果目标表已经存在了，那么创建任务会失败</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--hive-table</code></td>
<td style="text-align: left">后面接要创建的hive表</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--table</code></td>
<td style="text-align: left">指定关系数据库表名</td>
</tr>
</tbody></table>

<h1 id="2.4-导入数据到-hive">2.4 导入数据到 hive</h1>

<p>执行下面的命令会将 mysql 中的数据导入到 hdfs 中，然后创建一个hive 表，最后再将 hdfs 上的文件移动到 hive 表的目录下面。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --delete-target-dir
</code></pre></div>
<p>说明：</p>

<ul>
<li>可以在 hive 的表名前面指定数据库名称</li>
<li>可以通过 <code class="prettyprint">--create-hive-table</code> 创建表，如果表已经存在则会执行失败</li>
</ul>

<p>接下来可以查看 hive 中的数据：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hive -e <span class="s1">&#39;select * from dw_srclog.tbls&#39;</span>
34  1406784308  8   0   root    0   45  test1   EXTERNAL_TABLE  null    null    NULL
40  1406797005  9   0   root    0   52  test2   EXTERNAL_TABLE  null    null    NULL
42  1407122307  7   0   root    0   59  test3   EXTERNAL_TABLE  null    null    NULL
</code></pre></div>
<p>直接查看文件内容：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hadoop fs -cat /user/hive/warehouse/dw_srclog.db/tbls/part-m-00000
34140678430880root045go_goodsEXTERNAL_TABLEnullnullnull
40140679700590root052merchantEXTERNAL_TABLEnullnullnull
</code></pre></div>
<p>从上面可见，数据导入到 hive 中之后分隔符为默认分隔符，参考上文你可以通过设置参数指定其他的分隔符。</p>

<p>另外，Sqoop 默认地导入空值（NULL）为 null 字符串，而 hive 使用 \N 去标识空值（NULL），故你在 import 或者 export 时候，需要做相应的处理。在 import 时，使用如下命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import  ... --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span>
</code></pre></div>
<p>在导出时，使用下面命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import  ... --input-null-string <span class="s1">&#39;&#39;</span> --input-null-non-string <span class="s1">&#39;&#39;</span>
</code></pre></div>
<p>一个完整的例子如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&quot;\t&quot;</span> --lines-terminated-by <span class="s2">&quot;\n&quot;</span> --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --compression-codec <span class="s2">&quot;com.hadoop.compression.lzo.LzopCodec&quot;</span>
</code></pre></div>
<h1 id="2.5-增量导入">2.5 增量导入</h1>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--check-column (col)</code></td>
<td style="text-align: left">用来作为判断的列名，如id</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--incremental (mode)</code></td>
<td style="text-align: left">append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--last-value (value)</code></td>
<td style="text-align: left">指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值</td>
</tr>
</tbody></table>

<h1 id="2.6-合并-hdfs-文件">2.6 合并 hdfs 文件</h1>

<p>将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中，示例如：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">sqoop merge –new-data /test/p1/person –onto /test/p2/person –target-dir /test/merged –jar-file /opt/data/sqoop/person/Person.jar –class-name Person –merge-key id
</code></pre></div>
<p>其中，<code class="prettyprint">–class-name</code> 所指定的 class 名是对应于 Person.jar 中的 Person 类，而 Person.jar 是通过 Codegen 生成的</p>

<table><thead>
<tr>
<th style="text-align: left">参数</th>
<th style="text-align: left">说明</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><code class="prettyprint">--new-data &lt;path&gt;</code></td>
<td style="text-align: left">Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能优先保留的，原则上一般是存放越新数据的目录就对应这个参数。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--onto &lt;path&gt;</code></td>
<td style="text-align: left">Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能被更新数据替换掉的，原则上一般是存放越旧数据的目录就对应这个参数。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--merge-key &lt;col&gt;</code></td>
<td style="text-align: left">合并键，一般是主键ID</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--jar-file &lt;file&gt;</code></td>
<td style="text-align: left">合并时引入的jar包，该jar包是通过Codegen工具生成的jar包</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--class-name &lt;class&gt;</code></td>
<td style="text-align: left">对应的表名或对象名，该class类是包含在jar包中的。</td>
</tr>
<tr>
<td style="text-align: left"><code class="prettyprint">--target-dir &lt;path&gt;</code></td>
<td style="text-align: left">合并后的数据在HDFS里的存放目录</td>
</tr>
</tbody></table>

<h1 id="3.-参考文章">3. 参考文章</h1>

<ul>
<li><a href="http://www.zihou.me/html/2014/01/28/9114.html">Sqoop中文手册</a></li>
</ul>
</div>

      <!--
      <div id="pay" style="text-align:center;">
        ----EOF-----
        <br>
        <section>
  <h4>Sponsor</h4>
	<img src="http://javachen-rs.qiniudn.com/images/alipay.png" width="150/">
  <p class="small">觉得此博客对你有帮助，支付宝扫码赞助吧</p>
</section>

      </div>
      -->
      <p class="meta">
      	
            Categories:
      	    
          	<a class="btn btn-default btn-xs" href="/categories.html#hadoop">hadoop</a>
          
      	

      	
            Tags:
      	    
          	<a class="btn btn-default btn-xs" href="/tags.html#sqoop">sqoop</a>
          
      	
      </p>
	</article>

	<ul class="pager">
	  
	  <li class="previous"><a class="btn btn-xs" href="/2014/07/31/summary-of-july-in-2014" title="2014年7月总结">&larr; Prev</a></li>
	  
	  
	  <li class="next"><a class="btn btn-xs" href="/2014/08/19/upgrading-from-cdh4-to-cdh5" title="升级cdh4到cdh5">Next &rarr;</a></li>
	  
	</ul>

  
<div id="comments">
  <div class="ds-thread" data-thread-key="/2014/08/04/import-data-to-hive-with-sqoop"  data-title="Sqoop导入关系数据库到Hive - 天松的个人博客"></div>
</div>



</div>

        </div>
      </div>

      <footer class="footer text-center">
  <p>&copy; 2009-2015 <a href="/" target="_blank" title="86后，工作在深圳；Java程序员、Hadoop工程师；主要关注Java、Scala、Hadoop、Kettle、关注大数据处理技术。">天松</a>. With Help from <a href="//jekyllrb.com/" target="_blank" title="Transform your plain text into static websites and blogs.">Jekyll</a> and <a href="//getbootstrap.com/" target="_blank" title="Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.">Bootstrap</a>, theme from <a href="http://havee.me/" target="_blank" title="http://havee.me/">Havee</a>.

  <span style="float:right;"><a href="/about.html">About</a></span>

	
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




  </p>
  <div id="toTop" style="display: block;">
    <a href="#">▲</a><a href="#footer">▼</a>
  </div>
</footer>

    </div>

    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/core.js"></script>

    
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




    
    <!-- duoshuo Begin -->
    <script type="text/javascript">
      var duoshuoQuery = {short_name:"xiaotian120"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script>
    <!-- duoshuo End -->
    
  </body>
</html>
