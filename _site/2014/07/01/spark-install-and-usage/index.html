<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Spark安装和使用 - 天松的个人博客</title>
  <meta name="description" content="本文主要记录 Spark 的安装过程配置过程并测试 Spark 的一些基本使用方法。">
  <meta name="keywords" content="java, hadoop, hive, hbase, spark, linux, scala, python, mysql">
  <meta name="author" content="天松">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
  <meta name="sogou_site_verification" content="ofwXWFdthV"/>

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" >

  <link rel="canonical" href="http://blog.xiatiansong.com/2014/07/01/spark-install-and-usage">
  <link rel="stylesheet" href="/static/css/bootstrap.min.css" media="all">
  <link rel="stylesheet" href="/static/css/style.css" media="all">
  <link rel="stylesheet" href="/static/css/pygments.css" media="all">
  <link rel="stylesheet" href="/static/css/font-awesome.css" media="all">

  <!-- atom & rss feed -->
  <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="天松的个人博客 RSS Feed">
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="天松的个人博客 ATOM Feed">
</head>


  <body>
    <div class="container">
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<header class="header">
  <div class="title"><a title="天松的个人博客" href="/">天松的个人博客</a></div>
  <ul class="nav navbar-nav navbar-right visible-lg visible-md">
    <li>
    <form id="search-form" class="form-group has-success visible-lg" role="form">
      <input type="text" class="form-control input-sm" placeholder="Search" id="query" style="width: 160px;">
    </form>
    </li>
    <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
    <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
    <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
    <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
    
    <li><a href="https://github.com/xiatiansong" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
    
    
    
    
    
    <li><a href="http://weibo.com/xiaotian120" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
    

    <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
  </ul>
</header>


      <div class="wrapper">
        <div class="row">
          <div id="search-loader" style="display:none;text-align:center">
            <img src="http://xiaotian120.qiniudn.com/images/loading.gif">
          </div>
          <div class="col-md-12">
  <article class="news-item">

      <h2  class="news-item"> Spark安装和使用  
        <time class="small">2014.07.01</time>
      </h2>

      <div><p>本文主要记录 Spark 的安装过程配置过程并测试 Spark 的一些基本使用方法。为了方便，这里使用 CDH 的 yum 源方式来安装 Spark，注意本文安装的 Spark 版本为 1.1。</p>

<ul>
<li>操作系统：CentOs 6.4</li>
<li>CDH 版本：5.2.0</li>
<li>Spark 版本：1.1</li>
</ul>

<p>关于 yum 源的配置以及 hadoop 的安装，请参考<a href="/2013/04/06/install-cloudera-cdh-by-yum">使用yum安装CDH Hadoop集群</a>。</p>

<h1 id="1.-spark-安装">1. Spark 安装</h1>

<p>选择一个节点来安装 Spark ，首先查看 Spark 相关的包有哪些：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum list <span class="p">|</span>grep spark
spark-core.noarch                 1.1.0+cdh5.2.0+56-1.cdh5.2.0.p0.35.el6 @cdh
spark-history-server.noarch       1.1.0+cdh5.2.0+56-1.cdh5.2.0.p0.35.el6 @cdh
spark-master.noarch               1.1.0+cdh5.2.0+56-1.cdh5.2.0.p0.35.el6 @cdh
spark-python.noarch               1.1.0+cdh5.2.0+56-1.cdh5.2.0.p0.35.el6 @cdh
spark-worker.noarch               1.1.0+cdh5.2.0+56-1.cdh5.2.0.p0.35.el6 @cdh
hue-spark.x86_64                  3.6.0+cdh5.2.0+509-1.cdh5.2.0.p0.37.el6
</code></pre></div>
<p>以上包作用如下：</p>

<ul>
<li>spark-core: spark 核心功能</li>
<li>spark-worker: spark-worker 初始化脚本</li>
<li>spark-master: spark-master 初始化脚本</li>
<li>spark-python: spark 的 Python 客户端</li>
<li>hue-spark: spark 和 hue 集成包</li>
<li>spark-history-server</li>
</ul>

<p>安装脚本如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo yum install spark-core spark-master spark-worker spark-python spark-history-server
</code></pre></div>
<h1 id="2.-配置">2. 配置</h1>

<h2 id="修改配置文件">修改配置文件</h2>

<p>设置环境变量，在 <code class="prettyprint">.bashrc</code> 中加入下面一行，并使其生效：</p>
<div class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="na">export SPARK_HOME</span><span class="o">=</span><span class="s">/usr/lib/spark</span>
</code></pre></div>
<p>可以修改配置文件 <code class="prettyprint">/etc/spark/conf/spark-env.sh</code>，其内容如下，你可以根据需要做一些修改。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">STANDALONE_SPARK_MASTER_HOST</span><span class="o">=</span><span class="sb">`</span>hostname<span class="sb">`</span>

<span class="nb">export </span><span class="nv">SPARK_MASTER_IP</span><span class="o">=</span><span class="nv">$STANDALONE_SPARK_MASTER_HOST</span>

<span class="c">### Let&#39;s run everything with JVM runtime, instead of Scala</span>
<span class="nb">export </span><span class="nv">SPARK_LAUNCH_WITH_SCALA</span><span class="o">=</span>0
<span class="nb">export </span><span class="nv">SPARK_LIBRARY_PATH</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_HOME</span><span class="k">}</span>/lib
<span class="nb">export </span><span class="nv">SCALA_LIBRARY_PATH</span><span class="o">=</span><span class="k">${</span><span class="nv">SPARK_HOME</span><span class="k">}</span>/lib
<span class="nb">export </span><span class="nv">SPARK_MASTER_WEBUI_PORT</span><span class="o">=</span>18080
<span class="nb">export </span><span class="nv">SPARK_MASTER_PORT</span><span class="o">=</span>7077
<span class="nb">export </span><span class="nv">SPARK_WORKER_PORT</span><span class="o">=</span>7078
<span class="nb">export </span><span class="nv">SPARK_WORKER_WEBUI_PORT</span><span class="o">=</span>18081
<span class="nb">export </span><span class="nv">SPARK_WORKER_DIR</span><span class="o">=</span>/var/run/spark/work
<span class="nb">export </span><span class="nv">SPARK_LOG_DIR</span><span class="o">=</span>/var/log/spark

<span class="k">if</span> <span class="o">[</span> -n <span class="s2">&quot;$HADOOP_HOME&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
<span class="k">  </span><span class="nb">export </span><span class="nv">SPARK_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$SPARK_LIBRARY_PATH</span>:<span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/lib/native
<span class="k">fi</span>


<span class="nb">export </span><span class="nv">HIVE_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HIVE_CONF_DIR</span><span class="k">:-</span><span class="p">/etc/hive/conf</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_CONF_DIR</span><span class="k">:-</span><span class="p">/etc/hadoop/conf</span><span class="k">}</span>


<span class="c">### Comment above 2 lines and uncomment the following if</span>
<span class="c">### you want to run with scala version, that is included with the package</span>
<span class="c">#export SCALA_HOME=${SCALA_HOME:-/usr/lib/spark/scala}</span>
<span class="c">#export PATH=$PATH:$SCALA_HOME/bin</span>
</code></pre></div>
<h2 id="配置-spark-history-server">配置 Spark History Server</h2>

<p>执行下面命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir /user/spark
<span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir /user/spark/applicationHistory
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chown -R spark:spark /user/spark
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chmod 1777 /user/spark/applicationHistory
</code></pre></div>
<p>在 Spark 客户端创建 <code class="prettyprint">/etc/spark/conf/spark-defaults.conf</code>：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">cp /etc/spark/conf/spark-defaults.conf.template /etc/spark/conf/spark-defaults.conf
</code></pre></div>
<p>在 <code class="prettyprint">/etc/spark/conf/spark-defaults.conf</code> 添加两行：</p>
<div class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="na">spark.eventLog.dir</span><span class="o">=</span><span class="s">/user/spark/applicationHistory</span>
<span class="na">spark.eventLog.enabled</span><span class="o">=</span><span class="s">true</span>
</code></pre></div>
<p>如果想 YARN ResourceManager 访问 Spark History Server ，则添加一行：</p>
<div class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="na">spark.yarn.historyServer.address</span><span class="o">=</span><span class="s">http://HISTORY_HOST:HISTORY_PORT</span>
</code></pre></div>
<h1 id="3.-启动和停止">3. 启动和停止</h1>

<p>启动脚本：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo service spark-master start
<span class="nv">$ </span>sudo service spark-worker start
</code></pre></div>
<p>停止脚本：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo service spark-worker stop
<span class="nv">$ </span>sudo service spark-master stop
</code></pre></div>
<p>当然，你还可以设置开机启动：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo chkconfig spark-worker on
<span class="nv">$ </span>sudo chkconfig spark-master on
</code></pre></div>
<p>运行日志保存在 <code class="prettyprint">/var/log/spark</code>，你可以通过<code class="prettyprint">&lt;http://IP:18080/&gt;</code>（我这里 IP 为 cdh1）访问 spark master 的 web 界面</p>

<p>当然，你也可以使用 spark 自带的脚本来启动和停止，这些脚本在 <code class="prettyprint">/usr/lib/spark/sbin</code> 目录下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>ls /usr/lib/spark/sbin
slaves.sh        spark-daemons.sh  start-master.sh  stop-all.sh
spark-config.sh  spark-executor    start-slave.sh   stop-master.sh
spark-daemon.sh  start-all.sh      start-slaves.sh  stop-slaves.sh
</code></pre></div>
<p>例如，你也可以通过下面脚本启动master：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">cd</span> /usr/lib/spark/sbin
<span class="nv">$ </span>./start-master.sh
</code></pre></div>
<p>类似地，通过下面命令启动worker：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:18080
</code></pre></div>
<h1 id="4.-测试">4. 测试</h1>

<h2 id="运行测试例子">运行测试例子</h2>

<p>运行 Spark 自带的 Example，在 spark home 目录运行：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/run-example SparkPi 10
ls: cannot access /usr/lib/spark/lib/spark-examples-*hadoop*.jar: No such file or directory
Failed to find Spark examples assembly in /usr/lib/spark/lib or /usr/lib/spark/examples/target
You need to build Spark before running this program
</code></pre></div>
<p>出现上面异常，需要把 examples/lib 目录拷贝到 /usr/lib/spark 目录下，然后再运行。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">cd</span> /usr/lib/spark
cp -r examples/lib .
</code></pre></div>
<p>你还可以运行 spark shell 的交互模式：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/spark-shell --master <span class="nb">local</span><span class="o">[</span>2<span class="o">]</span>

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _<span class="se">\ \/</span> _ <span class="se">\/</span> _ / __/  _/
   /___/ .__/<span class="se">\_</span>,_/_/ /_/<span class="se">\_\ </span>  version 1.1.0
      /_/

Using Scala version 2.10.4 <span class="o">(</span>Java HotSpot<span class="o">(</span>TM<span class="o">)</span> 64-Bit Server VM, Java 1.6.0_45<span class="o">)</span>
Type in expressions to have them evaluated.

Spark context available as sc.

scala&gt;
</code></pre></div>
<p>通过 Python API 来运行交互模式：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/pyspark --master <span class="nb">local</span><span class="o">[</span>2<span class="o">]</span>

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _<span class="se">\ \/</span> _ <span class="se">\/</span> _ / __/  _/
   /__ / .__/<span class="se">\_</span>,_/_/ /_/<span class="se">\_\ </span>  version 1.1.0
      /_/

Using Python version 2.6.6 <span class="o">(</span>r266:84292, Jan 22 2014 09:42:36<span class="o">)</span>
SparkContext available as sc.
&gt;&gt;&gt;
</code></pre></div>
<p>你也可以运行 Python 编写的应用(注意：<strong>使用 yum 安装 cdh5.2后，spark 的 home 目录下不存在 examples/src/ 目录</strong>)：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>./bin/spark-submit examples/src/main/python/pi.py 10
</code></pre></div>
<h2 id="在集群上运行">在集群上运行</h2>

<p>Spark 目前支持三种集群管理模式：</p>

<ul>
<li>Standalone</li>
<li>Apache Mesos</li>
<li>Hadoop YARN</li>
</ul>

<h3 id="standalone-模式">Standalone 模式</h3>

<p>你可以通过 spark-shel l运行下面的 wordcount 例子，因为 hdfs 上的输入和输出文件都涉及到用户的访问权限，故这里使用 hive 用户来启动 spark-shell：</p>

<p>读取 hdfs 的一个例子：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">echo</span> <span class="s2">&quot;hello world&quot;</span> &gt;test.txt
<span class="nv">$ </span>hadoop fs -put test.txt /tmp

<span class="nv">$ </span>spark-shell
scala&gt; val <span class="nv">file</span> <span class="o">=</span> sc.textFile<span class="o">(</span><span class="s2">&quot;hdfs://cdh1:8020/tmp/test.txt&quot;</span><span class="o">)</span>
scala&gt; file.count<span class="o">()</span>
</code></pre></div>
<p>更复杂的一个例子，运行 mapreduce 统计单词数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-shell
scala&gt; val <span class="nv">file</span> <span class="o">=</span> sc.textFile<span class="o">(</span><span class="s2">&quot;hdfs://cdh1:8020/tmp/test.txt&quot;</span><span class="o">)</span>
scala&gt; val <span class="nv">counts</span> <span class="o">=</span> file.flatMap<span class="o">(</span><span class="nv">line</span> <span class="o">=</span>&gt; line.split<span class="o">(</span><span class="s2">&quot; &quot;</span><span class="o">))</span>.map<span class="o">(</span><span class="nv">word</span> <span class="o">=</span>&gt; <span class="o">(</span>word, 1<span class="o">))</span>.reduceByKey<span class="o">(</span>_ + _<span class="o">)</span>
scala&gt; counts.saveAsTextFile<span class="o">(</span><span class="s2">&quot;hdfs://cdh1:8020/tmp/output&quot;</span><span class="o">)</span>
</code></pre></div>
<p>运行过程中，可能会出现下面的错误：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">14/10/24 14:51:40 WARN hdfs.BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
14/10/24 14:51:40 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1738)
    at java.lang.Runtime.loadLibrary0(Runtime.java:823)
    at java.lang.System.loadLibrary(System.java:1028)
    at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:32)
    at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:71)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:249)
    at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1836)
    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1801)
    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)
</code></pre></div>
<p>该异常的解决方法可以参考 <a href="http://blog.csdn.net/pelick/article/details/11599391">Spark连接Hadoop读取HDFS问题小结</a> 这篇文章。</p>

<p>解决方法：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">cp /usr/lib/hadoop/lib/native/libgplcompression.so <span class="nv">$JAVA_HOME</span>/jre/lib/amd64/
cp /usr/lib/hadoop/lib/native/libhadoop.so <span class="nv">$JAVA_HOME</span>/jre/lib/amd64/
cp /usr/lib/hadoop/lib/native/libsnappy.so <span class="nv">$JAVA_HOME</span>/jre/lib/amd64/
</code></pre></div>
<p>运行完成之后，你可以查看 <code class="prettyprint">hdfs://cdh1:8020/tmp/output</code> 目录下的文件内容。</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hadoop fs -cat /tmp/output/part-00000
<span class="o">(</span>hello,1<span class="o">)</span>
<span class="o">(</span>world,1<span class="o">)</span>
</code></pre></div>
<p>另外，spark-shell 后面还可以加上其他参数，例如指定 IP 和端口、运行核数：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-shell --master spark://cdh1:7077 --cores 2
scala&gt;
</code></pre></div>
<p>另外，也可以使用 spark-submit 以 Standalone 模式运行 SparkPi 程序：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode client --master spark://cdh1:7077 /usr/lib/spark/examples/lib/spark-examples-1.1.0-cdh5.2.0-hadoop2.5.0-cdh5.2.0.jar 10
</code></pre></div>
<h3 id="spark-on-yarn">Spark on Yarn</h3>

<p>以 YARN 客户端方式运行 SparkPi 程序：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode client --master yarn /usr/lib/spark/examples/lib/spark-examples-1.1.0-cdh5.2.0-hadoop2.5.0-cdh5.2.0.jar 10
</code></pre></div>
<p>以 YARN 集群方式运行 SparkPi 程序：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode cluster --master yarn /usr/lib/spark/examples/lib/spark-examples-1.1.0-cdh5.2.0-hadoop2.5.0-cdh5.2.0.jar 10
</code></pre></div>
<p>运行在 YARN 集群之上的时候，可以手动把 spark-assembly 相关的 jar 包拷贝到 hdfs 上去，然后设置 <code class="prettyprint">SPARK_JAR</code> 环境变量：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hdfs dfs -mkdir -p /user/spark/share/lib
<span class="nv">$ </span>hdfs dfs -put <span class="nv">$SPARK_HOME</span>/assembly/lib/spark-assembly_*.jar  /user/spark/share/lib/spark-assembly.jar

<span class="nv">$ SPARK_JAR</span><span class="o">=</span>hdfs://&lt;nn&gt;:&lt;port&gt;/user/spark/share/lib/spark-assembly.jar
</code></pre></div>
<h1 id="5.-spark-sql">5. Spark-SQL</h1>

<p>Spark 安装包中包括了 Spark-SQL ，运行 spark-sql 命令，出现下面异常：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">cd</span> /usr/lib/spark/bin
<span class="nv">$ </span>./spark-sql
java.lang.ClassNotFoundException: org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver
    at java.net.URLClassLoader<span class="nv">$1</span>.run<span class="o">(</span>URLClassLoader.java:202<span class="o">)</span>
    at java.security.AccessController.doPrivileged<span class="o">(</span>Native Method<span class="o">)</span>
    at java.net.URLClassLoader.findClass<span class="o">(</span>URLClassLoader.java:190<span class="o">)</span>
    at java.lang.ClassLoader.loadClass<span class="o">(</span>ClassLoader.java:306<span class="o">)</span>
    at java.lang.ClassLoader.loadClass<span class="o">(</span>ClassLoader.java:247<span class="o">)</span>
    at java.lang.Class.forName0<span class="o">(</span>Native Method<span class="o">)</span>
    at java.lang.Class.forName<span class="o">(</span>Class.java:247<span class="o">)</span>
    at org.apache.spark.deploy.SparkSubmit<span class="nv">$.</span>launch<span class="o">(</span>SparkSubmit.scala:319<span class="o">)</span>
    at org.apache.spark.deploy.SparkSubmit<span class="nv">$.</span>main<span class="o">(</span>SparkSubmit.scala:75<span class="o">)</span>
    at org.apache.spark.deploy.SparkSubmit.main<span class="o">(</span>SparkSubmit.scala<span class="o">)</span>

Failed to load Spark SQL CLI main class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.
You need to build Spark with -Phive.
</code></pre></div>
<p>从上可以知道  Spark-SQL 编译时没有集成 Hive，故需要重新编译 spark 源代码。</p>

<h2 id="编译-spark-sql">编译 Spark-SQL</h2>

<p>下载代码：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>git clone git@github.com:cloudera/spark.git
<span class="nv">$ </span><span class="nb">cd </span>spark
<span class="nv">$ </span>git checkout -b origin/cdh5-1.1.0_5.2.0
</code></pre></div>
<p>编译代码，集成 yarn 和 hive，有三种方式：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sbt/sbt -Dhadoop.version<span class="o">=</span>2.5.0-cdh5.2.0 -Pyarn -Phive assembly
</code></pre></div>
<p>等很长很长一段时间，会提示错误。</p>

<p>改为 maven 编译：</p>

<p>修改根目录下的 pom.xml，添加一行 <code class="prettyprint">&lt;module&gt;sql/hive-thriftserver&lt;/module&gt;</code>：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;modules&gt;</span>
    <span class="nt">&lt;module&gt;</span>core<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>bagel<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>graphx<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>mllib<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>tools<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>streaming<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/catalyst<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/core<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/hive<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>sql/hive-thriftserver<span class="nt">&lt;/module&gt;</span> <span class="c">&lt;!--添加的一行--&gt;</span>
    <span class="nt">&lt;module&gt;</span>repl<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>assembly<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/twitter<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/kafka<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/flume<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/flume-sink<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/zeromq<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>external/mqtt<span class="nt">&lt;/module&gt;</span>
    <span class="nt">&lt;module&gt;</span>examples<span class="nt">&lt;/module&gt;</span>
  <span class="nt">&lt;/modules&gt;</span>
</code></pre></div>
<p>然后运行：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">export </span><span class="nv">MAVEN_OPTS</span><span class="o">=</span><span class="s2">&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;</span>
<span class="nv">$ </span>mvn -Pyarn -Dhadoop.version<span class="o">=</span>2.5.0-cdh5.2.0 -Phive -DskipTests clean package
</code></pre></div>
<p>如果编译成功之后， 会在 assembly/target/scala-2.10 目录下生成：spark-assembly-1.1.0-cdh5.2.0.jar，在 examples/target/scala-2.10 目录下生成：spark-examples-1.1.0-cdh5.2.0.jar</p>

<p>但是，经测试 cdh5.2.0 版本中的 spark 的 sql/hive-thriftserver 模块存在编译错误，最后无法编译成功，故需要等到 cloudera 官方更新源代码或者等待下一个 cdh 版本集成 spark-sql。</p>

<p>如果编译成功了，则将 spark-assembly-1.1.0-cdh5.2.0.jar 拷贝到 /usr/lib/spark/assembly/lib 目录，然后再来运行 spark-sql。</p>
</div>

      <!--
      <div id="pay" style="text-align:center;">
        ----EOF-----
        <br>
        <section>
  <h4>Sponsor</h4>
	<img src="http://xiaotian120.qiniudn.com/images/alipay.png" width="150/">
  <p class="small">觉得此博客对你有帮助，支付宝扫码赞助吧</p>
</section>

      </div>
      -->
      <p class="meta">
      	
            Categories:
      	    
          	<a class="btn btn-default btn-xs" href="/categories.html#spark">spark</a>
          
      	

      	
            Tags:
      	    
          	<a class="btn btn-default btn-xs" href="/tags.html#spark">spark</a>
          
      	
      </p>
	</article>

	<ul class="pager">
	  
	  <li class="previous"><a class="btn btn-xs" href="/2014/06/26/some-tips-about-hbase" title="HBase中的一些注意事项">&larr; Prev</a></li>
	  
	  
	  <li class="next"><a class="btn btn-xs" href="/2014/07/17/manual-install-cdh-hadoop" title="手动安装Hadoop集群的过程">Next &rarr;</a></li>
	  
	</ul>

  
<div id="comments">
  <div class="ds-thread" data-thread-key="/2014/07/01/spark-install-and-usage"  data-title="Spark安装和使用 - 天松的个人博客"></div>
</div>



</div>

        </div>
      </div>

      <footer class="footer text-center">
  <p>&copy; 2009-2015 <a href="/" target="_blank" title="86后，工作在深圳；Java程序员、Hadoop工程师；主要关注Java、Scala、Hadoop、Kettle、关注大数据处理技术。">天松</a>. With Help from <a href="//jekyllrb.com/" target="_blank" title="Transform your plain text into static websites and blogs.">Jekyll</a> and <a href="//getbootstrap.com/" target="_blank" title="Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.">Bootstrap</a>, theme from <a href="http://havee.me/" target="_blank" title="http://havee.me/">Havee</a>.

  <span style="float:right;"><a href="/about.html">About</a></span>

	
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




  </p>
  <div id="toTop" style="display: block;">
    <a href="#">▲</a><a href="#footer">▼</a>
  </div>
</footer>

    </div>

    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/core.js"></script>

    
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




    
    <!-- duoshuo Begin -->
    <script type="text/javascript">
      var duoshuoQuery = {short_name:"sunshine1987"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script>
    <!-- duoshuo End -->
    
  </body>
</html>
