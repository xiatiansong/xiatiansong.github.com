<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Flume-ng的原理和使用 - 夏天松的个人博客</title>
  <meta name="description" content="Flume 是 Cloudera 提供的日志收集系统，具有分布式、高可靠、高可用性等特点，对海量日志采集、聚合和传输，Flume 支持在日志系统中定制各类数据发送方，同时，Flume提供对数据进行简单处理，并写到各种数据接受方的能力。">
  <meta name="keywords" content="Flume">
  <meta name="author" content="夏天松">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
  <meta name="sogou_site_verification" content="ofwXWFdthV"/>

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" >

  <link rel="canonical" href="http://blog.xiatiansong.com/2014/07/22/flume-ng">
  <link rel="stylesheet" href="/static/css/bootstrap.min.css" media="all">
  <link rel="stylesheet" href="/static/css/style.css" media="all">
  <link rel="stylesheet" href="/static/css/pygments.css" media="all">
  <link rel="stylesheet" href="/static/css/font-awesome.css" media="all">

  <!-- atom & rss feed -->
  <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="夏天松的个人博客 RSS Feed">
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="夏天松的个人博客 ATOM Feed">
</head>


  <body>
    <div class="container">
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<header class="header">
  <div class="title"><a title="夏天松的个人博客" href="/">夏天松的个人博客</a></div>
  <ul class="nav navbar-nav navbar-right visible-lg visible-md">
    <li>
    <form id="search-form" class="form-group has-success visible-lg" role="form">
      <input type="text" class="form-control input-sm" placeholder="Search" id="query" style="width: 160px;">
    </form>
    </li>
    <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
    <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
    <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
    <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
    
    <li><a href="https://github.com/xiatiansong" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
    
    
    
    
    
    <li><a href="http://weibo.com/xiaotian120" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
    

    <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
  </ul>
</header>


      <div class="wrapper">
        <div class="row">
          <div id="search-loader" style="display:none;text-align:center">
            <img src="http://javachen-rs.qiniudn.com/images/loading.gif">
          </div>
          <div class="col-md-12">
  <article class="news-item">

      <h2  class="news-item"> Flume-ng的原理和使用  
        <time class="small">2014.07.22</time>
      </h2>

      <div><h1 id="1.-介绍">1. 介绍</h1>

<p>Flume 是 Cloudera 提供的日志收集系统，具有分布式、高可靠、高可用性等特点，对海量日志采集、聚合和传输，Flume 支持在日志系统中定制各类数据发送方，同时，Flume提供对数据进行简单处理，并写到各种数据接受方的能力。</p>

<p>Flume 使用 java 编写，其需要运行在 Java1.6 或更高版本之上。</p>

<ul>
<li>官方网站：<a href="http://flume.apache.org/">http://flume.apache.org/</a></li>
<li>用户文档：<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></li>
<li>开发文档：<a href="http://flume.apache.org/FlumeDeveloperGuide.html">http://flume.apache.org/FlumeDeveloperGuide.html</a></li>
</ul>

<h1 id="2.-架构">2. 架构</h1>

<h2 id="2.1-数据流">2.1 数据流</h2>

<p>Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。</p>

<p>Flume 传输的数据的基本单位是 Event，如果是文本文件，通常是一行记录，这也是事务的基本单位。Event 从 Source，流向 Channel，再到 Sink，本身为一个 byte 数组，并可携带 headers 信息。Event 代表着一个数据流的最小完整单元，从外部数据源来，向外部的目的地去。</p>

<p>Flume 运行的核心是 Agent。它是一个完整的数据收集工具，含有三个核心组件，分别是 source、channel、sink。通过这些组件，Event 可以从一个地方流向另一个地方，如下图所示。</p>

<p><img src="http://flume.apache.org/_images/UserGuide_image00.png" alt=""></p>

<ul>
<li>source 可以接收外部源发送过来的数据。不同的 source，可以接受不同的数据格式。比如有目录池(spooling directory)数据源，可以监控指定文件夹中的新文件变化，如果目录中有文件产生，就会立刻读取其内容。</li>
<li>channel 是一个存储地，接收 source 的输出，直到有 sink 消费掉 channel 中的数据。channel 中的数据直到进入到下一个channel中或者进入终端才会被删除。当 sink 写入失败后，可以自动重启，不会造成数据丢失，因此很可靠。</li>
<li>sink 会消费 channel 中的数据，然后送给外部源或者其他 source。如数据可以写入到 HDFS 或者 HBase 中。</li>
</ul>

<p>flume 允许多个 agent 连在一起，形成前后相连的多级跳。</p>

<p><img src="http://flume.apache.org/_images/UserGuide_image02.png" alt=""></p>

<h2 id="2.2-核心组件">2.2 核心组件</h2>

<h3 id="2.2.1-source">2.2.1 source</h3>

<p>Client端操作消费数据的来源，Flume 支持 Avro，log4j，syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。也可以 写一个 Source，以 IPC 或 RPC 的方式接入自己的应用，Avro和 Thrift 都可以(分别有 NettyAvroRpcClient 和 ThriftRpcClient 实现了 RpcClient接口)，其中 Avro 是默认的 RPC 协议。具体代码级别的 Client 端数据接入，可以参考官方手册。</p>

<p>对现有程序改动最小的使用方式是使用是直接读取程序原来记录的日志文件，基本可以实现无缝接入，不需要对现有程序进行任何改动。 <br>
对于直接读取文件 Source,有两种方式： </p>

<ul>
<li>ExecSource: 以运行 Linux 命令的方式，持续的输出最新的数据，如 <code class="prettyprint">tail -F 文件名</code> 指令，在这种方式下，取的文件名必须是指定的。 ExecSource 可以实现对日志的实时收集，但是存在Flume不运行或者指令执行出错时，将无法收集到日志数据，无法保证日志数据的完整性。</li>
<li>SpoolSource: 监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点：拷贝到 spool 目录下的文件不可以再打开编辑；spool 目录下不可包含相应的子目录。</li>
</ul>

<p>SpoolSource 虽然无法实现实时的收集数据，但是可以使用以分钟的方式分割文件，趋近于实时。</p>

<p>如果应用无法实现以分钟切割日志文件的话， 可以两种收集方式结合使用。 在实际使用的过程中，可以结合 log4j 使用，使用 log4j的时候，将 log4j 的文件分割机制设为1分钟一次，将文件拷贝到spool的监控目录。 </p>

<p>log4j 有一个 TimeRolling 的插件，可以把 log4j 分割文件到 spool 目录。基本实现了实时的监控。Flume 在传完文件之后，将会修改文件的后缀，变为 .COMPLETED（后缀也可以在配置文件中灵活指定）</p>

<h3 id="2.2.2-channel">2.2.2 Channel</h3>

<p>当前有几个 channel 可供选择，分别是 Memory Channel, JDBC Channel , File Channel，Psuedo Transaction Channel。比较常见的是前三种 channel。</p>

<ul>
<li>MemoryChannel 可以实现高速的吞吐，但是无法保证数据的完整性。</li>
<li>MemoryRecoverChannel 在官方文档的建议上已经建义使用FileChannel来替换。</li>
<li>FileChannel保证数据的完整性与一致性。在具体配置FileChannel时，建议FileChannel设置的目录和程序日志文件保存的目录设成不同的磁盘，以便提高效率。 </li>
</ul>

<p>File Channel 是一个持久化的隧道（channel），它持久化所有的事件，并将其存储到磁盘中。因此，即使 Java 虚拟机当掉，或者操作系统崩溃或重启，再或者事件没有在管道中成功地传递到下一个代理（agent），这一切都不会造成数据丢失。Memory Channel 是一个不稳定的隧道，其原因是由于它在内存中存储所有事件。如果 java 进程死掉，任何存储在内存的事件将会丢失。另外，内存的空间收到 RAM大小的限制,而 File Channel 这方面是它的优势，只要磁盘空间足够，它就可以将所有事件数据存储到磁盘上。</p>

<h3 id="2.2.3-sink">2.2.3 sink</h3>

<p>Sink在设置存储数据时，可以向文件系统、数据库、hadoop存数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析.</p>

<p>更多sink的内容可以参考<a href="http://flume.apache.org/FlumeDeveloperGuide.html#sink">官方手册</a>。</p>

<h2 id="2.3-可靠性">2.3 可靠性</h2>

<p>Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。</p>

<p>Flume 使用事务性的方式保证传送Event整个过程的可靠性。Sink 必须在 Event 被存入 Channel 后，或者，已经被传达到下一站agent里，又或者，已经被存入外部数据目的地之后，才能把 Event 从 Channel 中 remove 掉。这样数据流里的 event 无论是在一个 agent 里还是多个 agent 之间流转，都能保证可靠，因为以上的事务保证了 event 会被成功存储起来。而 Channel 的多种实现在可恢复性上有不同的保证。也保证了 event 不同程度的可靠性。比如 Flume 支持在本地保存一份文件 channel 作为备份，而memory channel 将 event 存在内存 queue 里，速度快，但丢失的话无法恢复。</p>

<h2 id="2.4-可恢复性">2.4 可恢复性</h2>

<h1 id="3.-安装和使用">3. 安装和使用</h1>

<p>Flume 的 rpm 安装方式很简单，这里不做说明。</p>

<h2 id="示例1：-avro-数据源">示例1： avro 数据源</h2>

<p>安装成功之后，在 /etc/flume/conf 目录创建f1.conf 文件，内容如下:</p>
<div class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="na">agent-1.channels.ch-1.type</span> <span class="o">=</span> <span class="s">memory</span>

<span class="na">agent-1.sources.avro-source1.channels</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sources.avro-source1.type</span> <span class="o">=</span> <span class="s">avro</span>
<span class="na">agent-1.sources.avro-source1.bind</span> <span class="o">=</span> <span class="s">0.0.0.0</span>
<span class="na">agent-1.sources.avro-source1.port</span> <span class="o">=</span> <span class="s">41414</span>
<span class="na">agent-1.sources.avro-source1.threads</span> <span class="o">=</span> <span class="s">5</span>

<span class="na">agent-1.sinks.log-sink1.channel</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sinks.log-sink1.type</span> <span class="o">=</span> <span class="s">logger</span>

<span class="na">agent-1.channels</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sources</span> <span class="o">=</span> <span class="s">avro-source1</span>
<span class="na">agent-1.sinks</span> <span class="o">=</span> <span class="s">log-sink1</span>
</code></pre></div>
<p>关于 avro-source 配置说明，请参考 <a href="http://flume.apache.org/FlumeUserGuide.html#avro-source">avro-source</a></p>

<p>接下来启动 agent：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/f1.conf -Dflume.root.logger<span class="o">=</span>DEBUG,console -n agent-1
</code></pre></div>
<p>参数说明：</p>

<ul>
<li><code class="prettyprint">-n</code> 指定agent名称</li>
<li><code class="prettyprint">-c</code> 指定配置文件目录</li>
<li><code class="prettyprint">-f</code> 指定配置文件</li>
<li><code class="prettyprint">-Dflume.root.logger=DEBUG,console</code> 设置日志等级</li>
</ul>

<p>下面可以启动一个 avro-client 客户端生产数据：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>flume-ng avro-client -c /etc/flume-ng/conf -H localhost -p 41414 -F /etc/passwd -Dflume.root.logger<span class="o">=</span>DEBUG,console
</code></pre></div>
<h2 id="示例2：spooldir-数据源">示例2：spooldir 数据源</h2>

<p>在 /etc/flume/conf 目录创建 f2.conf 文件，内容如下:</p>
<div class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="na">agent-1.channels</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sources</span> <span class="o">=</span> <span class="s">src-1</span>

<span class="na">agent-1.channels.ch-1.type</span> <span class="o">=</span> <span class="s">memory</span>

<span class="na">agent-1.sources.src-1.type</span> <span class="o">=</span> <span class="s">spooldir</span>
<span class="na">agent-1.sources.src-1.channels</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sources.src-1.spoolDir</span> <span class="o">=</span> <span class="s">/root/log</span>
<span class="na">agent-1.sources.src-1.fileHeader</span> <span class="o">=</span> <span class="s">true</span>

<span class="na">agent-1.sinks.log-sink1.channel</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sinks.log-sink1.type</span> <span class="o">=</span> <span class="s">logger</span>

<span class="na">agent-1.sinks</span> <span class="o">=</span> <span class="s">log-sink1</span>
</code></pre></div>
<p>关于 Spooling Directory Source 配置说明，请参考 <a href="http://flume.apache.org/FlumeUserGuide.html#spooling-directory-source">Spooling Directory Source</a></p>

<p>接下来启动 agent：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/f2.conf -Dflume.root.logger<span class="o">=</span>DEBUG,console -n agent-1
</code></pre></div>
<p>然后，手动拷贝一个文件到 /root/log 目录，观察日志输出以及/root/log 目录下的变化。</p>

<h2 id="示例3：spooldir-数据源，写入-hdfs">示例3：spooldir 数据源，写入 hdfs</h2>

<p>在 /etc/flume/conf 目录创建 f3.conf 文件，内容如下:</p>
<div class="highlight"><pre><code class="language-properties" data-lang="properties"><span class="na">agent-1.channels.ch-1.type</span> <span class="o">=</span> <span class="s">file</span>
<span class="na">agent-1.channels.ch-1.checkpointDir</span><span class="o">=</span> <span class="s">/root/checkpoint</span>
<span class="na">agent-1.channels.ch-1.dataDirs</span><span class="o">=</span> <span class="s">/root/data</span>

<span class="na">agent-1.sources.src-1.type</span> <span class="o">=</span> <span class="s">spooldir</span>
<span class="na">agent-1.sources.src-1.channels</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sources.src-1.spoolDir</span> <span class="o">=</span> <span class="s">/root/log</span>
<span class="na">agent-1.sources.src-1.deletePolicy</span><span class="o">=</span> <span class="s">never</span>
<span class="na">agent-1.sources.src-1.fileHeader</span> <span class="o">=</span> <span class="s">true</span>

<span class="na">agent-1.sources.src-1.interceptors</span> <span class="o">=</span><span class="s">i1</span>
<span class="na">agent-1.sources.src-1.interceptors.i1.type</span> <span class="o">=</span> <span class="s">timestamp</span>

<span class="na">agent-1.sinks.sink_hdfs.channel</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sinks.sink_hdfs.type</span> <span class="o">=</span> <span class="s">hdfs</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.path</span> <span class="o">=</span> <span class="s">hdfs://cdh1:8020/user/root/events/%Y-%m-%d</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.filePrefix</span> <span class="o">=</span> <span class="s">logs</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.inUsePrefix</span> <span class="o">=</span> <span class="s">.</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.rollInterval</span> <span class="o">=</span> <span class="s">30</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.rollSize</span> <span class="o">=</span> <span class="s">0</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.rollCount</span> <span class="o">=</span> <span class="s">0</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.batchSize</span> <span class="o">=</span> <span class="s">1000</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.writeFormat</span> <span class="o">=</span> <span class="s">text</span>
<span class="na">agent-1.sinks.sink_hdfs.hdfs.fileType</span> <span class="o">=</span> <span class="s">DataStream</span>
<span class="c">#agent-1.sinks.sink_hdfs.hdfs.fileType = CompressedStream</span>
<span class="c">#agent-1.sinks.sink_hdfs.hdfs.codeC = lzop</span>

<span class="na">agent-1.channels</span> <span class="o">=</span> <span class="s">ch-1</span>
<span class="na">agent-1.sources</span> <span class="o">=</span> <span class="s">src-1</span>
<span class="na">agent-1.sinks</span> <span class="o">=</span> <span class="s">sink_hdfs</span>
</code></pre></div>
<p>关于 HDFS Sink配置说明，请参考 <a href="http://flume.apache.org/FlumeUserGuide.html#hdfs-sink">HDFS Sink</a></p>

<p><strong>说明：</strong></p>

<ol>
<li>通过 interceptors 往 header 里添加 timestamp，这样做，可以在 hdfs.path 引用系统内部的时间变量或者主机的 hostname。</li>
<li>通过设置 <code class="prettyprint">hdfs.inUsePrefix</code>，例如设置为 <code class="prettyprint">.</code>时，hdfs 会把该文件当做隐藏文件，以避免在 mr 过程中读到这些临时文件，引起一些错误</li>
<li>如果使用 lzo 压缩，则需要手动创建 lzo 索引，可以通过修改 HdfsSink 的代码，通过代码创建索引</li>
<li>FileChannel 的目录最好是和 spooldir 的数据目录处于不同磁盘。</li>
</ol>

<h2 id="示例4：spooldir-数据源，写入-hbase">示例4：spooldir 数据源，写入 HBase</h2>

<p>关于 HBase Sink 配置说明，请参考 <a href="http://flume.apache.org/FlumeUserGuide.html#hbasesink">HBase Sink</a></p>

<h1 id="4.-开发相关">4. 开发相关</h1>

<h2 id="4.1-编译源代码">4.1 编译源代码</h2>

<p>从 github 下载源代码并编译：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>git clone git@github.com:cloudera/flume-ng.git -b cdh4-1.4.0_4.7.0
<span class="nv">$ </span><span class="nb">cd </span>flume-ng
<span class="nv">$ </span>mvn install -DskipTests -Phadoop-2
</code></pre></div>
<p>如果提示找不到 hadoop-test 的 jar 包，则修改 pom.xml 中的版本，如改为 <code class="prettyprint">2.0.0-mr1-cdh4.7.0</code>，具体版本视你使用的分支版本而定，我这里是 cdh4.7.0。</p>

<p>如果提示找不到 uanodeset-parser 的 jarb，则在 pom.xml 中添加下面仓库：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;repository&gt;</span>
  <span class="nt">&lt;id&gt;</span>tempo-db<span class="nt">&lt;/id&gt;</span>
  <span class="nt">&lt;url&gt;</span>http://maven.tempo-db.com/artifactory/list/twitter/
  <span class="nt">&lt;/url&gt;</span>
  <span class="nt">&lt;snapshots&gt;</span>
    <span class="nt">&lt;enabled&gt;</span>false<span class="nt">&lt;/enabled&gt;</span>
  <span class="nt">&lt;/snapshots&gt;</span>
<span class="nt">&lt;/repository&gt;</span>
</code></pre></div>
<h1 id="5.-最佳实践">5. 最佳实践</h1>

<p>参考<a href="http://tech.meituan.com/mt-log-system-arch.html">基于Flume的美团日志收集系统(一)架构和设计</a>，列出一些最佳实践：</p>

<ul>
<li>模块命名规则：所有的 Source 以 src 开头，所有的 Channel 以 ch 开头，所有的 Sink 以 sink 开头；</li>
<li>模块之间内部通信统一使用 Avro 接口；</li>
<li>将日志采集系统系统分为三层：Agent 层，Collector 层和 Store 层，其中 Agent 层每个机器部署一个进程，负责对单机的日志收集工作；Collector 层部署在中心服务器上，负责接收Agent层发送的日志，并且将日志根据路由规则写到相应的 Store 层中；Store 层负责提供永久或者临时的日志存储服务，或者将日志流导向其它服务器。</li>
<li>扩展 MemoryChannel 和 FileChannel ，提供 DualChannel 的实现，以提供高吞吐和大缓存</li>
<li>监控 collector HdfsSink写数据到 hdfs 的速度、FileChannel 中拥堵的 events 数量，以及写 hdfs 状态（查看是否有 .tmp 文件生成）</li>
</ul>

<p>美团对 flume 的改进代码见 github：<a href="https://github.com/dashengju/mt-flume">https://github.com/dashengju/mt-flume</a></p>
</div>

      <!--
      <div id="pay" style="text-align:center;">
        ----EOF-----
        <br>
        <section>
  <h4>Sponsor</h4>
	<img src="http://javachen-rs.qiniudn.com/images/alipay.png" width="150/">
  <p class="small">觉得此博客对你有帮助，支付宝扫码赞助吧</p>
</section>

      </div>
      -->
      <p class="meta">
      	
            Categories:
      	    
          	<a class="btn btn-default btn-xs" href="/categories.html#hadoop">hadoop</a>
          
      	

      	
            Tags:
      	    
          	<a class="btn btn-default btn-xs" href="/tags.html#hadoop">hadoop</a>
          
      	
      </p>
	</article>

	<ul class="pager">
	  
	  <li class="previous"><a class="btn btn-xs" href="/2014/07/18/install-hdfs-ha-in-cdh" title="CDH中配置HDFS HA">&larr; Prev</a></li>
	  
	  
	  <li class="next"><a class="btn btn-xs" href="/2014/07/25/collect-log-to-hive" title="采集日志到Hive">Next &rarr;</a></li>
	  
	</ul>

  
<div id="comments">
  <div class="ds-thread" data-thread-key="/2014/07/22/flume-ng"  data-title="Flume-ng的原理和使用 - 夏天松的个人博客"></div>
</div>



</div>

        </div>
      </div>

      <footer class="footer text-center">
  <p>&copy; 2009-2015 <a href="/" target="_blank" title="86后，工作在深圳；Java程序员、Hadoop工程师；主要关注Java、Scala、Hadoop、Kettle、关注大数据处理技术。">夏天松</a>. With Help from <a href="//jekyllrb.com/" target="_blank" title="Transform your plain text into static websites and blogs.">Jekyll</a> and <a href="//getbootstrap.com/" target="_blank" title="Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.">Bootstrap</a>, theme from <a href="http://havee.me/" target="_blank" title="http://havee.me/">Havee</a>.

  <span style="float:right;"><a href="/about.html">About</a></span>

	
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




  </p>
  <div id="toTop" style="display: block;">
    <a href="#">▲</a><a href="#footer">▼</a>
  </div>
</footer>

    </div>

    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/core.js"></script>

    
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




    
    <!-- duoshuo Begin -->
    <script type="text/javascript">
      var duoshuoQuery = {short_name:"xiaotian120"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script>
    <!-- duoshuo End -->
    
  </body>
</html>
