<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>夏天松的个人博客</title>
  <link href="//atom.xml" rel="self"/>
  <link href=""/>
  <updated>2015-01-17T11:23:43+08:00</updated>
  <id></id>
  <author>
    <name>夏天松</name>
  </author>

  
  <entry>
    <title>Django中的模型</title>
    <link href="/2015/01/14/django-model"/>
    <updated>2015-01-14T00:00:00+08:00</updated>
    <id>/2015/01/14/django-model</id>
    <content type="html">通过《如何创建一个Django网站》大概清楚了如何创建一个简单的 Django 网站，这篇文章主要是在此基础上介绍 Django 中模型的定义方法以及模型之间存在的几种映射关系。

模型的定义


Django 中每一个 Model 都继承自 django.db.models.Model。
在 Model 当中每一个属性 attribute 都代表一个数据库字段。
通过 Django Model API 可以执行数据库的增删改查, 而不需要写一些数据库的查询语句。


在 如何创建一个Django网站 中创建的 List 模型定义如下：
from django.db import mod...</content>
  </entry>
  
  <entry>
    <title>AngularJS PhoneCat代码分析</title>
    <link href="/2015/01/09/angular-phonecat-examples"/>
    <updated>2015-01-09T00:00:00+08:00</updated>
    <id>/2015/01/09/angular-phonecat-examples</id>
    <content type="html">AngularJS 官方网站提供了一个用于学习的示例项目：PhoneCat。这是一个Web应用，用户可以浏览一些Android手机，了解它们的详细信息，并进行搜索和排序操作。

本文主要分析 AngularJS 官方网站提供的一个用于学习的示例项目 PhoneCat 的构建、测试过程以及代码的运行原理。希望能够对 PhoneCat 项目有一个更加深入全面的认识。这其中包括以下内容：


该项目如何运行起来的
该项目如何进行前端单元测试
AngularJS 相关代码分析


以下内容如有理解不正确，欢迎指正！

环境搭建

对于 PhoneCat 项目的开发环境和测试环境的搭建，官方网站上...</content>
  </entry>
  
  <entry>
    <title>AngularJS基本知识点</title>
    <link href="/2015/01/08/basic-concepts-of-angularjs"/>
    <updated>2015-01-08T00:00:00+08:00</updated>
    <id>/2015/01/08/basic-concepts-of-angularjs</id>
    <content type="html">AngularJS 是一个 MV* 框架，最适于开发客户端的单页面应用。它不是个功能库，而是用来开发动态网页的框架。它专注于扩展 HTML 的功能，提供动态数据绑定（data binding），而且它能跟其它框架（如 JQuery 等）合作融洽。

1. 一个简单示例

通过下面的示例代码，可以运行一个简单的 AngularJS 应用：
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;body&amp;gt;

&amp;lt;div ng-app=&amp;quot;&amp;quot;&amp;gt;
  &amp;lt;p&amp;gt;在输入框中尝试输入：&amp;lt;/p&amp;gt;
  &amp;lt;p&amp;gt;姓名：...</content>
  </entry>
  
  <entry>
    <title>Gradle构建多模块项目</title>
    <link href="/2015/01/07/build-multi-module-project-with-gradle"/>
    <updated>2015-01-07T00:00:00+08:00</updated>
    <id>/2015/01/07/build-multi-module-project-with-gradle</id>
    <content type="html">废话不多说，直接进入主题。

创建项目

首先创建项目，名称为 test：
mkdir test &amp;amp;&amp;amp; cd test
gradle init

这时候的项目结构如下：
➜  test  tree
.
├── build.gradle
├── gradle
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
└── settings.gradle

2 directories, 6 files

然后...</content>
  </entry>
  
  <entry>
    <title>使用Spring Boot和Gradle创建项目</title>
    <link href="/2015/01/06/build-app-with-spring-boot-and-gradle"/>
    <updated>2015-01-06T00:00:00+08:00</updated>
    <id>/2015/01/06/build-app-with-spring-boot-and-gradle</id>
    <content type="html">Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。

本文主要是记录使用 Spring Boot 和 Gradle 创建项目的过程，其中会包括 Spring Boot 的安装及使用方法，希望通过这篇文章能够快速搭建一个项目。

开发环境


操作系统: mac
JDK：1.7.0_60
Gradle：2.2.1


创建项目

你可以通过 Spring Initializr 来创建一个空的项目，也可以手动创建，这里我使用的是手动创建 ...</content>
  </entry>
  
  <entry>
    <title>Python中的多线程</title>
    <link href="/2014/12/23/thread-in-python"/>
    <updated>2014-12-23T00:00:00+08:00</updated>
    <id>/2014/12/23/thread-in-python</id>
    <content type="html">线程模块

Python 通过两个标准库 thread 和 threading 提供对线程的支持。Python 的 thread 模块是比较底层的模块，Python 的 threading 模块是对 thread 做了一些包装的，可以更加方便的被使用。

thread 模块提供的其他方法：


start_new_thread(function,args,kwargs=None)：生一个新线程，在新线程中用指定参数和可选的 kwargs 调用 function 函数
allocate_lock()：分配一个 LockType 类型的锁对象（注意，此时还没有获得锁）
interrupt_m...</content>
  </entry>
  
  <entry>
    <title>使用Django创建Blog</title>
    <link href="/2014/12/19/create-blog-using-django"/>
    <updated>2014-12-19T00:00:00+08:00</updated>
    <id>/2014/12/19/create-blog-using-django</id>
    <content type="html">本文参考 Part 1: Creating a blog system using django + markdown 使用 django、bootstrap3 创建一个支持 markdown 语法的简单 blog，这是一个很好的学习 django 的例子，基本上涉及了 django 的方方面面，本文中相关的源码见 github。

相对于原文做了一些修改：


基于 django1.7，修复了不兼容的配置
前端使用 bootstrap3，并修改了页面的一些内容
【TODO】代码中关键的地方添加注释，帮助理解


安装依赖
pip install markdown pygments dj...</content>
  </entry>
  
  <entry>
    <title>JPA的使用</title>
    <link href="/2014/12/02/some-usages-of-jpa"/>
    <updated>2014-12-02T00:00:00+08:00</updated>
    <id>/2014/12/02/some-usages-of-jpa</id>
    <content type="html">JPA，Java 持久化规范，是从EJB2.x以前的实体 Bean 分离出来的，EJB3 以后不再有实体 bean，而是将实体 bean 放到 JPA 中实现。

JPA 是 sun 提出的一个对象持久化规范，各 JavaEE 应用服务器自主选择具体实现，JPA 的设计者是 Hibernate 框架的作者，因此Hibernate作为Jboss服务器中JPA的默认实现，Oracle的Weblogic使用EclipseLink(以前叫TopLink)作为默认的JPA实现，IBM的Websphere和Sun的Glassfish默认使用OpenJPA(Apache的一个开源项目)作为其默认的JP...</content>
  </entry>
  
  <entry>
    <title>Hadoop集群部署权限总结</title>
    <link href="/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop"/>
    <updated>2014-11-25T00:00:00+08:00</updated>
    <id>/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop</id>
    <content type="html">这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项，包括 Hadoop 各个组件集成 kerberos、openldap 和 sentry 的过程。如果你想了解详细的过程，请参考本博客中其他的文章。

1. 开始之前

hadoop 集群一共有三个节点，每个节点的 ip、hostname、角色如下：
192.168.56.121 cdh1 NameNode、kerberos-server、ldap-server、sentry-store
192.168.56.122 cdh2 DataNode、yarn、hive、impala
192.168.56.123...</content>
  </entry>
  
  <entry>
    <title>Spring集成JPA2.0</title>
    <link href="/2014/11/24/spring-with-jpa2"/>
    <updated>2014-11-24T00:00:00+08:00</updated>
    <id>/2014/11/24/spring-with-jpa2</id>
    <content type="html">JPA 全称 Java Persistence API，是Java EE 5标准之一，是一个 ORM 规范，由厂商来实现该规范，目前有 Hibernate、OpenJPA、TopLink、EclipseJPA 等实现。Spring目前提供集成Hibernate、OpenJPA、TopLink、EclipseJPA四个JPA标准实现。

1. 集成方式

Spring提供三种方法集成JPA：


1. LocalEntityManagerFactoryBean：适用于那些仅使用JPA进行数据访问的项目。
2. 从JNDI中获取：用于从Java EE服务器中获取指定的EntityManage...</content>
  </entry>
  
  <entry>
    <title>Zookeeper配置Kerberos认证</title>
    <link href="/2014/11/18/config-kerberos-in-cdh-zookeeper"/>
    <updated>2014-11-18T00:00:00+08:00</updated>
    <id>/2014/11/18/config-kerberos-in-cdh-zookeeper</id>
    <content type="html">关于 Hadoop 集群上配置 kerberos 以及 ldap 的过程请参考本博客以下文章：


HDFS配置Kerberos认证
YARN配置Kerberos认证
Hive配置Kerberos认证
Impala配置Kerberos认证
Hadoop配置LDAP集成Kerberos


参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下：
192.168.56.121        cdh1     NameNode、Hive、ResourceManager、HBase、impala-state-stor...</content>
  </entry>
  
  <entry>
    <title>Impala和Hive集成Sentry</title>
    <link href="/2014/11/14/config-impala-and-hive-with-sentry"/>
    <updated>2014-11-14T00:00:00+08:00</updated>
    <id>/2014/11/14/config-impala-and-hive-with-sentry</id>
    <content type="html">本文主要记录 CDH 5.2 Hadoop 集群中配置 Impala 和 Hive 集成 Sentry 的过程，包括 Sentry 的安装、配置以及和 Impala、Hive 集成后的测试。

使用 Sentry 来管理集群的权限，需要先在集群上配置好 Kerberos。

关于 Hadoop 集群上配置 kerberos 以及 ldap 的过程请参考本博客以下文章：


HDFS配置Kerberos认证
YARN配置Kerberos认证
Hive配置Kerberos认证
Impala配置Kerberos认证
Hadoop配置LDAP集成Kerberos


Sentry 会安装在三个节...</content>
  </entry>
  
  <entry>
    <title>Hadoop配置LDAP集成Kerberos</title>
    <link href="/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop"/>
    <updated>2014-11-12T00:00:00+08:00</updated>
    <id>/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop</id>
    <content type="html">本文主要记录 cdh hadoop 集群集成 ldap 的过程，这里 ldap 安装的是 OpenLDAP 。LDAP 用来做账号管理，Kerberos作为认证。授权一般来说是由应用来决定的，通过在 LDAP 数据库中配置一些属性可以让应用程序来进行授权判断。

关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。

参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下：
192.168.56.121        cdh1     Kerber...</content>
  </entry>
  
  <entry>
    <title>Impala配置Kerberos认证</title>
    <link href="/2014/11/06/config-kerberos-in-cdh-impala"/>
    <updated>2014-11-06T00:00:00+08:00</updated>
    <id>/2014/11/06/config-kerberos-in-cdh-impala</id>
    <content type="html">关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。

关于 Kerberos 的安装和 YARN 配置 kerberos 认证，请参考 YARN配置kerberos认证。

关于 Kerberos 的安装和 Hive 配置 kerberos 认证，请参考 Hive配置kerberos认证。


请先完成 HDFS 、YARN、Hive 配置 Kerberos 认证，再来配置 Impala 集成 Kerberos 认证 ！


参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点...</content>
  </entry>
  
  <entry>
    <title>Hive配置Kerberos认证</title>
    <link href="/2014/11/06/config-kerberos-in-cdh-hive"/>
    <updated>2014-11-06T00:00:00+08:00</updated>
    <id>/2014/11/06/config-kerberos-in-cdh-hive</id>
    <content type="html">关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。

关于 Kerberos 的安装和 YARN 配置 kerberos 认证，请参考 YARN配置kerberos认证。


请先完成 HDFS 和 YARN 配置 Kerberos 认证，再来配置 Hive 集成 Kerberos 认证 ！


参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下：
192.168.56.121        cdh1     NameNode、Hive、Re...</content>
  </entry>
  
  <entry>
    <title>YARN配置Kerberos认证</title>
    <link href="/2014/11/05/config-kerberos-in-cdh-yarn"/>
    <updated>2014-11-05T00:00:00+08:00</updated>
    <id>/2014/11/05/config-kerberos-in-cdh-yarn</id>
    <content type="html">关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。


请先完成 HDFS 配置 Kerberos 认证，再来配置 YARN 集成 Kerberos 认证 ！


参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下：
192.168.56.121        cdh1     NameNode、Hive、ResourceManager、HBase、Kerberos Server
192.168.56.122        cdh2     ...</content>
  </entry>
  
  <entry>
    <title>HDFS配置Kerberos认证</title>
    <link href="/2014/11/04/config-kerberos-in-cdh-hdfs"/>
    <updated>2014-11-04T00:00:00+08:00</updated>
    <id>/2014/11/04/config-kerberos-in-cdh-hdfs</id>
    <content type="html">本文主要记录 CDH Hadoop 集群上配置 HDFS 集成 Kerberos 的过程，包括 Kerberos 的安装和 Hadoop 相关配置修改说明。


注意：

下面第一、二部分内容，摘抄自《Hadoop的kerberos的实践部署》，主要是为了对 Hadoop 的认证机制和 Kerberos 认证协议做个简单介绍。在此，对原作者表示感谢。


1. Hadoop 的认证机制

简单来说，没有做 kerberos 认证的 Hadoop，只要有 client 端就能够连接上。而且，通过一个有 root 的权限的内网机器，通过创建对应的 linux 用户，就能够得到 Hadoop ...</content>
  </entry>
  
  <entry>
    <title>Mac上使用homebrew安装PostgreSql</title>
    <link href="/2014/10/30/install-postgresql-on-mac-using-homebrew"/>
    <updated>2014-10-30T00:00:00+08:00</updated>
    <id>/2014/10/30/install-postgresql-on-mac-using-homebrew</id>
    <content type="html">安装

brew 安装 postgresql ：
$ brew install postgresql

查看安装的版本：
$ pg_ctl -V
pg_ctl (PostgreSQL) 9.3.5

初始化数据库：
$ initdb /usr/local/var/postgres
The files belonging to this database system will be owned by user &amp;quot;june&amp;quot;.
This user must also own the server process.

The database cluster will b...</content>
  </entry>
  
  <entry>
    <title>Django中的模板</title>
    <link href="/2014/10/30/django-template"/>
    <updated>2014-10-30T00:00:00+08:00</updated>
    <id>/2014/10/30/django-template</id>
    <content type="html">通过《如何创建一个Django网站》大概清楚了如何创建一个简单的 Django 网站，这篇文章主要是在此基础上介绍 Django 中模板相关的用法。

视图中使用模板

在《如何创建一个Django网站》中使用模板的方式如下：
from django.http import HttpResponse
import datetime

def current_datetime(request):
    now = datetime.datetime.now()
    html = &amp;quot;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;It is now %s.&amp;lt;/body&amp;...</content>
  </entry>
  
  <entry>
    <title>Impala查询功能测试</title>
    <link href="/2014/10/24/impala-query-table-tutorial"/>
    <updated>2014-10-24T00:00:00+08:00</updated>
    <id>/2014/10/24/impala-query-table-tutorial</id>
    <content type="html">关于 Impala 使用方法的一些测试，包括加载数据、查看数据库、聚合关联查询、子查询等等。

1. 准备测试数据

以下测试以 impala 用户来运行：
$ su - impala
-bash-4.1$ whoami
impala
$ hdfs dfs -ls /user
Found 5 items
drwxr-xr-x   - hdfs   hadoop          0 2014-09-22 18:36 /user/hdfs
drwxrwxrwt   - mapred hadoop          0 2014-07-23 21:37 /user/history
drwxr...</content>
  </entry>
  
  <entry>
    <title>当前数据仓库建设过程</title>
    <link href="/2014/10/23/hive-warehouse-in-2014"/>
    <updated>2014-10-23T00:00:00+08:00</updated>
    <id>/2014/10/23/hive-warehouse-in-2014</id>
    <content type="html">一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。

1. 数据采集和存储

采集数据之前，先要定义数据如何存放在 hadoop 以及一些相关约束。约束如下：


所有的日志数据都存放在 hdfs 上的 /logroot 路径下面
hive 中数据库命名方式为 dw_XXXX，例如：dw_srclog 存放外部来源的原始数据，dw_stat 存放统计结果的数据
原始数据都加工成为结构化的文本文件，字段分隔符统一使用制表符，并在 lzo 压缩之后上传到 hdfs 中。
hive 中使用外部表保存数据，数据存放在...</content>
  </entry>
  
  <entry>
    <title>CDH 5.2.0 的改变</title>
    <link href="/2014/10/20/cdh5.2-release"/>
    <updated>2014-10-20T00:00:00+08:00</updated>
    <id>/2014/10/20/cdh5.2-release</id>
    <content type="html">最近 CDH 5.2.0 发布了，想看看其做了哪些改进、带来哪些不兼容以及是否有必要升级现有的 hadoop 集群。

1. CDH 5.2.0 新特性

1.1. Apache Avro

Avro 版本使用1.7.6，重要的一些改变：


AVRO-1398。增加同步间隔，从16k 调整到64k，该参数可以在 mapreduce 的配置参数中通过 avro.mapred.sync.interval 参数来设置
AVRO-1355。schema 中不能包括相同的 field 名称。


1.2. Apache Hadoop

HDFS

提供新的功能：


HDFS Data at R...</content>
  </entry>
  
  <entry>
    <title>Spring源码整体架构</title>
    <link href="/2014/09/29/spring-source-codes"/>
    <updated>2014-09-29T00:00:00+08:00</updated>
    <id>/2014/09/29/spring-source-codes</id>
    <content type="html">前言

Spring 是一个开源框架，是为了解决企业应用程序开发复杂性而创建的。框架的主要优势之一就是其分层架构，分层架构允许您选择使用哪一个组件，同时为 J2EE 应用程序开发提供集成的框架。

从这篇文章开始，我讲开始阅读并介绍 Spring 源码的设计思想，希望能改对 Spring 框架有一个初步的全面的认识，并且学习其架构设计方面的一些理念和方法。

Spring 源码地址：https://github.com/spring-projects/spring-framework

概述

Spring的整体架构

Spring 总共有十几个组件，其中核心组件只有三个：Core、Co...</content>
  </entry>
  
  <entry>
    <title>编译Dubbo源码并测试</title>
    <link href="/2014/09/24/compile-and-test-dubbo"/>
    <updated>2014-09-24T00:00:00+08:00</updated>
    <id>/2014/09/24/compile-and-test-dubbo</id>
    <content type="html">Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，每天为2000+ 个服务提供3,000,000,000+ 次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用。 


项目主页：http://alibaba.github.io/dubbo-doc-static/Home-zh.htm 
项目源码：https://github.com/alibaba/dubbo


1. 安装

首先从 github 下载源代码并阅读 readme.md ，参考该文档，首先下载 opensesame，并编译：
$ git clone git...</content>
  </entry>
  
  <entry>
    <title>Mahout推荐引擎介绍</title>
    <link href="/2014/09/22/mahout-recommend-engine"/>
    <updated>2014-09-22T00:00:00+08:00</updated>
    <id>/2014/09/22/mahout-recommend-engine</id>
    <content type="html">Mahout 是一个来自 Apache 的、开源的机器学习软件库，他主要关注于推荐引擎（协同过滤）、聚类和分类。

推荐一般是基于物品或者用户进行推荐相关。

聚类是讲大量的事物组合为拥有类似属性的簇，借以在一些规模较大或难于理解的数据集上发现层次结构和顺序，以揭示一些有用的模式或让数据集更易于理解。

分类有助于判断一个新的输入或新的事物是否于以前观察到的模式相匹配，它通常还被用于筛选异常的行为或模式，来检测可疑的网络活动或欺骗行为。

推荐系统

推荐引擎算法应用最广的两大类：基于用户和基于物品的推荐。这两者都是协同过滤的范畴：仅仅通过了解用户于物品之间的关系进行推荐。这些技术无需了...</content>
  </entry>
  
  <entry>
    <title>使用Gradle构建项目</title>
    <link href="/2014/09/15/build-project-with-gradle"/>
    <updated>2014-09-15T00:00:00+08:00</updated>
    <id>/2014/09/15/build-project-with-gradle</id>
    <content type="html">Gradle 是一款基于 Groovy 语言、免费开源的构建工具，它既保持了 Maven 的优点，又通过使用 Groovy 定义的 DSL 克服了 Maven 中使用 XML 繁冗以及不灵活的缺点。

Gradle 官方网站：http://www.gradle.org/downloads

安装

一种方式是从 官方 下载解压然后配置环境变量。

Mac 上安装：
$ brew install gradle

测试是否安装成功：
$ gradle -v
------------------------------------------------------------
Gradle ...</content>
  </entry>
  
  <entry>
    <title>使用Groovy操作文件</title>
    <link href="/2014/09/12/file-operation-in-groovy"/>
    <updated>2014-09-12T00:00:00+08:00</updated>
    <id>/2014/09/12/file-operation-in-groovy</id>
    <content type="html">Java 读写文件比较麻烦，那 Groovy 操作文件又如何呢？

1. 读文件

读文件内容

在groovy中输出文件的内容：
println new File(&amp;quot;tmp.csv&amp;quot;).text  

上面代码非常简单，没有流的出现，没有资源关闭的出现，也没有异常控制的出现，所有的这些groovy已经搞定了。

读取每一行内容：
File file = new File(&amp;#39;tmp.csv&amp;#39;)
assert file.name == &amp;#39;tmp.csv&amp;#39;
assert ! file.isAbsolute()
assert file.pat...</content>
  </entry>
  
  <entry>
    <title>Llama的使用</title>
    <link href="/2014/09/09/llama"/>
    <updated>2014-09-09T00:00:00+08:00</updated>
    <id>/2014/09/09/llama</id>
    <content type="html">1. 介绍

Llama (Low Latency Application MAster) 是一个 Yarn 的  Application Master，用于协调 Impala 和 Yarn 之间的集群资源的管理和监控。Llama 使 Impala 能够获取、使用和释放资源配额，而不需要 Impala 使用 Yarn 管理的 container 进程。Llama 提供了 Thrift API 来和 Yarn 交互。

个人理解，Llama 的作用就是使 Impala 能够工作在 YARN 之上，使得 Impala 和 YARN 共享集群资源，提供低延迟的查询。


 Llama 官网地址...</content>
  </entry>
  
  <entry>
    <title>从零开始创建Grails应用</title>
    <link href="/2014/09/09/create-a-grails-app-step-by-step"/>
    <updated>2014-09-09T00:00:00+08:00</updated>
    <id>/2014/09/09/create-a-grails-app-step-by-step</id>
    <content type="html">本篇文章主要介绍如何从零开始一步一步创建一个 Grails 应用程序。整个过程中，你将学到如何改变 Grails 运行的端口，了解 Grails 应用的基础组成部分(领域类、控制器和视图)、指定字段的缺省值，以及其他许多内容。

1. 安装

下载压缩包然后解压或者通过rpm、deb发行包安装。

这里，我在 mac 上安装 grails ：
$ brew install grails

2. 创建应用

以 blog 为例，创建一个应用程序，在命令行输入 grails create-app blog ：
$ grails create-app blog
2014-9-9 10:43:2...</content>
  </entry>
  
  <entry>
    <title>Groovy语法介绍</title>
    <link href="/2014/09/05/about-groovy"/>
    <updated>2014-09-05T00:00:00+08:00</updated>
    <id>/2014/09/05/about-groovy</id>
    <content type="html">1. 介绍

Groovy 是基于 JRE 的脚本语言，和Perl，Python等等脚本语言一样，它能以快速简洁的方式来完成一些工作：如访问数据库，编写单元测试用例，快速实现产品原型等等。

Groovy 是由James Strachan 和 Bob McWhirter 这两位天才发明的（JSR 241 2004 年 3 月）。Groovy 完全以Java API为基础，使用了Java开发人员最熟悉的功能和库。Groovy 的语法近似Java，并吸收了 Ruby 的一些特点，因此 Groovy 在某些场合可以扮演一种 “咖啡伴侣”的角色。   

官网地址：http://groovy.c...</content>
  </entry>
  
  <entry>
    <title>安装Azkaban</title>
    <link href="/2014/08/25/install-azkaban"/>
    <updated>2014-08-25T00:00:00+08:00</updated>
    <id>/2014/08/25/install-azkaban</id>
    <content type="html">Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。

Azkaban 包括三个关键组件：


关系数据库（MySQL）
AzkabanWebServer
AzkabanExecutorServer




在 2.5版本之后，Azkaban 提供了两种模式来安装，一种是stand alone 的 “solo-server” 模式，另一种是两个 server 的模式。这里主要介绍第二种模式的安装方法。

...</content>
  </entry>
  
  <entry>
    <title>升级cdh4到cdh5</title>
    <link href="/2014/08/19/upgrading-from-cdh4-to-cdh5"/>
    <updated>2014-08-19T00:00:00+08:00</updated>
    <id>/2014/08/19/upgrading-from-cdh4-to-cdh5</id>
    <content type="html">现在的集群安装的是 cdh4.7 并配置了 NameNode HA，现在需要将其升级到 cdh5.2，升级原因这里不做说明。

1. 不兼容的变化

升级前，需要注意 cdh5 有哪些不兼容的变化，具体请参考：Apache Hadoop Incompatible Changes。这里列出一些关键的地方：

1.1 YARN 的变化


YARN_HOME 属性修改为HADOOP_YARN_HOME
yarn-site.xml 中做如下改变：


mapreduce.shuffle to mapreduce_shuffle 修改为 yarn.nodemanager.aux-services...</content>
  </entry>
  
  <entry>
    <title>Sqoop导入关系数据库到Hive</title>
    <link href="/2014/08/04/import-data-to-hive-with-sqoop"/>
    <updated>2014-08-04T00:00:00+08:00</updated>
    <id>/2014/08/04/import-data-to-hive-with-sqoop</id>
    <content type="html">Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。

1. 安装 Sqoop

使用 rpm 安装即可。
yum install sqoop sqoop-metastore -y


安装完之后需要下载 mysql jar 包到 sqoop 的 lib 目录。


这里使用 hive 的 metastore 的 mysql 数据库作为关系数据库，以 TBLS 表为例，该表结构和数据如下：
mysql&amp;gt; select * from TBLS limit 3;
+------+...</content>
  </entry>
  
  <entry>
    <title>2014年7月总结</title>
    <link href="/2014/07/31/summary-of-july-in-2014"/>
    <updated>2014-07-31T00:00:00+08:00</updated>
    <id>/2014/07/31/summary-of-july-in-2014</id>
    <content type="html">在休息了将近三个月之后，7月9日终于开始上班了，新的工作还是和 hadoop 相关。7月主要的工作内容如下：


搭建新的 hadoop 集群，hadoop 版本为 CDH4.7.0，并配置 NameNode 的 QJM HA 方案。配置 HA 方法见 CDH 中配置 HDFS HA
购买了三本书：


mahout实战
机器学习实战
这才是搜索引擎

调研了 flume-ng 日志采集方案


大众点评的大数据实践
analyzing-twitter-data-with-hadoop
Hadoop Analysis of Apache Logs Using Flume-NG, Hive...</content>
  </entry>
  
  <entry>
    <title>Impala新特性</title>
    <link href="/2014/07/29/new-features-in-impala"/>
    <updated>2014-07-29T00:00:00+08:00</updated>
    <id>/2014/07/29/new-features-in-impala</id>
    <content type="html">本文主要整理一下 Impala 每个版本的新特性，方便了解 Impala 做了哪些改进、修复了哪些 bug。

Impala 目前最新版本为 1.4.0，其下载地址为：http://archive.cloudera.com/impala/redhat/6/x86_64/impala/

不得不说的事情：


1.3.1 用于 CDH4
1.4.0 用于 CDH5


1.4.0


CDH5 中增加 DECIMAL 数据类型，可以设置精度，其语法为：DECIMAL[(precision[,scale])]
CDH5 中，impala 可以使用 HDFS 缓存特性加快频繁访问的数据的速度，...</content>
  </entry>
  
  <entry>
    <title>Phoenix Quick Start</title>
    <link href="/2014/07/28/phoenix-quick-start"/>
    <updated>2014-07-28T00:00:00+08:00</updated>
    <id>/2014/07/28/phoenix-quick-start</id>
    <content type="html">1. 介绍

Phoenix 是 Salesforce.com 开源的一个 Java 中间件，可以让开发者在Apache HBase 上执行 SQL 查询。Phoenix完全使用Java编写，代码位于 GitHub 上，并且提供了一个客户端可嵌入的 JDBC 驱动。


根据项目所述，Phoenix 被 Salesforce.com 内部使用，对于简单的低延迟查询，其量级为毫秒；对于百万级别的行数来说，其量级为秒。Phoenix 并不是像 HBase 那样用于 map-reduce job 的，而是通过标准化的语言来访问 HBase 数据的。

Phoenix 为 HBase 提供 SQ...</content>
  </entry>
  
  <entry>
    <title>采集日志到Hive</title>
    <link href="/2014/07/25/collect-log-to-hive"/>
    <updated>2014-07-25T00:00:00+08:00</updated>
    <id>/2014/07/25/collect-log-to-hive</id>
    <content type="html">我们现在的需求是需要将线上的日志以小时为单位采集并存储到 hive 数据库中，方便以后使用  mapreduce 或者 impala 做数据分析。为了实现这个目标调研了 flume 如何采集数据到 hive，其他的日志采集框架尚未做调研。

日志压缩

flume中有个 HdfsSink 组件，其可以压缩日志进行保存，故首先想到我们的日志应该以压缩的方式进行保存，遂选择了 lzo 的压缩格式，HdfsSink 的配置如下:
agent-1.sinks.sink_hdfs.channel = ch-1
agent-1.sinks.sink_hdfs.type = hdfs
agent-1....</content>
  </entry>
  
  <entry>
    <title>Flume-ng的原理和使用</title>
    <link href="/2014/07/22/flume-ng"/>
    <updated>2014-07-22T00:00:00+08:00</updated>
    <id>/2014/07/22/flume-ng</id>
    <content type="html">1. 介绍

Flume 是 Cloudera 提供的日志收集系统，具有分布式、高可靠、高可用性等特点，对海量日志采集、聚合和传输，Flume 支持在日志系统中定制各类数据发送方，同时，Flume提供对数据进行简单处理，并写到各种数据接受方的能力。

Flume 使用 java 编写，其需要运行在 Java1.6 或更高版本之上。


官方网站：http://flume.apache.org/
用户文档：http://flume.apache.org/FlumeUserGuide.html
开发文档：http://flume.apache.org/FlumeDeveloperGuide....</content>
  </entry>
  
  <entry>
    <title>CDH中配置HDFS HA</title>
    <link href="/2014/07/18/install-hdfs-ha-in-cdh"/>
    <updated>2014-07-18T00:00:00+08:00</updated>
    <id>/2014/07/18/install-hdfs-ha-in-cdh</id>
    <content type="html">最近又安装 hadoop 集群， 故尝试了一下配置 HDFS 的 HA，这里使用的是 QJM 的 HA 方案。

关于 hadoop 集群的安装部署过程你可以参考 使用yum安装CDH Hadoop集群 或者 手动安装 hadoop 集群的过程。

集群规划

我一共安装了三个节点的集群，对于 HA 方案来说，三个节点准备安装如下服务：


cdh1：hadoop-hdfs-namenode(primary) 、hadoop-hdfs-journalnode、hadoop-hdfs-zkfc
cdh2：hadoop-hdfs-namenode(standby)、hadoop-hdfs-j...</content>
  </entry>
  
  <entry>
    <title>手动安装Hadoop集群的过程</title>
    <link href="/2014/07/17/manual-install-cdh-hadoop"/>
    <updated>2014-07-17T00:00:00+08:00</updated>
    <id>/2014/07/17/manual-install-cdh-hadoop</id>
    <content type="html">最近又安装 Hadoop 集群，由于一些原因，没有使用 Hadoop 管理工具或者自动化安装脚本来安装集群，而是手动一步步的来安装，本篇文章主要是记录我手动安装 Hadoop 集群的过程，给大家做个参考。

这里所说的手动安装，是指一步步的通过脚本来安装集群，并不是使用一键安装脚本或者一些管理界面来安装。

开始之前，还是说明一下环境：


操作系统：CentOs6.4
CDH版本：4.7.0
节点数：4个


在开始之前，你可以看看我以前写的一篇文章 使用yum安装CDH Hadoop集群，因为有些细节已经为什么这样做我不会在这篇文章中讲述。

一些准备工作

在开始前，先选择一个节点...</content>
  </entry>
  
  <entry>
    <title>Spark安装和使用</title>
    <link href="/2014/07/01/spark-install-and-usage"/>
    <updated>2014-07-01T00:00:00+08:00</updated>
    <id>/2014/07/01/spark-install-and-usage</id>
    <content type="html">本文主要记录 Spark 的安装过程配置过程并测试 Spark 的一些基本使用方法。为了方便，这里使用 CDH 的 yum 源方式来安装 Spark，注意本文安装的 Spark 版本为 1.1。


操作系统：CentOs 6.4
CDH 版本：5.2.0
Spark 版本：1.1


关于 yum 源的配置以及 hadoop 的安装，请参考使用yum安装CDH Hadoop集群。

1. Spark 安装

选择一个节点来安装 Spark ，首先查看 Spark 相关的包有哪些：
$ yum list |grep spark
spark-core.noarch             ...</content>
  </entry>
  
  <entry>
    <title>HBase中的一些注意事项</title>
    <link href="/2014/06/26/some-tips-about-hbase"/>
    <updated>2014-06-26T00:00:00+08:00</updated>
    <id>/2014/06/26/some-tips-about-hbase</id>
    <content type="html">1. 安装集群前


配置SSH无密码登陆
DNS。HBase使用本地 hostname 才获得IP地址，正反向的DNS都是可以的。你还可以设置 hbase.regionserver.dns.interface 来指定主接口，设置 hbase.regionserver.dns.nameserver 来指定nameserver，而不使用系统带的
安装NTP服务，并配置和检查crontab是否生效
操作系统调优，包括最大文件句柄，nproc hard 和 soft limits等等
conf/hdfs-site.xml里面的 dfs.datanode.max.xcievers 参数，至少要有...</content>
  </entry>
  
  <entry>
    <title>MapReduce任务参数调优</title>
    <link href="/2014/06/24/tuning-in-mapreduce"/>
    <updated>2014-06-24T00:00:00+08:00</updated>
    <id>/2014/06/24/tuning-in-mapreduce</id>
    <content type="html">本文主要记录Hadoop 2.x版本中MapReduce参数调优，不涉及Yarn的调优。

Hadoop的默认配置文件（以cdh5.0.1为例）：


core-default.xml
hdfs-default.xml
mapred-default.xml



说明：

在hadoop2中有些参数名称过时了，例如原来的mapred.reduce.tasks改名为mapreduce.job.reduces了，当然，这两个参数你都可以使用，只是第一个参数过时了。


1. 操作系统调优


增大打开文件数据和网络连接上限，调整内核参数net.core.somaxconn，提高读写速度和网络...</content>
  </entry>
  
  <entry>
    <title>MapReduce任务运行过程</title>
    <link href="/2014/06/24/the-running-process-of-mapreduce-job"/>
    <updated>2014-06-24T00:00:00+08:00</updated>
    <id>/2014/06/24/the-running-process-of-mapreduce-job</id>
    <content type="html">下图是MapReduce任务运行过程的一个图：



Map-Reduce的处理过程主要涉及以下四个部分：


客户端Client：用于提交Map-reduce任务job
JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker
TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker
HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件


上图中主要包括以下过程：


提交作业
作业初始化
任务分配
执行任务
进度和状态更...</content>
  </entry>
  
  <entry>
    <title>HBase和Cassandra比较</title>
    <link href="/2014/06/24/hbase-vs-cassandra"/>
    <updated>2014-06-24T00:00:00+08:00</updated>
    <id>/2014/06/24/hbase-vs-cassandra</id>
    <content type="html">HBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。  

Cassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google   Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。

HBase vs Cassandra...</content>
  </entry>
  
  <entry>
    <title>Hive中的排序语法</title>
    <link href="/2014/06/22/sort-in-hive-query"/>
    <updated>2014-06-22T00:00:00+08:00</updated>
    <id>/2014/06/22/sort-in-hive-query</id>
    <content type="html">ORDER BY

hive中的ORDER BY语句和关系数据库中的sql语法相似。他会对查询结果做全局排序，这意味着所有的数据会传送到一个Reduce任务上，这样会导致在大数量的情况下，花费大量时间。

与数据库中 ORDER BY 的区别在于在hive.mapred.mode = strict模式下，必须指定 limit 否则执行会报错。
hive&amp;gt; set hive.mapred.mode=strict;
hive&amp;gt; select * from test order by id;
FAILED: SemanticException 1:28 In strict mode...</content>
  </entry>
  
  <entry>
    <title>Lucene介绍</title>
    <link href="/2014/06/21/the-introduction-of-lucene"/>
    <updated>2014-06-21T00:00:00+08:00</updated>
    <id>/2014/06/21/the-introduction-of-lucene</id>
    <content type="html">1. Lucene是什么

Lucene 是一个开源的、成熟的全文索引与信息检索(IR)库，采用Java实现。信息检索式指文档搜索、文档内信息搜索或者文档相关的元数据搜索等操作。Lucene是apache软件基金会项目组的一个子项目，是一个开放源代码的全文检索引擎工具包，即它不是一个完整的搜索应用程序，而是为你的应用程序提供索引和搜索功能。

Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。全文检索（Full-Text Retrieval）是指以文本作为检索对象，找出含有指定词汇的文本。全面、准...</content>
  </entry>
  
  <entry>
    <title>Storm集群安装部署步骤</title>
    <link href="/2014/06/19/how-to-install-and-deploy-a-storm-cluster"/>
    <updated>2014-06-19T00:00:00+08:00</updated>
    <id>/2014/06/19/how-to-install-and-deploy-a-storm-cluster</id>
    <content type="html">开始学习Storm，本文主要记录Storm集群安装部署步骤，不包括对Storm的介绍。

安装storm集群，需要依赖以下组件：


Zookeeper
Python
Zeromq
Storm
JDK
JZMQ


故安装过程根据上面的组件分为以下几步：


安装JDK
安装Zookeeper集群
安装Python及依赖
安装Storm


另外，操作系统环境为：Centos6.4，安装用户为：root。

1. 安装JDK

安装jdk有很多方法，可以参考文博客使用yum安装CDH Hadoop集群中的jdk安装步骤，需要说明的是下面的zookeeper集群安装方法也可以参考此文。

...</content>
  </entry>
  
  <entry>
    <title>Effective Java 笔记</title>
    <link href="/2014/06/17/note-about-effective-java"/>
    <updated>2014-06-17T00:00:00+08:00</updated>
    <id>/2014/06/17/note-about-effective-java</id>
    <content type="html">创建和销毁对象

NO.1 考虑用静态工厂方法代替构造函数

静态工厂方法好处：


1、构造函数有命名的限制，而静态方法有自己的名字，更加易于理解。
2、静态工厂方法在每次调用的时候不要求创建一个新的对象。这种做法对于一个要频繁创建相同对象的程序来说，可以极大的提高性能。它使得一个类可以保证是一个singleton；他使非可变类可以保证“不会有两个相等的实例存在”。
3、静态工厂方法在选择返回类型时有更大的灵活性。使用静态工厂方法，可以通过调用方法时使用不同的参数创建不同类的实例，还可以创建非公有类的对象，这就封装了类的实现细节。
4、在创建参数化类型实例的时候，他们使代码变的更加简洁...</content>
  </entry>
  
  <entry>
    <title>HBase源码分析：HTable put过程</title>
    <link href="/2014/06/13/hbase-code-about-htable-put"/>
    <updated>2014-06-13T00:00:00+08:00</updated>
    <id>/2014/06/13/hbase-code-about-htable-put</id>
    <content type="html">HBase版本：0.94.15-cdh4.7.0

在 HBase中，大部分的操作都是在RegionServer完成的，Client端想要插入、删除、查询数据都需要先找到相应的 RegionServer。什么叫相应的RegionServer？就是管理你要操作的那个Region的RegionServer。Client本身并 不知道哪个RegionServer管理哪个Region，那么它是如何找到相应的RegionServer的？本文就是在研究源码的基础上了解这个过程。

首先来看看写过程的序列图：



客户端代码

1、put方法

HTable的put有两个方法：
public voi...</content>
  </entry>
  
  <entry>
    <title>Hive Over HBase的介绍</title>
    <link href="/2014/06/12/intro-of-hive-over-hbase"/>
    <updated>2014-06-12T00:00:00+08:00</updated>
    <id>/2014/06/12/intro-of-hive-over-hbase</id>
    <content type="html">Hive Over HBase是基于Hive的HQL查询引擎支持对hbase表提供及时查询的功能，它并不是将hql语句翻译成mapreduce来运行，其响应时间在秒级别。

特性

支持的字段类型：

boolean, tinyint, smallint, int, bigint, float, double, string, struct
(当hbase中的rowkey字段为struct类型，请将子字段定义为string类型，同时指定表的collection items terminated分隔字符以及各字段的长度参数:hbase.rowkey.column.length)

支持的s...</content>
  </entry>
  
  <entry>
    <title>HBase客户端实现并行扫描</title>
    <link href="/2014/06/12/hbase-parallel-client-scanner"/>
    <updated>2014-06-12T00:00:00+08:00</updated>
    <id>/2014/06/12/hbase-parallel-client-scanner</id>
    <content type="html">HBase中有一个类可以实现客户端扫描数据，叫做ClientScanner，该类不是并行的，有没有办法实现一个并行的扫描类，加快扫描速度呢？

如果是一个Scan，我们可以根据startkey和stopkey将其拆分为多个子Scan，然后让这些Scan并行的去查询数据，然后分别返回执行结果。

实现方式

说明：我使用的HBase版本为：cdh4-0.94.15_4.7.0。

在org.apache.hadoop.hbase.client创建ParallelClientScanner类，代码如下：
package org.apache.hadoop.hbase.client;

imp...</content>
  </entry>
  
  <entry>
    <title>HBase实现简单聚合计算</title>
    <link href="/2014/06/12/hbase-aggregate-client"/>
    <updated>2014-06-12T00:00:00+08:00</updated>
    <id>/2014/06/12/hbase-aggregate-client</id>
    <content type="html">本文主要记录如何通过打补丁的方式将“hbase中实现简单聚合计算”的特性引入hbase源代码中，并介绍通过命令行和java代码的使用方法。

支持的简单聚合计算，包括：


rowcount
min
max
sum
std
avg
median


1、 下载并编译hbase源代码

我这里使用的HBase源代码版本是：cdh4-0.94.6_4.3.0，如果你使用其他版本，有可能patch打不上。

2、 引入patch

基于提交日志add-aggregate-support-in-hbase-shell生成patch文件，然后打patch，或者也可以使用其他方法：
$ git ap...</content>
  </entry>
  
  <entry>
    <title>Hive中数据的加载和导出</title>
    <link href="/2014/06/09/hive-data-manipulation-language"/>
    <updated>2014-06-09T00:00:00+08:00</updated>
    <id>/2014/06/09/hive-data-manipulation-language</id>
    <content type="html">关于 Hive DML 语法，你可以参考 apache 官方文档的说明:Hive Data Manipulation Language。

apache的hive版本现在应该是 0.13.0，而我使用的 hadoop 版本是 CDH5.0.1，其对应的 hive 版本是 0.12.0。故只能参考apache官方文档来看 cdh5.0.1 实现了哪些特性。

因为 hive 版本会持续升级，故本篇文章不一定会和最新版本保持一致。

1. 准备测试数据

首先创建普通表：
create table test(id int, name string) ROW FORMAT DELIMITED ...</content>
  </entry>
  
  <entry>
    <title>Hive中的FetchTask任务</title>
    <link href="/2014/06/09/fetchtask-in-hive"/>
    <updated>2014-06-09T00:00:00+08:00</updated>
    <id>/2014/06/09/fetchtask-in-hive</id>
    <content type="html">Hive中有各种各样的Task任务，其中FetchTask算是最简单的一种了。FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。当你执行简单的select * with limit语句的时候，其不会运行mapreduce任务。

例如，运行下面语句不会出现mapreduce任务（说明：t表有一个字段，id为int类型，该表没有数据）：
hive&amp;gt; select * from t limit 1;            
OK
Time taken: 2.466 seconds

去掉limit语句，再执行一次，结果如下：
hi...</content>
  </entry>
  
  <entry>
    <title>使用Scrapy爬取知乎网站</title>
    <link href="/2014/06/08/using-scrapy-to-cralw-zhihu"/>
    <updated>2014-06-08T00:00:00+08:00</updated>
    <id>/2014/06/08/using-scrapy-to-cralw-zhihu</id>
    <content type="html">本文主要记录使用使用 Scrapy 登录并爬取知乎网站的思路。Scrapy的相关介绍请参考 使用Scrapy抓取数据。

相关代码，见 https://github.com/javachen/scrapy-zhihu-github ，在阅读这部分代码之前，请先了解 Scrapy 的一些基本用法。

使用cookie模拟登陆

关于 cookie 的介绍和如何使用 python 实现模拟登陆，请参考python爬虫实践之模拟登录。

从这篇文章你可以学习到如何获取一个网站的 cookie 信息。下面所讲述的方法就是使用 cookie 来模拟登陆知乎网站并爬取用户信息。

一个模拟登陆知乎网...</content>
  </entry>
  
  <entry>
    <title>MongoDB介绍</title>
    <link href="/2014/06/06/about-mongodb"/>
    <updated>2014-06-06T00:00:00+08:00</updated>
    <id>/2014/06/06/about-mongodb</id>
    <content type="html">MongoDB 是一个开源的，高性能，无模式（或者说是模式自由），使用 C++ 语言编写的面向文档的数据库。正因为 MongoDB 是面向文档的，所以它可以管理类似 JSON 的文档集合。又因为数据可以被嵌套到复杂的体系中并保持可以查询可索引，这样一来，应用程序便可以以一种更加自然的方式来为数据建模。

官方网站：http://www.mongodb.org/

MongoDB介绍

所谓“面向集合”（Collenction-Orented），意思是数据被分组存储在数据集中，被称为一个集合（Collenction)。每个集合在数据库中都有一个唯一的标识名，并且可以包含无限数目的文档。集合...</content>
  </entry>
  
  <entry>
    <title>不用Cloudera Manager安装Cloudera Search</title>
    <link href="/2014/06/03/install_cloudera_search_without_cm"/>
    <updated>2014-06-03T00:00:00+08:00</updated>
    <id>/2014/06/03/install_cloudera_search_without_cm</id>
    <content type="html">Cloudera Search 用来在 hadoop 基础上建立索引和全文检索，Cloudera Search 有两种安装方法，本文是不使用 Cloudera Manager 来安装。

Cloudera Search介绍

Cloudera Search 核心部件包括 Hadoop 和 Solr，后者建立在 Lucene 之上；而 Hadoop 也正是在06年正式成为 Lucene 的一个子项目而发展起来的。

通过 Tika, Cloudera Search 支持大量的被广泛使用的文件格式；除此之外，Cloudera Search 还支持很多其他在Hadoop应用中常用的数据，譬如 ...</content>
  </entry>
  
  <entry>
    <title>关于CAP理论的一些笔记</title>
    <link href="/2014/05/30/note-about-brewers-cap-theorem"/>
    <updated>2014-05-30T00:00:00+08:00</updated>
    <id>/2014/05/30/note-about-brewers-cap-theorem</id>
    <content type="html">CAP的概念

2000年，Eric Brewer教授在ACM分布式计算年会上指出了著名的CAP理论：


分布式系统不可能同时满足一致性(C: Consistency)，可用性(A: Availability)和分区容错性(P: Tolerance of network Partition)这三个需求。大约两年后，Seth Gilbert 和 Nancy lynch两人证明了CAP理论的正确性。


三者的含义如下：


Consistency：一致性，一个服务是一致的完整操作或完全不操作（A service that is consistent operates fully or n...</content>
  </entry>
  
  <entry>
    <title>使用Scrapy抓取数据</title>
    <link href="/2014/05/24/using-scrapy-to-cralw-data"/>
    <updated>2014-05-24T00:00:00+08:00</updated>
    <id>/2014/05/24/using-scrapy-to-cralw-data</id>
    <content type="html">Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。


官方主页： http://www.scrapy.org/
中文文档：Scrapy 0.22 文档
GitHub项目主页：https://github.com/scrapy/scrapy


Scrapy 使用了 Twisted 异步网络库来处理网络通讯。整体架构大致如下（注：图片来自互联网）：



Scrapy主要包括了以下组件：


引擎，用来处理整个系统的数据流处理，触发事务。
调度器，用来接受引擎...</content>
  </entry>
  
  <entry>
    <title>Nutch介绍及使用</title>
    <link href="/2014/05/20/nutch-intro"/>
    <updated>2014-05-20T00:00:00+08:00</updated>
    <id>/2014/05/20/nutch-intro</id>
    <content type="html">1. Nutch介绍

Nutch是一个开源的网络爬虫项目，更具体些是一个爬虫软件，可以直接用于抓取网页内容。

现在Nutch分为两个版本，1.x和2.x。1.x最新版本为1.7，2.x最新版本为2.2.1。两个版本的主要区别在于底层的存储不同。

1.x版本是基于Hadoop架构的，底层存储使用的是HDFS，而2.x通过使用Apache Gora，使得Nutch可以访问HBase、Accumulo、Cassandra、MySQL、DataFileAvroStore、AvroStore等NoSQL。

2. 编译Nutch

Nutch1.x从1.7版本开始不再提供完整的部署文件，只提...</content>
  </entry>
  
  <entry>
    <title>Python开发框架Flask</title>
    <link href="/2014/05/11/flask-intro"/>
    <updated>2014-05-11T00:00:00+08:00</updated>
    <id>/2014/05/11/flask-intro</id>
    <content type="html">1. Flask介绍

Flask 是一个基于Python的微型的web开发框架。虽然Flask是微框架，不过我们并不需要像别的微框架建议的那样把所有代码都写到单文件中。毕竟微框架真正的含义是简单和短小。



关于Flask值得知道的一些事：


Flask由Armin Ronacher于2010年创建。
Flask的灵感来自Sinatra。（Sinatra是一个极力避免小题大作的创建web应用的Ruby框架。）
Flask 依赖两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 工具集。
Flask遵循“约定优于配置”以及合理的默认值原则。


默认情况下，Flask...</content>
  </entry>
  
  <entry>
    <title>bower介绍</title>
    <link href="/2014/05/10/bower-intro"/>
    <updated>2014-05-10T00:00:00+08:00</updated>
    <id>/2014/05/10/bower-intro</id>
    <content type="html">1. bower介绍

bower是用于web前端开发的包管理器。对于前端包管理方面的问题，它提供了一套通用、客观的解决方案。它通过一个API暴露包之间的依赖模型，这样更利于使用更合适的构建工具。bower没有系统级的依赖，在不同app之间也不互相依赖，依赖树是扁平的。



bower运行在Git之上，它将所有包都视作一个黑盒子。任何类型的资源文件都可以打包为一个模块，并且可以使用任何规范（例如：AMD、CommonJS等）。

包管理工具一般有以下的功能：


注册机制：每个包需要确定一个唯一的 ID 使得搜索和下载的时候能够正确匹配，所以包管理工具需要维护注册信息，可以依赖其他平台...</content>
  </entry>
  
  <entry>
    <title>All Things Markdown</title>
    <link href="/2014/04/24/all-things-markdown"/>
    <updated>2014-04-24T00:00:00+08:00</updated>
    <id>/2014/04/24/all-things-markdown</id>
    <content type="html">目录


概述
特点
语法
编辑器
浏览器插件
实现版本
参考资料


概述

Markdown 是一种轻量级标记语言，创始人为约翰·格鲁伯（John Gruber）和亚伦·斯沃茨（Aaron Swartz）。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档”。这种语言吸收了很多在电子邮件中已有的纯文本标记的特性。

特点

兼容 HTML

要在Markdown中输写HTML区块元素，比如&amp;lt;div&amp;gt;、&amp;lt;table&amp;gt;、&amp;lt;pre&amp;gt;、&amp;lt;p&amp;gt; 等标签，必须在前后加上空行与其它内容区隔开，还要求它们的开始...</content>
  </entry>
  
  <entry>
    <title>重装Mac系统之后</title>
    <link href="/2014/04/23/after-reinstall-mac"/>
    <updated>2014-04-23T00:00:00+08:00</updated>
    <id>/2014/04/23/after-reinstall-mac</id>
    <content type="html">本文主要记录重装Mac系统之后的一些软件安装和环境变量配置。

系统偏好配置

设置主机名：
$ sudo scutil --set HostName june－mac

设置鼠标滚轮滑动的方向：系统偏好设置－－&amp;gt;鼠标－－&amp;gt;&amp;quot;滚动方向：自然&amp;quot;前面的勾去掉

显示/隐藏Mac隐藏文件：
defaults write com.apple.finder AppleShowAllFiles -bool true  #显示Mac隐藏文件的命令
defaults write com.apple.finder AppleShowAllFiles -bool false ...</content>
  </entry>
  
  <entry>
    <title>Ubuntu系统编译Bigtop</title>
    <link href="/2014/04/17/building-bigtop-on-ubuntu"/>
    <updated>2014-04-17T00:00:00+08:00</updated>
    <id>/2014/04/17/building-bigtop-on-ubuntu</id>
    <content type="html">1. 安装系统依赖

系统更新并安装新的包
sudo apt-get update

sudo apt-get install -y cmake git-core git-svn subversion checkinstall build-essential dh-make debhelper ant ant-optional autoconf automake liblzo2-dev libzip-dev sharutils libfuse-dev reprepro libtool libssl-dev asciidoc xmlto ssh curl

sudo apt-get ins...</content>
  </entry>
  
  <entry>
    <title>Java笔记：Java内容模型</title>
    <link href="/2014/04/09/note-about-jvm-memery-model"/>
    <updated>2014-04-09T00:00:00+08:00</updated>
    <id>/2014/04/09/note-about-jvm-memery-model</id>
    <content type="html">基本概念

《深入理解Java内容模型》详细讲解了java的内存模型，这里对其中的一些基本概念做个简单的笔记。以下内容摘自 《深入理解Java内存模型》读书总结

1. 并发

定义：即，并发(同时)发生。在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理机上运行，但任一个时刻点上只有一个程序在处理机上运行。

并发需要处理两个关键问题：线程之间如何通信及线程之间如何同步。


通信 —— 是指线程之间如何交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。
同步—— 是指程序用于控制不同线程之间操作发生相对顺序的机制...</content>
  </entry>
  
  <entry>
    <title>RHEL系统下安装atlassian-jira-5</title>
    <link href="/2014/04/09/install-jira5-on-rhel-system"/>
    <updated>2014-04-09T00:00:00+08:00</updated>
    <id>/2014/04/09/install-jira5-on-rhel-system</id>
    <content type="html">部署环境


操作系统：RHEL 6.4 x86_64
Jira版本：atlassian-jira-5.2.11-x64.bin
安装路径:/opt/atlassian/jira/
数据保存路径：/opt/atlassian/application-data/jira
安装用户：jira
数据库：postgresql
JDK：1.6.0_43


jira下载页面：https://www.atlassian.com/software/jira/download

安装步骤

运行安装文件
$ . atlassian-jira-5.2.11-x64.bin

在安装过程中会出现选项：

确...</content>
  </entry>
  
  <entry>
    <title>PostgreSQL测试工具PGbench</title>
    <link href="/2014/04/08/a-benchmark-tool-on-postgresql"/>
    <updated>2014-04-08T00:00:00+08:00</updated>
    <id>/2014/04/08/a-benchmark-tool-on-postgresql</id>
    <content type="html">pgbench 是一个简单的给 PostgreSQL 做性能测试的程序。它反复运行同样的 SQL 命令序列，可能是在多个并发数据库会话上头，然后检查平均的事务速度（每秒的事务数 tps）。缺省的时候，pgbench 测试一个（松散的）接近 TPC-B 的情况，每个事务包括五个 SELECT，UPDATE，和 INSERT命令。不过，我们可以很轻松地使用自己的事务脚本文件来实现其它情况。

典型的输出看上去会是这样：
transaction type: TPC-B (sort of)
scaling factor: 10
number of clients: 10
number of tr...</content>
  </entry>
  
  <entry>
    <title>PostgreSQL监控指标</title>
    <link href="/2014/04/07/some-metrics-in-postgresql"/>
    <updated>2014-04-07T00:00:00+08:00</updated>
    <id>/2014/04/07/some-metrics-in-postgresql</id>
    <content type="html">数据库版本：9.3.1（不同版本数据库相关表列名可能略有不同）

数据库状态信息

数据库状态信息主要体现数据库的当前状态

1.目前客户端的连接数
postgres=# SELECT count(*) FROM pg_stat_activity WHERE NOT pid=pg_backend_pid();

2.连接状态
postgres=# SELECT pid,waiting,current_timestamp - least(query_start,xact_start) AS runtime,substr(query,1,25) AS current_query 
FROM ...</content>
  </entry>
  
  <entry>
    <title>RHEL系统安装PostgreSQL</title>
    <link href="/2014/04/07/install-postgresql-on-rhel-system"/>
    <updated>2014-04-07T00:00:00+08:00</updated>
    <id>/2014/04/07/install-postgresql-on-rhel-system</id>
    <content type="html">环境说明


OS：RHEL6.4（x86_64）
postgresql版本：PostgreSQL9.2.8


安装步骤

1. 下载所需的PostgreSQL rpm包

基础安装：


postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm
postgresql92-9.2.8-1PGDG.rhel6.x86_64.rpm
postgresql92-server-9.2.8-1PGDG.rhel6.x86_64.rpm


扩展安装：


postgresql92-contrib-9.2.8-1PGDG.rhel6.x86_64.rpm
pos...</content>
  </entry>
  
  <entry>
    <title>RHEL系统安装MySQL主备环境</title>
    <link href="/2014/04/06/mysql-config-for-master-slave-replication"/>
    <updated>2014-04-06T00:00:00+08:00</updated>
    <id>/2014/04/06/mysql-config-for-master-slave-replication</id>
    <content type="html">环境准备


操作系统： rhel6.4
数据库： percona 5.6.14
使用3306端口保证端口未被占用，selinux关闭状态


原理说明

mysql的复制（Replication)是一个异步的复制，从一个mysql instance(称之为master)复制到另一个mysql instance(称之为slave).实现整个复制操作主要由三个进程完成的，其中俩个进程在slave(sql进程和io进程），另外一个进程在master（IO进程）上。

要实施复制，首先要打开master端的binary log(bin-log)功能，否则无法实现。因为整个复制过程实际上就是sl...</content>
  </entry>
  
  <entry>
    <title>RHEL系统安装MySql</title>
    <link href="/2014/04/06/install-mysql-on-rhel-system"/>
    <updated>2014-04-06T00:00:00+08:00</updated>
    <id>/2014/04/06/install-mysql-on-rhel-system</id>
    <content type="html">环境说明


操作系统:linux6.4
MySql版本：percona 5.6.14
rpm包下载地址：http://www.percona.com/downloads/Percona-Server-5.6/LATEST/RPM/rhel6/x86_64


安装步骤

1. 安装所需要的rpm包
rpm -ivh Percona-Server-shared-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm
rpm -ivh Percona-Server-client-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm
rpm -i...</content>
  </entry>
  
  <entry>
    <title>BroadleafCommerce介绍</title>
    <link href="/2014/04/04/introduction-to-broadleaf-commerce"/>
    <updated>2014-04-04T00:00:00+08:00</updated>
    <id>/2014/04/04/introduction-to-broadleaf-commerce</id>
    <content type="html">1. 介绍

BroadleafCommerce是一个Java开源电子商务网站框架。其目标是开发企业级商务网站，它提供健壮的数据和服务模型、富客户端管理平台、已经一些核心电子商务有关的工具。

2. 特性

2.1  Catalog （目录分类）

提供灵活的产品和类型管理，一个重要的特性是可以继承产品分类来满足特殊的商业需求。管理界面可以管理各种类别和产品。

2.2  Promotion System（促销系统）

可通过配置的方式管理促销。以下类促销示无需客制化而通过管理界面即可管理：


百分比折扣、金额折扣、固定价格(Percent Off / Dollar Off / Fix...</content>
  </entry>
  
  <entry>
    <title>使用SaltStack安装JDK1.6</title>
    <link href="/2014/04/01/install-jdk-with-saltstack"/>
    <updated>2014-04-01T00:00:00+08:00</updated>
    <id>/2014/04/01/install-jdk-with-saltstack</id>
    <content type="html">创建states文件

在/srv/salt目录下创建jdk目录，并在jdk目录创建init.sls文件，init.sls文件内容如下：
jdk-file:
 file.managed:
   - source: salt://jdk/files/jdk1.6.0_39.tar.gz
   - name: /usr/java/jdk1.6.0_39.tar.gz
   - include_empty: True

jdk-install:
 cmd.run:
   - name: &amp;#39;/bin/tar -zxf jdk1.6.0_39.tar.gz &amp;amp;&amp;amp; /bin/...</content>
  </entry>
  
  <entry>
    <title>使用Lua和OpenResty搭建验证码服务器</title>
    <link href="/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty"/>
    <updated>2014-04-01T00:00:00+08:00</updated>
    <id>/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty</id>
    <content type="html">Lua下有个Lua-GD图形库，通过简单的Lua语句就能控制、生成图片。

环境说明：


操作系统：RHEL6.4
RHEL系统默认已安装RPM包的Lua-5.1.4，但其只具有Lua基本功能，不提供 lua.h 等，但 Lua-GD 编译需要用到 lua.h，故 Lua 需要编译安装。
Lua-GD 版本号格式为X.Y.XrW，其中X.Y.Z代表gd版本，W代表效力版本，所以 lua-gd 版本：lua-gd-2.0.33r2 相对应 gd 版本为：gd-2.0.33，须注意保持一致。
因生成gif的lua脚本中用到md5加密，故需编译安装md5。
因为生成图片需要唯一命名，故依赖 ...</content>
  </entry>
  
  <entry>
    <title>CDH4.5.0 新特性</title>
    <link href="/2014/03/31/what-is-new-in-CDH4.5.0"/>
    <updated>2014-03-31T00:00:00+08:00</updated>
    <id>/2014/03/31/what-is-new-in-CDH4.5.0</id>
    <content type="html">Apache Flume

新特性：


FLUME-2190 - 引入一个新的Twitter firehose的feed源
FLUME-2109 - HTTP输入源支持HTTPS.
FLUME-1666 - 系统日志的TCP源现在可以保持时间戳和处理领域中的事件主体.
FLUME-2202 - AsyncHBaseSink can now coalesce increments to the same row and column per transaction to reduce the number of RPC calls
FLUME-2189 - Avro Source can...</content>
  </entry>
  
  <entry>
    <title>Python模拟新浪微博登录</title>
    <link href="/2014/03/18/simulate-weibo-login-in-python"/>
    <updated>2014-03-18T00:00:00+08:00</updated>
    <id>/2014/03/18/simulate-weibo-login-in-python</id>
    <content type="html">看到一篇Python模拟新浪微博登录的文章，想熟悉一下其中实现方式，并且顺便掌握python相关知识点。

代码

下面的代码是来自上面这篇文章，并稍作修改添加了一些注释。
# -*- coding: utf-8 -*

import urllib2
import urllib
import cookielib

import lxml.html as HTML

class Fetcher(object):
    def __init__(self, username=None, pwd=None, cookie_filename=None):
        #获取一个保存cook...</content>
  </entry>
  
  <entry>
    <title>Solr的schema.xml</title>
    <link href="/2014/03/15/schema-in-solr"/>
    <updated>2014-03-15T00:00:00+08:00</updated>
    <id>/2014/03/15/schema-in-solr</id>
    <content type="html">schema.xml是Solr一个配置文件，它包含了你的文档所有的字段，以及当文档被加入索引或查询字段时，这些字段是如何被处理的。这个文件被存储在Solr主文件夹下的conf目录下，默认的路径./solr/conf/schema.xml，也可以是Solr webapp的类加载器所能确定的路径。在下载的Solr包里，有一个schema的样例文件，用户可以从那个文件出发，来观察如何编写自己的Schema.xml。

type节点

先来看下type节点，这里面定义FieldType子节点，包括name、class、positionIncrementGap等一些参数。必选参数：


name：...</content>
  </entry>
  
  <entry>
    <title>IDH HBase中实现的一些特性</title>
    <link href="/2014/03/15/new-features-in-idh-hbase"/>
    <updated>2014-03-15T00:00:00+08:00</updated>
    <id>/2014/03/15/new-features-in-idh-hbase</id>
    <content type="html">IDH为Intel&amp;#39;s Distribution of Hadoop的简称，中文为英特尔Hadoop发行版，目前应该没有人在维护该产品了。这里简单介绍一下IDH HBase中实现的一些特性。

以下部分内容摘自IDH官方的一些文档，部分内容来自我的整理：

1、 单调数据的加盐处理

对于写入的rowkey是基本单调的（例如时序数据），IDH引入了一个新的接口：SaltedTableInterface


提高近乎透明的“加盐”，方便使用
封装了get、scan、put、delete等操作


2、提供了Rolling Scanner应对HFile数量大量增加情况下的get、sc...</content>
  </entry>
  
  <entry>
    <title>在Solr中使用中文分词</title>
    <link href="/2014/03/14/split-chinese-in-solr"/>
    <updated>2014-03-14T00:00:00+08:00</updated>
    <id>/2014/03/14/split-chinese-in-solr</id>
    <content type="html">使用全文检索，中文分词是离不开的，这里我采用的是 mmseg4j 分词器。mmseg4j分词器内置了对solr的支持，最新版本可支持4.X版本的sorl，使用起来很是方便。

下载mmseg4j

GoogleCode地址：http://code.google.com/p/mmseg4j/

请下载最新版本：mmseg4j-1.9.1，然后将mmseg4j-1.9.1/dist下的jar包拷贝至solr.war的lib目录，例如：apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/

配置schema.xml

使用mmseg4j中文分词器，首先需...</content>
  </entry>
  
  <entry>
    <title>BroadLeaf项目集成SendCloud</title>
    <link href="/2014/03/14/broadleaf-project-with-sendcloud"/>
    <updated>2014-03-14T00:00:00+08:00</updated>
    <id>/2014/03/14/broadleaf-project-with-sendcloud</id>
    <content type="html">《BroadLeaf项目搜索功能改进》一文中介绍了 BroadLeaf 项目中如何改进搜索引擎这一块的代码，其中使用的是单节点的 solr 服务器，这篇文章主要介绍 BroadLeaf 项目如何集成 sendcloud 集群。

1、SolrCloud环境搭建

参考 《Apache SolrCloud安装》，搭建Solr集群环境，将Demosite所用的Solr配置文件solrconfig.xml和schema.xml上传到zookeeper集群中，保证成功启动Solr集群。

2、扩展SearcheService类

扩展SearchService类的步骤与单节点集成一致，此处不再叙...</content>
  </entry>
  
  <entry>
    <title>BroadLeaf项目搜索功能改进</title>
    <link href="/2014/03/13/improve-the-search-function-in-broadleaf-project"/>
    <updated>2014-03-13T00:00:00+08:00</updated>
    <id>/2014/03/13/improve-the-search-function-in-broadleaf-project</id>
    <content type="html">Broadleaf Commerce 是一个开源的Java电子商务平台，基于Spring框架开发，提供一个可靠、可扩展的架构，可进行深度的定制和快速开发。

关于Solr

Broadleaf项目中关于商品的搜索使用了嵌入式的Solr服务器，这个从配置文件中可以看出来。


项目主页： http://www.broadleafcommerce.com/
示例网站： http://demo.broadleafcommerce.org/
示例网站源代码： https://github.com/BroadleafCommerce/DemoSite


从示例网站源代码的applicationC...</content>
  </entry>
  
  <entry>
    <title>Apache SolrCloud安装</title>
    <link href="/2014/03/10/how-to-install-solrcloud"/>
    <updated>2014-03-10T00:00:00+08:00</updated>
    <id>/2014/03/10/how-to-install-solrcloud</id>
    <content type="html">SolrCloud 通过 ZooKeeper 集群来进行协调，使一个索引进行分片，各个分片可以分布在不同的物理节点上，多个物理分片组成一个完成的索引 Collection。SolrCloud 自动支持 Solr Replication，可以同时对分片进行复制，冗余存储。下面，我们基于 Solr 最新的 4.4.0 版本进行安装配置 SolrCloud 集群。

1. 安装环境

我使用的安装程序各版本如下：


Solr： Apache Solr-4.4.0
Tomcat： Apache Tomcat 6.0.36
ZooKeeper： Apache ZooKeeper 3.4.5


...</content>
  </entry>
  
  <entry>
    <title>HBase源码：HRegionServer启动过程</title>
    <link href="/2014/03/09/hbase-note-about-hregionserver-startup"/>
    <updated>2014-03-09T00:00:00+08:00</updated>
    <id>/2014/03/09/hbase-note-about-hregionserver-startup</id>
    <content type="html">版本：HBase 0.94.15-cdh4.7.0

关于HMaster启动过程，请参考HBase源码：HMaster启动过程。先启动了HMaster之后，再启动HRegionServer。

运行HRegionServerStarter类启动HRegionServer：
package my.test.start;

import org.apache.hadoop.hbase.regionserver.HRegionServer;

public class HRegionServerStarter {

    public static void main(String[] ar...</content>
  </entry>
  
  <entry>
    <title>HBase源码：HMaster启动过程</title>
    <link href="/2014/03/09/hbase-note-about-hmaster-startup"/>
    <updated>2014-03-09T00:00:00+08:00</updated>
    <id>/2014/03/09/hbase-note-about-hmaster-startup</id>
    <content type="html">版本：HBase 0.94.15-cdh4.7.0

调试HMaster


说明：

这部分参考和使用了https://github.com/codefollower/HBase-Research上的代码（注意：原仓库已经被作者删除了），包括该作者自己写的一些测试类和文档。


首先，在IDE里启动HMaster和HRegionServer：

运行/hbase/src/test/java/my/test/start/HMasterStarter.java，当看到提示Waiting for region servers count to settle时，
再打开同目录中的HRegion...</content>
  </entry>
  
  <entry>
    <title>2013年度年终总结</title>
    <link href="/2014/03/06/summary-of-the-work-in-2013"/>
    <updated>2014-03-06T00:00:00+08:00</updated>
    <id>/2014/03/06/summary-of-the-work-in-2013</id>
    <content type="html">回首2011年和2012年的年终总结，发现公司在2012年提到的一些不足仍然出现在2013年，不知道每年的总结是否有被认真阅读过、重视过。故虽谏且议，使人不得而知焉。

2013年，通过了RHCE考试，掌握了shell编程，初识Python；

2013年，不再负责、管理具体的项目，可还不是逃不过事后填坑的无奈；

2013年，Cassandra不再，迎来Hadoop，满腔热血的学习Hadoop的安装、部署、原理、开发甚至还做了一些入门普及培训，但真的只是一个人在战斗；

2013年，开始是一个人带着几个同事在探索和研究hadoop，慢慢地失去了自己的自主权，更多的时间是被花在了具体的项...</content>
  </entry>
  
  <entry>
    <title>Apache Solr查询语法</title>
    <link href="/2014/03/03/solr-query-syntax"/>
    <updated>2014-03-03T00:00:00+08:00</updated>
    <id>/2014/03/03/solr-query-syntax</id>
    <content type="html">查询参数

常用：


q - 查询字符串，必须的。
fl - 指定返回那些字段内容，用逗号或空格分隔多个。
start - 返回第一条记录在完整找到结果中的偏移位置，0开始，一般分页用。
rows - 指定返回结果最多有多少条记录，配合start来实现分页。
sort - 排序，格式：sort=&amp;lt;field name&amp;gt;+&amp;lt;desc|asc&amp;gt;[,&amp;lt;field name&amp;gt;+&amp;lt;desc|asc&amp;gt;]。示例：（inStock desc, price asc）表示先 &amp;quot;inStock&amp;quot; 降序, 再 &amp;quot;price&amp;quot...</content>
  </entry>
  
  <entry>
    <title>Apache Solr介绍及安装</title>
    <link href="/2014/02/26/how-to-install-solr"/>
    <updated>2014-02-26T00:00:00+08:00</updated>
    <id>/2014/02/26/how-to-install-solr</id>
    <content type="html">Solr是什么

Solr是一个基于Lucene java库的企业级搜索服务器，包含XML/HTTP，JSON API，高亮查询结果，缓存，复制，还有一个WEB管理界面。Solr运行在Servlet容器中，其架构如下：



主要功能包括全文检索，高亮命中，分面搜索(faceted search)，近实时索引，动态集群，数据库集成，富文本索引，空间搜索；通过提供分布式索引，复制，负载均衡查询，自动故障转移和恢复，集中配置等功能实现高可用，可伸缩和可容错。

Solr和Lucene的本质区别有以下三点：搜索服务器、企业级和管理。Lucene本质上是搜索库，不是独立的应用程序，而Solr是。...</content>
  </entry>
  
  <entry>
    <title>使用Vagrant创建虚拟机安装Hadoop</title>
    <link href="/2014/02/23/create-virtualbox-by-vagrant"/>
    <updated>2014-02-23T00:00:00+08:00</updated>
    <id>/2014/02/23/create-virtualbox-by-vagrant</id>
    <content type="html">安装VirtualBox

下载地址：https://www.virtualbox.org/wiki/Downloads/

安装Vagrant

下载安装包：http://downloads.vagrantup.com/，然后安装。

下载box

下载适合你的box，地址：http://www.vagrantbox.es/。

例如下载 CentOS6.5：
$ wget https://github.com/2creatives/vagrant-centos/releases/download/v6.5.3/centos65-x86_64-20140116.box

添加box

...</content>
  </entry>
  
  <entry>
    <title>Python基础入门</title>
    <link href="/2014/02/22/python-introduction-of-basics"/>
    <updated>2014-02-22T00:00:00+08:00</updated>
    <id>/2014/02/22/python-introduction-of-basics</id>
    <content type="html">1. Python介绍

Python是一种解释性的面向对象的语言。Python使用C语言编写，不需要事先声明变量的类型（动态类型），但是一旦变量有了值，那么这个变脸是有一个类型的，不同类型的变量之间赋值需要类型转换（强类型）。

1.1 安装 Python

现在的操作系统都自带安装了 Python，要测试你是否安装了Python，你可以打开一个shell程序（就像konsole或gnome-terminal），然后输入如下所示的命令python -V
$ python -V
Python 2.7.4

如果你看见向上面所示的那样一些版本信息，那么你已经安装了Python了。

2. ...</content>
  </entry>
  
  <entry>
    <title>Confluence 5.4.2安装</title>
    <link href="/2014/02/21/install-confluence5-4-2"/>
    <updated>2014-02-21T00:00:00+08:00</updated>
    <id>/2014/02/21/install-confluence5-4-2</id>
    <content type="html">Confluence是Atlassian公司出品的团队协同与知识管理工具。 Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。通过它可以实现团队成员之间的协作和知识共享。

1、下载

下载指定版本Confluence
$ mkdir -p /data/confluence
$ cd /data/confluence
$ wget www.atlassian.com/software/confluence/downloads/binary/atlassian-confluence-5.4.2.tar.gz

2、安装

解压：
$ tar -zxvf a...</content>
  </entry>
  
  <entry>
    <title>CDH 5 Beta 2 的新变化</title>
    <link href="/2014/02/21/cdh5rn_whats_new_in_b2"/>
    <updated>2014-02-21T00:00:00+08:00</updated>
    <id>/2014/02/21/cdh5rn_whats_new_in_b2</id>
    <content type="html">本文是同事对CDH 5.0.0 Beta 2的翻译，仅供大家参考。

这是 CDH 5.0.0 Beta 2的初稿。鉴于 CDH 5 目前的发布版本是测试版，它不应用于生产环境中；它只是用来评估、测试的。对于生产环境，请使用 CDH 4,最近的文档在 CDH Documentation

Apache Crunch

Apache Crunch 项目开发了新的 Java API，简化了在 Apache Hadoop 之上的数据管道的创建过程。

Crunch APIs 是以 FlumeJava 为蓝本开发的，FlumeJava 是 Google 用来在他们自己的 MapReduce 实现...</content>
  </entry>
  
  <entry>
    <title>Backbone中的模型</title>
    <link href="/2014/02/16/backbone-model"/>
    <updated>2014-02-16T00:00:00+08:00</updated>
    <id>/2014/02/16/backbone-model</id>
    <content type="html">创建model

模型是所有Javascript应用程序的核心，包括交互数据及相关的大量逻辑： 转换、验证、计算属性和访问控制。你可以用特定的方法扩展Backbone.Model，模型也提供了一组基本的管理变化的功能。
Person = Backbone.Model.extend({
    initialize: function(){
        alert(&amp;quot;Welcome to this world&amp;quot;);
    }
});

var person = new Person;

new一个model的实例后就会触发initialize()函数。

设置属性...</content>
  </entry>
  
  <entry>
    <title>在CentOs6系统上安装Ganglia</title>
    <link href="/2014/01/25/how-to-install-ganglia-on-centos6"/>
    <updated>2014-01-25T00:00:00+08:00</updated>
    <id>/2014/01/25/how-to-install-ganglia-on-centos6</id>
    <content type="html">Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，由RRDTool工具处理数据，并生成相应的的图形显示，以Web方式直观的提供给客户端。如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。

配置yum源

首先配置好CentOs系统的yum源，然后需要包含有ganglia的yum源。

在/etc/yum.repos.d下创建ganglia.rep...</content>
  </entry>
  
  <entry>
    <title>在RHEL系统上安装Nagios</title>
    <link href="/2014/01/24/how-to-install-nagios-on-rhel6"/>
    <updated>2014-01-24T00:00:00+08:00</updated>
    <id>/2014/01/24/how-to-install-nagios-on-rhel6</id>
    <content type="html">在管理机上安装rpm包
$ rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
$ rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm
$ yum -y install nagios nagios-plugins-all nagios-plugins-nrpe nrpe php httpd
$ chkconfig httpd on &amp;amp;&amp;amp; chkconfig nagios on...</content>
  </entry>
  
  <entry>
    <title>All Things OpenTSDB</title>
    <link href="/2014/01/22/all-things-opentsdb"/>
    <updated>2014-01-22T00:00:00+08:00</updated>
    <id>/2014/01/22/all-things-opentsdb</id>
    <content type="html">1. OpenTSDB介绍

OpenTSDB用HBase存储所有的时序（无须采样）来构建一个分布式、可伸缩的时间序列数据库。它支持秒级数据采集所有metrics，支持永久存储，可以做容量规划，并很容易的接入到现有的报警系统里。OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化、图形化等。

对于运维工程师而言，OpenTSDB可以获取基础设施和服务的实时状态信息，展示集群的各种软硬件错误，性能变化以及性能瓶颈。对于管理者而言，OpenTSDB可以衡量系统的SLA，理...</content>
  </entry>
  
  <entry>
    <title>All Things Jekyll</title>
    <link href="/2014/01/21/all-things-about-jekyll"/>
    <updated>2014-01-21T00:00:00+08:00</updated>
    <id>/2014/01/21/all-things-about-jekyll</id>
    <content type="html">Jekyll是一个简洁的、特别针对博客平台的静态网站生成器。它使用一个模板目录作为网站布局的基础框架，并在其上运行Textile、Markdown或Liquid标记语言的转换器，最终生成一个完整的静态Web站点，可以被放置在Apache或者你喜欢的其他任何Web服务器上。它同时也是GitHub Pages、一个由GitHub提供的用于托管项目主页或博客的服务，在后台所运行的引擎。

1. 安装

Jekyll使用动态脚本语言Ruby写成。请首先下载并安装Ruby，目前需要的ruby版本为1.9.1。

在使用Jekyll之前，你可能想要对Ruby语言有一些初步了解（非必需）。

安装Je...</content>
  </entry>
  
  <entry>
    <title>SSH远程连接时环境变量问题</title>
    <link href="/2014/01/18/bash-problem-when-ssh-access"/>
    <updated>2014-01-18T00:00:00+08:00</updated>
    <id>/2014/01/18/bash-problem-when-ssh-access</id>
    <content type="html">1. 问题

RHEL服务器A有个启动脚本（普通用户user01运行），里面使用ifconfig获取ip地址如下：
Localhost_ip=$(ifconfig |awk -F &amp;#39;addr:|Bcast&amp;#39; &amp;#39;/Bcast/{print $2}&amp;#39;)

由于普通用户user01不能直接识别ifconfig命令，只能使用全路径/sbin/ifconfig，目前处理方式为修改~/.bash_profile文件添加环境变量如下：
PATH=$PATH:$HOME/bin

改成如下：
PATH=$PATH:$HOME/bin:/sbin



经过如上配置后服务器...</content>
  </entry>
  
  <entry>
    <title>HBase笔记：Region拆分策略</title>
    <link href="/2014/01/16/hbase-region-split-policy"/>
    <updated>2014-01-16T00:00:00+08:00</updated>
    <id>/2014/01/16/hbase-region-split-policy</id>
    <content type="html">Region 概念

Region是表获取和分布的基本元素，由每个列族的一个Store组成。对象层级图如下：
Table       (HBase table)
    Region       (Regions for the table)
         Store          (Store per ColumnFamily for each Region for the table)
              MemStore        (MemStore for each Store for each Region for the table)
          ...</content>
  </entry>
  
  <entry>
    <title>Vim配置和插件管理</title>
    <link href="/2014/01/14/vim-config-and-plugins"/>
    <updated>2014-01-14T00:00:00+08:00</updated>
    <id>/2014/01/14/vim-config-and-plugins</id>
    <content type="html">这篇文章主要是记录vim配置中各个配置项的含义并且收藏一些常用的插件及其使用方法。

1. Vim配置

目前我的vimrc配置放置在:https://github.com/javachen/snippets/blob/master/dotfiles/.vimrc，其中大多数用英文注释。

2. 插件管理

使用 pathogen来管理插件

项目地址:   https://github.com/tpope/vim-pathogen

安装方法：
$ mkdir -p ~/.vim/autoload ~/.vim/bundle &amp;amp;&amp;amp; \
$ curl -LSso ~/.v...</content>
  </entry>
  
  <entry>
    <title>SiteMesh介绍</title>
    <link href="/2014/01/13/about-sitemesh"/>
    <updated>2014-01-13T00:00:00+08:00</updated>
    <id>/2014/01/13/about-sitemesh</id>
    <content type="html">1. SiteMesh简介

SiteMesh是由一个基于Web页面布局、装饰以及与现存Web应用整合的框架。它能帮助我们在由大量页面构成的项目中创建一致的页面布局和外观，如一致的导航条，一致的banner，一致的版权等等。它不仅仅能处理动态的内容，如jsp，php，asp等产生的内容，它也能处理静态的内容，如htm的内容，使得它的内容也符合你的页面结构的要求。甚至于它能将HTML文件象include那样将该文件作为一个面板的形式嵌入到别的文件中去。所有的这些，都是GOF的Decorator模式的最生动的实现。尽管它是由java语言来实现的，但它能与其他Web应用很好地集成。

2. S...</content>
  </entry>
  
  <entry>
    <title>如何创建一个Django网站</title>
    <link href="/2014/01/11/how-to-create-a-django-site"/>
    <updated>2014-01-11T00:00:00+08:00</updated>
    <id>/2014/01/11/how-to-create-a-django-site</id>
    <content type="html">本文演示如何创建一个简单的 django 网站，使用的 django 版本为1.7。

1. 创建工程

运行下面命令就可以创建一个 django 工程，工程名字叫 todo_site ：
$ django-admin.py startproject todo_site

创建后的工程目录如下：
todo_site
├── manage.py
└── todo_site
    ├── __init__.py
    ├── settings.py
    ├── templates
    ├── urls.py
    └── wsgi.py

说明：


__init__.py ：...</content>
  </entry>
  
  <entry>
    <title>重装Linux-Mint系统之后</title>
    <link href="/2014/01/09/after-reinstall-the-system"/>
    <updated>2014-01-09T00:00:00+08:00</updated>
    <id>/2014/01/09/after-reinstall-the-system</id>
    <content type="html">本文主要记录重装Linux-Mint系统之后的一些软件安装和环境变量配置。

安装常用工具
sudo apt-get install ctags curl vsftpd git vim tmux meld htop putty subversion  nload  iptraf iftop tree openssh-server gconf-editor gnome-tweak-tool

挂载exfat格式磁盘
sudo apt-get install exfat-fuse exfat-utils

安装ibus

在终端输入命令:
sudo add-apt-repository pp...</content>
  </entry>
  
  <entry>
    <title>Hive使用HAProxy配置HA</title>
    <link href="/2014/01/08/hive-ha-by-haproxy"/>
    <updated>2014-01-08T00:00:00+08:00</updated>
    <id>/2014/01/08/hive-ha-by-haproxy</id>
    <content type="html">HAProxy是一款提供高可用性、负载均衡以及基于TCP（第四层）和HTTP（第七层）应用的代理软件，HAProxy是完全免费的、借助HAProxy可以快速并且可靠的提供基于TCP和HTTP应用的代理解决方案。


免费开源，稳定性也是非常好，这个可通过我做的一些小项目可以看出来，单Haproxy也跑得不错，稳定性可以与硬件级的F5相媲美。
根据官方文档，HAProxy可以跑满10Gbps-New benchmark of HAProxy at 10 Gbps using Myricom&amp;#39;s 10GbE NICs （Myri-10G PCI-Express），这个数值作为软件级负...</content>
  </entry>
  
  <entry>
    <title>Git配置和一些常用命令</title>
    <link href="/2013/12/27/some-git-configs-and-cammands"/>
    <updated>2013-12-27T00:00:00+08:00</updated>
    <id>/2013/12/27/some-git-configs-and-cammands</id>
    <content type="html">Git是一个分布式版本控制／软件配置管理软件，原来是linux内核开发者林纳斯·托瓦兹（Linus Torvalds）为了更好地管理linux内核开发而创立的。

Git配置
git config --global user.name &amp;quot;javachen&amp;quot;
git config --global user.email &amp;quot;june.chan@foxmail.com&amp;quot;
git config --global color.ui true
git config --global alias.co checkout
git config --global a...</content>
  </entry>
  
  <entry>
    <title>SaltStack学习笔记</title>
    <link href="/2013/11/18/study-note-of-saltstack"/>
    <updated>2013-11-18T00:00:00+08:00</updated>
    <id>/2013/11/18/study-note-of-saltstack</id>
    <content type="html">1. 关于本文档

这份文档如其名，是我自己整理的学习 SaltStack 的过程记录。只是过程记录，没有刻意像教程那样去做。所以呢，从前至后，中间不免有一些概念不清不明的地方。因为事实上，在某个阶段对于一些概念本来就不可能明白。所以，整个过程只求在形式上的能用即可。前面就不要太纠结概念和原理，知道怎么用就好。

希望这篇文章能够让你快速了解并使用saltstack。文章还在编写中。

2. 关于SaltStack

2.1. 什么是SaltStack

SaltStack是开源的管理基础设置的轻量级工具，容易搭建，为远程管理服务器提供一种更好、更快速、更有扩展性的解决方案。通过简单、可...</content>
  </entry>
  
  <entry>
    <title>使用SaltStack安装JBoss</title>
    <link href="/2013/11/16/install-jboss-with-saltstack"/>
    <updated>2013-11-16T00:00:00+08:00</updated>
    <id>/2013/11/16/install-jboss-with-saltstack</id>
    <content type="html">SaltStack是一个具备puppet与func功能为一身的集中化管理平台，其基于python实现，功能十分强大，各模块融合度及复用性极高。SaltStack 采用 zeromq 消息队列进行通信，和 Puppet/Chef 比起来，SaltStack 速度快得多。

在开始使用SaltStack之前，首先要对SaltStack的基础进行一系列的学习，这里，强烈推荐官网的Tutorial,在完成了整个Tutorial之后，通过Module Index页面，我们能够快速查阅Salt所有模块的功能与用法:http://docs.saltstack.com/py-modindex.html
...</content>
  </entry>
  
  <entry>
    <title>安装SaltStack和Halite</title>
    <link href="/2013/11/11/install-saltstack-and-halite"/>
    <updated>2013-11-11T00:00:00+08:00</updated>
    <id>/2013/11/11/install-saltstack-and-halite</id>
    <content type="html">本文记录安装SaltStack和halite过程。

首先准备两台rhel或者centos虚拟机sk1和sk2，sk1用于安装master，sk2安装minion。

配置yum源

在每个节点上配置yum源：
$ rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm

然后通过下面命令查看epel参考是否安装成功：
$ yum list #或者查看/etc/yum.repos.d目录下是否有epel.repo

如果没有安装成功，则可以手动下载epel-release-6-8....</content>
  </entry>
  
  <entry>
    <title>在Eclipse中调试运行HBase</title>
    <link href="/2013/11/01/debug-hbase-in-eclipse"/>
    <updated>2013-11-01T00:00:00+08:00</updated>
    <id>/2013/11/01/debug-hbase-in-eclipse</id>
    <content type="html">这篇文章记录一下如何在eclipse中调试运行hbase。

下载并编译源代码

请参考编译hbase源代码并打补丁

修改配置文件

修改 conf/hbase-site.xml文件：
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.defaults.for.version&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;0.94.6-cdh4.4.0&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;fil...</content>
  </entry>
  
  <entry>
    <title>编译CDH HBase源代码并打补丁</title>
    <link href="/2013/10/28/compile-hbase-source-code-and-apply-patches"/>
    <updated>2013-10-28T00:00:00+08:00</updated>
    <id>/2013/10/28/compile-hbase-source-code-and-apply-patches</id>
    <content type="html">写了一篇博客记录编译CDH HBase源代码并打补丁的过程，如有不正确的，欢迎指出！

下载源代码

从Cloudera github上下载最新分支源代码，例如：当前最新分支为cdh4-0.94.6_4.4.0
$ git clone git@github.com:cloudera/hbase.git -b cdh4-0.94.6_4.4.0 cdh4-0.94.6_4.4.0

说明：


-b 指定下载哪个分支
最后一个参数指定下载下来的文件名称


添加snappy压缩支持

编译snappy
$ svn checkout http://snappy.googlecode.com/...</content>
  </entry>
  
  <entry>
    <title>HiveServer2中使用jdbc客户端用户运行mapreduce</title>
    <link href="/2013/10/17/run-mapreduce-with-client-user-in-hive-server2"/>
    <updated>2013-10-17T00:00:00+08:00</updated>
    <id>/2013/10/17/run-mapreduce-with-client-user-in-hive-server2</id>
    <content type="html">最近做了个web系统访问hive数据库，类似于官方自带的hwi、安居客的hwi改进版和大众点评的polestar(github地址)系统，但是和他们的实现不一样，查询Hive语句走的不是cli而是通过jdbc连接hive-server2。为了实现mapreduce任务中资源按用户调度，需要hive查询自动绑定当前用户、将该用户传到yarn服务端并使mapreduce程序以该用户运行。本文主要是记录实现该功能过程中遇到的一些问题以及解决方法,如果你有更好的方法和建议，欢迎留言发表您的看法！

说明

集群环境使用的是cdh4.3，没有开启kerberos认证。


写完这篇文章之后，在微博...</content>
  </entry>
  
  <entry>
    <title>Hive连接产生笛卡尔集</title>
    <link href="/2013/10/17/cartesian-product-in-hive-inner-join"/>
    <updated>2013-10-17T00:00:00+08:00</updated>
    <id>/2013/10/17/cartesian-product-in-hive-inner-join</id>
    <content type="html">在使用hive过程中遇到这样的一个异常：
FAILED: ParseException line 1:18 Failed to recognize predicate &amp;#39;a&amp;#39;. Failed rule: &amp;#39;kwInner&amp;#39; in join type specifier

执行的hql语句如下：
[root@javachen.com ~]# hive -e &amp;#39;select a.* from t a, t b where a.id=b.id&amp;#39;

从异常信息中很难看出出错原因，hive.log中也没有打印出详细的异常对战信息。改用jdbc连接hi...</content>
  </entry>
  
  <entry>
    <title>最近的工作</title>
    <link href="/2013/09/08/recent-work"/>
    <updated>2013-09-08T00:00:00+08:00</updated>
    <id>/2013/09/08/recent-work</id>
    <content type="html">最近一直在构思这篇博客的内容，到现在还是不知道从何下手。自从将博客从wordpress迁移到github上之后，就很少在博客写一些关于工作和生活的文章，所以想写一篇关于工作的博客，记录最近做过的事情以及一些当时的所思所想。

最近半年多一直在做hadoop方面的工作，也就是接触hadoop才半年多时间。最开始接触hadoop是去年的11月21日，那天去Intel公司参加了两天的hadoop培训。培训的内容很多干货也有很多枯燥的东西，所以边听边瞌睡的听完了两天的培训内容。培训的ppt打印出来了，时不时地会翻看上面讲述的内容，然后在网上搜索些相关的资料。

最先接触的hadoop发行版是In...</content>
  </entry>
  
  <entry>
    <title>hive中如何确定map数</title>
    <link href="/2013/09/04/how-to-decide-map-number"/>
    <updated>2013-09-04T00:00:00+08:00</updated>
    <id>/2013/09/04/how-to-decide-map-number</id>
    <content type="html">hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。当运行一个hql语句的时候，map数是如何计算出来的呢？有哪些方法可以调整map数呢？



hive默认的input format

在cdh-4.3.0的hive中查看hive.input.format值（为什么是hive.input.format？）：
hive&amp;gt; set hive.input.format;
hive.input.format=org.apache.hadoop.hive.ql.io.Com...</content>
  </entry>
  
  <entry>
    <title>我的jekyll配置和修改</title>
    <link href="/2013/08/31/my-jekyll-config"/>
    <updated>2013-08-31T00:00:00+08:00</updated>
    <id>/2013/08/31/my-jekyll-config</id>
    <content type="html">主要记录使用jekyll搭建博客时的一些配置和修改。

注意：


使用时请删除{和%以及{和{之间的空格。


预览文章
source ~/.bash_profile
jekyll server

添加about me 边栏

参考the5fire的技术博客在index.html页面加入如下代码：
&amp;lt;section&amp;gt;
&amp;lt;h4&amp;gt;About me&amp;lt;/h4&amp;gt;
&amp;lt;div&amp;gt;
 一个Java方案架构师，主要从事hadoop相关工作。&amp;lt;a href=&amp;quot;/about.html&amp;quot;&amp;gt;更多信息&amp;lt;/a&amp;gt; 
&amp;lt;br...</content>
  </entry>
  
  <entry>
    <title>使用ZooKeeper实现配置同步</title>
    <link href="/2013/08/23/publish-proerties-using-zookeeper"/>
    <updated>2013-08-23T00:00:00+08:00</updated>
    <id>/2013/08/23/publish-proerties-using-zookeeper</id>
    <content type="html">前言

应用项目中都会有一些配置信息，这些配置信息数据量少，一般会保存到内存、文件或者数据库，有时候需要动态更新。当需要在多个应用服务器中修改这些配置文件时，需要做到快速、简单、不停止应用服务器的方式修改并同步配置信息到所有应用中去。本篇文章就是介绍如何使用ZooKeeper来实现配置的动态同步。

ZooKeeper

在《hive Driver类运行过程》一文中可以看到hive为了支持并发访问引入了ZooKeeper来实现分布式锁。参考《ZooKeeper典型应用场景一览》一文，ZooKeeper还可以用作其他用途，例如：


数据发布与订阅（配置中心）
负载均衡
命名服务(Nami...</content>
  </entry>
  
  <entry>
    <title>Hive源码分析：Driver类运行过程</title>
    <link href="/2013/08/22/hive-Driver"/>
    <updated>2013-08-22T00:00:00+08:00</updated>
    <id>/2013/08/22/hive-Driver</id>
    <content type="html">说明：

本文的源码分析基于hive-0.12.0-cdh5.0.1。

概括

从《hive cli的入口类》中可以知道hive中处理hive命令的处理器一共有以下几种：
（1）set       SetProcessor，设置修改参数,设置到SessionState的HiveConf里。 
（2）dfs       DfsProcessor，使用hadoop的FsShell运行hadoop的命令。 
（3）add       AddResourceProcessor，添加到SessionState的resource_map里，运行提交job的时候会写入Hadoop的Distribut...</content>
  </entry>
  
  <entry>
    <title>Hive源码分析：CLI入口类</title>
    <link href="/2013/08/21/hive-CliDriver"/>
    <updated>2013-08-21T00:00:00+08:00</updated>
    <id>/2013/08/21/hive-CliDriver</id>
    <content type="html">说明：

本文的源码分析基于hive-0.10.0-cdh4.3.0。

启动脚本

从shell脚本/usr/lib/hive/bin/ext/cli.sh可以看到hive cli的入口类为org.apache.hadoop.hive.cli.CliDriver
    cli () {
      CLASS=org.apache.hadoop.hive.cli.CliDriver
      execHiveCmd $CLASS &amp;quot;$@&amp;quot;
    }
    cli_help () {
      CLASS=org.apache.hadoop.hive.cli...</content>
  </entry>
  
  <entry>
    <title>使用Hadoop中遇到的一些问题</title>
    <link href="/2013/08/17/some-problems-about-hadoop"/>
    <updated>2013-08-17T00:00:00+08:00</updated>
    <id>/2013/08/17/some-problems-about-hadoop</id>
    <content type="html">本文主要记录安装hadoop过程需要注意的一些细节以及使用hadoop过程中发现的一些问题以及对应解决办法，有些地方描述的不是很清楚可能还会不准确，之后会重现问题然后修改完善这篇文章。

安装hadoop过程中需要注意以下几点：


每个节点配置hosts
每个节点配置时钟同步
如果没有特殊要求，关闭防火墙
hadoop需要在/tmp目录下存放一些日志和临时文件，要求/tmp目录权限必须为1777






使用intel的hadoop发行版IDH过程遇到问题：

1、 IDH集群中需要配置管理节点到集群各节点的无密码登录，公钥文件存放路径为/etc/intelcloud目录下，文件名...</content>
  </entry>
  
  <entry>
    <title>Hadoop自动化安装shell脚本</title>
    <link href="/2013/08/02/hadoop-install-script"/>
    <updated>2013-08-02T00:00:00+08:00</updated>
    <id>/2013/08/02/hadoop-install-script</id>
    <content type="html">之前写过一些如何安装Cloudera Hadoop的文章，安装hadoop过程中，最开始是手动安装apache版本的hadoop，其次是使用Intel的IDH管理界面安装IDH的hadoop，再然后分别手动和通过cloudera manager安装hadoop，也使用bigtop-util yum方式安装过apache的hadoop。

安装过程中参考了很多网上的文章，解压缩过cloudera的cloudera-manager-installer.bin，发现并修复了IDH shell脚本中关于puppt的自认为是bug的一个bug，最后整理出了一个自动安装hadoop的shell脚本，...</content>
  </entry>
  
  <entry>
    <title>远程调试Hadoop各组件</title>
    <link href="/2013/08/01/remote-debug-hadoop"/>
    <updated>2013-08-01T00:00:00+08:00</updated>
    <id>/2013/08/01/remote-debug-hadoop</id>
    <content type="html">远程调试对应用程序开发十分有用。例如，为不能托管开发平台的低端机器开发程序，或在专用的机器上（比如服务不能中断的 Web 服务器）调试程序。其他情况包括：运行在内存小或 CUP 性能低的设备上的 Java 应用程序（比如移动设备），或者开发人员想要将应用程序和开发环境分开，等等。

为了进行远程调试，必须使用 Java Virtual Machine (JVM) V5.0 或更新版本。

JPDA 简介

Sun Microsystem 的 Java Platform Debugger Architecture (JPDA) 技术是一个多层架构，使您能够在各种环境中轻松调试 Java 应...</content>
  </entry>
  
  <entry>
    <title>安装RHadoop</title>
    <link href="/2013/07/20/install-rhadoop"/>
    <updated>2013-07-20T00:00:00+08:00</updated>
    <id>/2013/07/20/install-rhadoop</id>
    <content type="html">1. R Language Install

安装相关依赖
yum install -y perl* pcre-devel tcl-devel zlib-devel bzip2-devel libX11-devel tk-devel tetex-latex *gfortran*  compat-readline5
yum install libRmath-*
rpm -Uvh --force --nodeps  R-core-2.10.0-2.el5.x86_64.rpm
rpm -Uvh R-2.10.0-2.el5.x86_64.rpm R-devel-2.10.0-2.el5.x8...</content>
  </entry>
  
  <entry>
    <title>通过Cloudera Manager安装CDH</title>
    <link href="/2013/06/24/install-cdh-by-cloudera-manager"/>
    <updated>2013-06-24T00:00:00+08:00</updated>
    <id>/2013/06/24/install-cdh-by-cloudera-manager</id>
    <content type="html">1 方法一

你可以从https://ccp.cloudera.com/display/SUPPORT/Downloads下载cloudera-manager-installer.bin，然后修改执行权限并执行该脚本。

该脚本中配置的rhel6的yum源为：http://archive.cloudera.com/cm4/redhat/6/x86_64/cm/4/，下载的过程必须连网并且rpm的过程会非常慢，这种方法对虚拟机或者是无法连网的内网机器来说根本无法使用。

因为知道所有的rpm都在上面网址可以下载到，故你可以手动下载这些rpm然后手动安装，详细过程请参考：通过cloudera...</content>
  </entry>
  
  <entry>
    <title>HBase笔记：存储结构</title>
    <link href="/2013/06/15/hbase-note-about-data-structure"/>
    <updated>2013-06-15T00:00:00+08:00</updated>
    <id>/2013/06/15/hbase-note-about-data-structure</id>
    <content type="html">从HBase的架构图上可以看出，HBase中的存储包括HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog等，本篇文章统一介绍他们的作用即存储结构。

以下是网络上流传的HBase存储架构图:



HBase中的每张表都通过行键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理，而HRegion的分配由HMaster管理。

HMaster的作用：


为Region server分配region
负责Region s...</content>
  </entry>
  
  <entry>
    <title>Java笔记：单例模式</title>
    <link href="/2013/06/09/note-about-java-singleton-model"/>
    <updated>2013-06-09T00:00:00+08:00</updated>
    <id>/2013/06/09/note-about-java-singleton-model</id>
    <content type="html">什么是单例模式呢？就是在整个系统中，只有一个唯一存在的实例。使用Singleton的好处还在于可以节省内存，因为它限制了实例的个数，有利于Java垃圾回收。

单例模式主要有3个特点：


1、单例类确保自己只有一个实例。
2、单例类必须自己创建自己的实例。
3、单例类必须为其他对象提供唯一的实例。


单例模式的实现方式有五种方法：懒汉，恶汉，双重校验锁，枚举和静态内部类。

懒汉模式：
public class Singleton {
    private static Singleton instance;
    private Singleton (){}

    publ...</content>
  </entry>
  
  <entry>
    <title>Java笔记：IO</title>
    <link href="/2013/06/09/note-about-java-io"/>
    <updated>2013-06-09T00:00:00+08:00</updated>
    <id>/2013/06/09/note-about-java-io</id>
    <content type="html">说明，本文内容来源于java io系列01之 &amp;quot;目录&amp;quot;，做了一些删减。

Java库的IO分为输入/输出两部分。

早期的Java 1.0版本的输入系统是InputStream及其子类，输出系统是OutputStream及其子类。

后来的Java 1.1版本对IO系统进行了重新设计。输入系统是Reader及其子类，输出系统是Writer及其子类。

Java1.1之所以要重新设计，主要是为了添加国际化支持(即添加了对16位Unicode码的支持)。具体表现为Java 1.0的IO系统是字节流，而Java 1.1的IO系统是字符流。

字节流，就是数据流中最小的数据单...</content>
  </entry>
  
  <entry>
    <title>Java笔记：工厂模式</title>
    <link href="/2013/06/09/note-about-java-factory-model"/>
    <updated>2013-06-09T00:00:00+08:00</updated>
    <id>/2013/06/09/note-about-java-factory-model</id>
    <content type="html">工厂模式主要是为创建对象提供接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。

工厂模式在《Java与模式》中分为三类：


1)简单工厂模式(Simple Factory)：不利于产生系列产品。
2)工厂方法模式(Factory Method)：又称为多形性工厂。
3)抽象工厂模式(Abstract Factory)：又称为工具箱，产生产品族，但不利于产生新的产品。


GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式(Factory Method)与抽象工厂模式(Abstract Factory)。将简单工厂模式(Simple Factory)看为工厂方法...</content>
  </entry>
  
  <entry>
    <title>Java笔记：多线程</title>
    <link href="/2013/06/08/note-about-java-thread"/>
    <updated>2013-06-08T00:00:00+08:00</updated>
    <id>/2013/06/08/note-about-java-thread</id>
    <content type="html">线程：进程中并发的一个顺序执行流程。

并发原理：CPU分配时间片，多线程交替运行。宏观并行，微观串行。

多线程间堆空间共享，栈空间独立。堆存的是地址，栈存的是变量（如：局部变量）。

创建线程两种方式：继承Thread类或实现Runnable接口。

Thread对象代表一个线程。

多线程共同访问的同一个对象（临界资源），如果破坏了不可分割的操作（原子操作），就会造成数据不一致的情况。

线程状态图



说明：
线程共包括以下5种状态。


1. 新建状态(New)： 线程对象被创建后，就进入了新建状态。例如，Thread thread = new Thread()。
2. 就绪...</content>
  </entry>
  
  <entry>
    <title>Java笔记：集合框架实现原理</title>
    <link href="/2013/06/08/java-collection-framework"/>
    <updated>2013-06-08T00:00:00+08:00</updated>
    <id>/2013/06/08/java-collection-framework</id>
    <content type="html">
这篇文章是对http://www.cnblogs.com/skywang12345/category/455711.html中java集合框架相关文章的一个总结，在此对原作者的辛勤整理表示感谢。


Java集合是java提供的工具包，包含了常用的数据结构：集合、链表、队列、栈、数组、映射等。Java集合工具包位置是java.util.*

Java集合主要可以划分为4个部分：List列表、Set集合、Map映射、工具类(Iterator迭代器、Enumeration枚举类、Arrays和Collections)。

Java集合工具包框架图(如下)：



Collection是一个...</content>
  </entry>
  
  <entry>
    <title>Kettle访问IDH2.3中的HBase</title>
    <link href="/2013/04/17/access-idh-2.3-hbase-in-kettle"/>
    <updated>2013-04-17T00:00:00+08:00</updated>
    <id>/2013/04/17/access-idh-2.3-hbase-in-kettle</id>
    <content type="html">摘要

Kettle是一款国外开源的ETL工具，纯java编写，可以在Window、Linux、Unix上运行，绿色无需安装，数据抽取高效稳定。big-data-plugin是kettle中用于访问bigdata，包括hadoop、cassandra、mongodb等nosql数据库的一个插件。

截至目前，kettle的版本为4.4.1，big-data-plugin插件支持cloudera CDH3u4、CDH4.1，暂不支持Intel的hadoop发行版本IDH。

本文主要介绍如何让kettle支持IDH的hadoop版本。

方法

假设你已经安装好IDH-2.3的集群，并已经...</content>
  </entry>
  
  <entry>
    <title>Kettle中添加一个参数字段到输出</title>
    <link href="/2013/04/07/add-a-field-from-paramter-to-output"/>
    <updated>2013-04-07T00:00:00+08:00</updated>
    <id>/2013/04/07/add-a-field-from-paramter-to-output</id>
    <content type="html">kettle可以将输入流中的字段输出到输出流中，输入输出流可以为数据库、文件或其他，通常情况下输入流中字段为已知确定的，如果我想在输出流中添加一个来自转换的命令行参数的一个字段，该如何操作？

上述问题可以拆分为两个问题：


从命令行接受一个参数作为一个字段
合并输入流和这个字段


问题1

第一个问题可以使用kettle中获取系统信息组件，定义一个变量，该值来自命令行参数，见下图：



问题2

第二个问题可以使用kettle中记录关联 (笛卡尔输出)组件将两个组件关联起来，输出一个笛卡尔结果集，关联条件设定恒为true，在运行前设置第一个参数的值，然后运行即可。



下载脚本...</content>
  </entry>
  
  <entry>
    <title>使用yum安装CDH Hadoop集群</title>
    <link href="/2013/04/06/install-cloudera-cdh-by-yum"/>
    <updated>2013-04-06T00:00:00+08:00</updated>
    <id>/2013/04/06/install-cloudera-cdh-by-yum</id>
    <content type="html">Update:


2014.07.21 添加 lzo 的安装
2014.05.20 修改cdh4为cdh5进行安装。
2014.10.22 添加安装 cdh5.2 注意事项。


1、cdh5.2 发布了，其中 YARN 的一些配置参数做了修改，需要特别注意。
2、Hive 的元数据如果使用 PostgreSql9.X，需要设置 standard_conforming_strings 为 off



环境


CentOS 6.4 x86_64
CDH 5.2.0
jdk1.6.0_31


集群规划为3个节点，每个节点的ip、主机名和部署的组件分配如下：
    192.168.5...</content>
  </entry>
  
  <entry>
    <title>安装Impala过程</title>
    <link href="/2013/03/29/install-impala"/>
    <updated>2013-03-29T00:00:00+08:00</updated>
    <id>/2013/03/29/install-impala</id>
    <content type="html">与Hive类似，Impala也可以直接与HDFS和HBase库直接交互。只不过Hive和其它建立在MapReduce上的框架适合需要长时间运行的批处理任务。例如：那些批量提取，转化，加载（ETL）类型的Job，而Impala主要用于实时查询。

Hadoop集群各节点的环境设置及安装过程见 使用yum安装CDH Hadoop集群，参考这篇文章。

1. 环境


CentOS 6.4 x86_64
CDH 5.0.1
jdk1.6.0_31


集群规划为3个节点，每个节点的ip、主机名和部署的组件分配如下：
192.168.56.121        cdh1     NameNode...</content>
  </entry>
  
  <entry>
    <title>手动安装Cloudera Hive CDH</title>
    <link href="/2013/03/24/manual-install-Cloudera-hive-CDH"/>
    <updated>2013-03-24T00:00:00+08:00</updated>
    <id>/2013/03/24/manual-install-Cloudera-hive-CDH</id>
    <content type="html">本文主要记录手动安装Cloudera Hive集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，hadoop各个组件和jdk版本如下：
    hadoop-2.0.0-cdh4.6.0
    hbase-0.94.15-cdh4.6.0
    hive-0.10.0-cdh4.6.0
    jdk1.6.0_38

hadoop各组件可以在这里下载。

集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：
    192.168.0.1        desktop1     NameNode、Hive、Resou...</content>
  </entry>
  
  <entry>
    <title>手动安装Cloudera HBase CDH</title>
    <link href="/2013/03/24/manual-install-Cloudera-hbase-CDH"/>
    <updated>2013-03-24T00:00:00+08:00</updated>
    <id>/2013/03/24/manual-install-Cloudera-hbase-CDH</id>
    <content type="html">本文主要记录手动安装Cloudera HBase集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，hadoop各个组件和jdk版本如下：
    hadoop-2.0.0-cdh4.6.0
    hbase-0.94.15-cdh4.6.0
    hive-0.10.0-cdh4.6.0
    jdk1.6.0_38

hadoop各组件可以在这里下载。

集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：
    192.168.0.1        desktop1     NameNode、Hive、Reso...</content>
  </entry>
  
  <entry>
    <title>手动安装Cloudera Hadoop CDH</title>
    <link href="/2013/03/24/manual-install-Cloudera-Hadoop-CDH"/>
    <updated>2013-03-24T00:00:00+08:00</updated>
    <id>/2013/03/24/manual-install-Cloudera-Hadoop-CDH</id>
    <content type="html">安装版本

hadoop各个组件和jdk版本如下：
    hadoop-2.0.0-cdh4.6.0
    hbase-0.94.15-cdh4.6.0
    hive-0.10.0-cdh4.6.0
    jdk1.6.0_38

hadoop各组件可以在这里下载。

安装前说明


确定安装目录为/opt
检查hosts文件是否设置集群各节点的hostname和ip映射
关闭每个节点的防火墙
设置每个节点时钟同步


规划

集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下：
    192.168.0.1        desktop1     NameNod...</content>
  </entry>
  
  <entry>
    <title>【笔记】Hadoop安装部署</title>
    <link href="/2013/03/08/note-about-installing-hadoop-cluster"/>
    <updated>2013-03-08T00:00:00+08:00</updated>
    <id>/2013/03/08/note-about-installing-hadoop-cluster</id>
    <content type="html">安装虚拟机

使用VirtualBox安装rhel6.3，存储为30G，内存为1G，并使用复制克隆出两个新的虚拟机，这样就存在3台虚拟机，设置三台虚拟机的主机名称，如：rhel-june、rhel-june-1、rhel-june-2

配置网络

a. VirtualBox全局设定-网络中添加一个新的连接：vboxnet0

b. 设置每一个虚拟机的网络为Host-Only

c.分别修改每个虚拟机的ip，DHCP或手动设置
vim /etc/sysconfig/network-scripts/ifcfg-eth0
vim /etc/udev/rules.d/70-persistent...</content>
  </entry>
  
  <entry>
    <title>2012年度总结</title>
    <link href="/2013/02/20/summary-of-the-work-in-2012"/>
    <updated>2013-02-20T00:00:00+08:00</updated>
    <id>/2013/02/20/summary-of-the-work-in-2012</id>
    <content type="html">2012年是在公司工作的第二年，在总结2012年的得与失的时候，有必要和《2011的度年终总结》相比较，在比较中审视自己在2012年是否有改进2011年存在的不足、是否有实现2011年定下的2012年工作计划。
以下是2012年相对于2011年的一些变化。


 2011年，在调研云计算产品过程中，深刻的意识到自身在linux方面存在的不足；2012年，熟悉了基本的linux命令，能够读懂并编写简单shell脚本；
  2011年，工作环境是win7+fedora；2012年，一直使用fedora操作系统工作、编码；
  2011年，较多的时间花在编写代码、完成开发任务上；2012年，更...</content>
  </entry>
  
  <entry>
    <title>使用Octopress将博客从wordpress迁移到GitHub</title>
    <link href="/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress"/>
    <updated>2012-06-03T00:00:00+08:00</updated>
    <id>/2012/06/03/migrate-blog-form-wordpress-to-github-with-octopress</id>
    <content type="html">Step1 - 在本机安装Octopress

首先，必须先在本机安装配置Git和Ruby,Octopress需要Ruby版本至少为1.9.2。你可以使用RVM或rbenv安装ruby，安装方法见Octopress官方文档：http://octopress.org/docs/setup/

我使用rvm安装：
    rvm install 1.9.2 &amp;amp;&amp;amp; rvm use 1.9.2
安装完之后可以查看ruby版本：
    ruby --version
结果为：
    ruby 1.9.2p320 (2012-04-20 revision 35421) [x86_6...</content>
  </entry>
  
  <entry>
    <title>Kettle dependency management</title>
    <link href="/2012/04/13/kettle-dependency-management"/>
    <updated>2012-04-13T00:00:00+08:00</updated>
    <id>/2012/04/13/kettle-dependency-management</id>
    <content type="html">pentaho的项目使用了ant和ivy解决项目依赖,所以必须编译源码需要ivy工具.直接使用ivy编译pentaho的bi server项目,一直没有编译成功.
使用ivy编译kettle的源代码却是非常容易的事情.

该篇文章翻译并参考了Will Gorman在pentaho的wiki上添加的Kettle dependency management,文章标题没作修改.
编写此文,是为了记录编译kettle源码的方法和过程.

以下是对原文的一个简单翻译.
将kettle作为一个产品发行是一个很有趣的事情.有很多来自于pentaho其他项目(其中有一些有依赖于kettle)的jar包被...</content>
  </entry>
  
  <entry>
    <title>哈希表</title>
    <link href="/2012/03/26/hash-and-hash-functions"/>
    <updated>2012-03-26T00:00:00+08:00</updated>
    <id>/2012/03/26/hash-and-hash-functions</id>
    <content type="html">定义

一般的线性表、树，数据在结构中的相对位置是随机的，即和记录的关键字之间不存在确定的关系，因此，在结构中查找记录时需进行一系列和关键字的比较。这一类查找方法建立在“比较“的基础上，查找的效率依赖于查找过程中所进行的比较次数。 若想能直接找到需要的记录，必须在记录的存储位置和它的关键字之间建立一个确定的对应关系f，使每个关键字和结构中一个唯一的存储位置相对应，这就是哈希表。

哈希表又称散列表。哈希表存储的基本思想是：以数据表中的每个记录的关键字 k为自变量，通过一种函数H(k)计算出函数值。把这个值解释为一块连续存储空间（即数组空间）的单元地址（即下标），将该记录存储到这个单元中。...</content>
  </entry>
  
  <entry>
    <title>如何在Kettle4.2上面实现cassandra的输入与输出</title>
    <link href="/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2"/>
    <updated>2012-03-23T00:00:00+08:00</updated>
    <id>/2012/03/23/how-to-implement-cassandra-input-and-output-in-kettle4-2</id>
    <content type="html">这是在QQ群里有人问到的一个问题。

如何在pdi-ce-4.2.X-stable上面实现cassandra的输入与输出,或是实现hadoop,hbase,mapreduce,mongondb的输入输出?

在kettle中实现cassandra的输入与输出有以下两种方式:


第一种方式:自己编写cassandra输入输出组件
第二种方式:使用别人编写好的插件,将其集成进来


当然还有第三种方法,直接使用4.3版本的pdi.

第一种方法需要对cassandra很熟悉编写插件才可以做到,第二种方法可以通过拷贝pdi-ce-big-data-4.3.0-preview中的文件来完成.
...</content>
  </entry>
  
  <entry>
    <title>2011年度年终总结</title>
    <link href="/2012/02/26/summary-of-the-work-in-2011"/>
    <updated>2012-02-26T00:00:00+08:00</updated>
    <id>/2012/02/26/summary-of-the-work-in-2011</id>
    <content type="html">2011年工作总结，只能算是七个月的工作总结，在这七个月里学到了许多、收获了许多、感悟了许多。以下是对这六个月的一个回顾与总结。

来公司最初的两个月主要负责云计算相关产品的调研工作，相关云计算产品有：Eucalyptus、OpenNebula、OpenStack。在这两个月里，对云计算的原理、服务、架构以及安装和部署有了初步的了解于实践，并积累了一些文档。在一次又一次的安装部署过程中，体验到了失败的痛苦、无助的迷茫、成功的喜悦，深刻的意识到自身在linux方面存在的不足；强烈的感觉到有必要学习一些语言，如Shell、Python、Groovy、Ruby等等。2012年，打算将工作环境切...</content>
  </entry>
  
  <entry>
    <title>Seam的启动过程</title>
    <link href="/2012/02/23/the-process-of-seam-initiation"/>
    <updated>2012-02-23T00:00:00+08:00</updated>
    <id>/2012/02/23/the-process-of-seam-initiation</id>
    <content type="html">了解seam2的人知道，seam是通过在web. xml中配置监听器启动的。注意，本文中的seam是指的seam2，不是seam3. 
&amp;lt;listener&amp;gt;
    &amp;lt;listenerclass&amp;gt;org. jboss. seam. servlet. SeamListener&amp;lt;/listenerclass&amp;gt;
&amp;lt;/listener&amp;gt;

该监听器会做哪些事情呢？看看Gavin King对SeamListener类的描述。

Drives certain Seam functionality such as initialization and c...</content>
  </entry>
  
  <entry>
    <title>Kettle运行作业之前的初始化过程</title>
    <link href="/2012/02/22/the-init-process-before-job-execution"/>
    <updated>2012-02-22T00:00:00+08:00</updated>
    <id>/2012/02/22/the-init-process-before-job-execution</id>
    <content type="html">本文主要描述Kettle是如何通过GUI调用代码启动线程执行作业的。

之前用英文写了一篇文章《The execution process of kettle’s job》 ，这篇文章只是用于英语写技术博客的一个尝试。由于很久没有使用英语写作了，故那篇文章只是简单的通过UML的序列图描述kettle运行job的一个java类调用过程。将上篇文章的序列图和这篇文章联系起来，会更加容易理解本文。

在Spoon界面点击运行按钮，Spoon GUI会调用Spoon.runFile()方法，这可以从xul文件（ui/menubar.xul）中的描述看出来。关于kettle中的xul的使用，不是本...</content>
  </entry>
  
  <entry>
    <title>The execution process of kettle’s job</title>
    <link href="/2012/02/21/the-execution-process-of-kettles-job"/>
    <updated>2012-02-21T00:00:00+08:00</updated>
    <id>/2012/02/21/the-execution-process-of-kettles-job</id>
    <content type="html">How to execute a kettle job in Spoon GUI or command line after we create a job in Spoon GUI? In Spoon GUI,the main class is &quot;org.pentaho.di.ui.spoon.Spoon.java&quot;.This class handles the main window of the Spoon graphical transformation editor.Many operations about a job or transformation such as ru...</content>
  </entry>
  
  <entry>
    <title>Kettle中定义错误处理</title>
    <link href="/2012/02/17/step-error-handling-in-kettle"/>
    <updated>2012-02-17T00:00:00+08:00</updated>
    <id>/2012/02/17/step-error-handling-in-kettle</id>
    <content type="html">在kettle执行的过程中，如果遇到错误，kettle会停止运行。在某些时候，并不希望kettle停止运行，这时候可以使用错误处理（Step Error Handling）。错误处理允许你配置一个步骤来取代出现错误时停止运行一个转换，出现错误的记录行将会传递给另一个步骤。在Step error handling settings对话框里，需要设置启用错误处理。

下面例子中读取postgres数据库中的a0表数据，然后输出到a1表：




a1表结构如下：

CREATE TABLE a1
(
  a double precision,
  id integer NOT NULL,
 ...</content>
  </entry>
  
  <entry>
    <title>JSF中EL表达式之this扩展</title>
    <link href="/2012/02/14/this-expression-of-jsf-el"/>
    <updated>2012-02-14T00:00:00+08:00</updated>
    <id>/2012/02/14/this-expression-of-jsf-el</id>
    <content type="html">本篇文章来自以前公司的一套jsf+seam+Hibernate的一套框架，其对jsf进行了一些改进，其中包括:EL表达式中添加this，通过jsf的渲染实现权限控制到按钮等等。JSF表达式中添加this，主要是为了在facelets页面使用this关键字引用（JSF自动查找）到当前页面对应的pojo类，详细说明见下午。因为，本文的文章是公司同事整理的，本文作者仅仅是将其分享出来，供大家参考思路，如果有什么不妥的话，请告知。

EL表达式this扩展
在业务系统中，大量页面具有大量区域是相似或者相同的，或者可能根据某些局部特征的变化具有一定的变化，jsf中通过facelet模板功能可以达到...</content>
  </entry>
  
  <entry>
    <title>使用Kettle数据迁移添加主键和索引</title>
    <link href="/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle"/>
    <updated>2012-01-05T00:00:00+08:00</updated>
    <id>/2012/01/05/add-primary-keys-and-indexes-when-migrating-datas-whith-kettle</id>
    <content type="html">Kettle是一款国外开源的etl工具，纯java编写，绿色无需安装，主要用于数据抽取、转换、装载。kettle兼容了市面上几十种数据库，故用kettle来做数据库的迁移视乎是个不错的选择。

kettle的数据抽取主要在于抽取数据，而没有考虑数据库的函数、存储过程、视图、表结构以及索引、约束等等，而这些东西恰恰都是数据迁移需要考虑的事情。当然，如果在不考虑数据库中的函数、存储过程、视图的情况下，使用kettle进行数据的迁移还算是一个可行的方案。

这篇文章主要是讲述在使用kettle进行数据库的迁移的时候如何迁移主键和索引，为什么要迁移主键和索引？异构数据库之间的迁移很难无缝的实现自...</content>
  </entry>
  
  <entry>
    <title>kettle进行数据迁移遇到的问题</title>
    <link href="/2012/01/04/some-problems-about-migrating-database-datas-with-kettle"/>
    <updated>2012-01-04T00:00:00+08:00</updated>
    <id>/2012/01/04/some-problems-about-migrating-database-datas-with-kettle</id>
    <content type="html">使用kettle进行oracle或db2数据导入到mysql或postgres数据库过程中遇到以下问题，以下只是一个简单描述，详细的说明以及所做的代码修改没有提及。下面所提到的最新的pdi程序是我修改kettle源码并编译之后的版本。

同时运行两个pdi程序，例如：一个为oracle到mysql，另一个为oracle到postgres，其中一个停止运行


原因：从oracle迁移到mysql创建的作业和转换文件和oracle到postgres的作业和转换保存到一个路径，导致同名称的转换相互之间被覆盖，故在运行时候会出现混乱。
解决办法：将新建的作业和转换分别保存在两个不同的路径，最好...</content>
  </entry>
  
  <entry>
    <title>Mondrian and OLAP</title>
    <link href="/2011/12/07/mondrian-and-olap"/>
    <updated>2011-12-07T00:00:00+08:00</updated>
    <id>/2011/12/07/mondrian-and-olap</id>
    <content type="html">Mondrian是一个用Java编写的OLAP引擎。他执行用MDX语言编写的查询，从关系数据库（RDBMS）中读取数据并且通过Java API以多维度的格式展示查询结果。

Online Analytical Processing
联机分析处理（OLAP）指在线实时的分析大量数据。与联机事务处理系统（On-Line Transaction Processing，简称OLTP）不同，OLTP中典型的操作如读和修改单个的少量的记录，而OLAP批量处理数据并且所有操作都是只读的。“online”意味着即使是处理大量的数据----百万条数据记录，占有几个GB内存----系统必须足够快的反回查询结...</content>
  </entry>
  
  <entry>
    <title>XUL 用户界面语言介绍</title>
    <link href="/2011/11/25/xml-user-interface-language-introuction"/>
    <updated>2011-11-25T00:00:00+08:00</updated>
    <id>/2011/11/25/xml-user-interface-language-introuction</id>
    <content type="html">XUL[1]是英文“XML User Interface Language”的首字母缩写。它是为了支持Mozilla系列的应用程序（如Mozilla Firefox和Mozilla Thunderbird）而开发的用户界面标示语言。顾名思义，它是一种应用XML来描述用户界面的标示语言。
XUL是开放标准，重用了许多现有的标准和技术[2]，包括CSS、JavaScript、DTD和RDF等。所以对于有网络编程和设计经验的人士来说，学习XUL比学习其他用户界面标示语言相对简单。
使用XUL的主要好处在于它提供了一套简易和跨平台的widget定义。这节省了编程人员在开发软件时所付出的努力。

...</content>
  </entry>
  
  <entry>
    <title>在eclipse中构建Pentaho BI Server工程</title>
    <link href="/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse"/>
    <updated>2011-09-28T00:00:00+08:00</updated>
    <id>/2011/09/28/build-pentaho-bi-server-source-code-in-eclipse</id>
    <content type="html">首先需要说明的是，Pentaho BI Server源代码在svn://source.pentaho.org/svnroot/bi-platform-v2/trunk/，并且用ivy构建。ivy没有用过也不熟悉，故不打算从这里使用ivy构建源码。

当然，您可以参考官方文档构建源码。

Pentaho BI Server打包后的文件存于这里，其中包括（本文使用的是3.9.0版本）：biserver-ce-3.9.0-stable.zip，bi-platform-3.9.0-stable-sources.zip，biserver-ce-3.9.0-stable-javadoc.zip。

...</content>
  </entry>
  
  <entry>
    <title>Pentaho现场支持遇到问题及解决办法</title>
    <link href="/2011/09/26/resolved-pentaho-problems-9-16"/>
    <updated>2011-09-26T00:00:00+08:00</updated>
    <id>/2011/09/26/resolved-pentaho-problems-9-16</id>
    <content type="html">很久没写文章了，最近在关注Pentaho。
 以下是9月16日现场提出的问题解决办法：
      1、PDF预览中文没显示，txt预览中文乱码：
           1）、设置File-&gt;Configuration -&gt;output-pageable-pdf的encoding 为Identity-H
           2）、将需要输出中文的报表项目的字体设置为中文字体，例如宋体
           3）、如要发布到服务器，需要修改如下的配置：
             pentaho/server/biserver-ee/tomcat/webapps/pentaho/WEB-I...</content>
  </entry>
  
  <entry>
    <title>在Fedora 15 上搭建Eucalyptus</title>
    <link href="/2011/08/20/install-eucalyptus-on-fedora-15"/>
    <updated>2011-08-20T00:00:00+08:00</updated>
    <id>/2011/08/20/install-eucalyptus-on-fedora-15</id>
    <content type="html">
在Fedora 15 上搭建Eucalyptus平台，在Fedora 15 上搭建Eucalyptus与在Centos上搭建Eucalyptus有什么区别呢？参照这篇文章Installing Eucalyptus (2.0) on Fedora 12，然后注意一些细节，视乎就能安装成功。不管你信不信，我是在虚拟机中安装fedora15，然后安装Eucalyptus失败了，失败的原因是xen的网络没有配置好，查看资源的时候free / max都为0000.

毕竟是第一次接触云计算，第一次接触XEN，第一次接触Eucalyptus，Eucalyptus改装的都装了，就是XEN的网络没有配...</content>
  </entry>
  
  <entry>
    <title>Export DhtmlxGrid to PDF in Java</title>
    <link href="/2011/08/11/export-dhtmlxgrid-to-pdf-in-java"/>
    <updated>2011-08-11T00:00:00+08:00</updated>
    <id>/2011/08/11/export-dhtmlxgrid-to-pdf-in-java</id>
    <content type="html">将DhtmlxGrid数据导出到pdf这是很常见的需求，dhtmlx官网提供了php和java版本的例子，你可以去官网查看这篇文章《Grid-to-Excel, Grid-to-PDF Available for Java》，你可以从以下地址下载导出程序源码：
Export to Excel
Export to PDF
当然，还有一个示例工程： .zip archive with an example

XML2PDF和XML2Excel工程内代码很相似，XML2PDF内部使用了PDFjet.jar导出PDF，而XML2Excel使用JXL导出Excel。
需要说明的是，还需要引入dht...</content>
  </entry>
  
  <entry>
    <title>自定义dhtmlxGrid表头菜单</title>
    <link href="/2011/07/31/custom-dhtmlxgrid-header-menu"/>
    <updated>2011-07-31T00:00:00+08:00</updated>
    <id>/2011/07/31/custom-dhtmlxgrid-header-menu</id>
    <content type="html">dhtmlxGrid可以定义表头菜单以及表格右键菜单，表格右键菜单可以自定义，但是表头菜单只能使用其提供的菜单。dhtmlxGrid默认的表头菜单可以决定表格中每一列是否在表格中显示，并没有提供更多的扩展，如果我想自定义表头菜单，该怎么做呢？本文就是基于自定义表格菜单，说说我的实现方式。
以下是dhtmlxGrid的表头菜单效果：


其功能过于单一，以下是表格右键菜单效果：

如果能够像表格菜单一样自定义表头菜单，那会是一件非常有意义的事情，因为dhtmlxGrid菜单都是一些针对行和单元格的操作，没有提过针对列的操作，比如我可能需要在某一列上实现该列的显示与隐藏、排序、改变列属性以及...</content>
  </entry>
  
  <entry>
    <title>Drag an item to dhtmlxGrid and add a column</title>
    <link href="/2011/07/24/drag-an-item-to-dhtmlxgrid-and-add-a-column"/>
    <updated>2011-07-24T00:00:00+08:00</updated>
    <id>/2011/07/24/drag-an-item-to-dhtmlxgrid-and-add-a-column</id>
    <content type="html">dhtmlxGrid支持tree和grid、grid之间、grid内部进行拖拽，如在grid内部进行拖拽，可以增加一行；在grid之间拖拽，第一个grid的记录删除，第二个grid增加一行记录。如果我想在拖拽之后不是添加一行而是一列，该怎么做呢？
现在有个需求，就是左边有个tree，右边有个grid，将左边tree的一个节点拖到右边grid的表头并动态增加一列。这个怎么做呢？
如果你想快点看到最后的实现方法，你可以直接跳到本文的最后参看源码。
首先看看dhtmlxTree 关于Drag-n-Drop的例子，其中有这样一个例子Custom Drag Out。
上面的例子，右边定义了一个输入...</content>
  </entry>
  
  <entry>
    <title>DhtmlxGrid Quick Start Guide</title>
    <link href="/2011/07/19/dhtmlxgrid-quick-start-guide"/>
    <updated>2011-07-19T00:00:00+08:00</updated>
    <id>/2011/07/19/dhtmlxgrid-quick-start-guide</id>
    <content type="html">说明:

本文来源于http://dhtmlx.com/docs/products/dhtmlxGrid/，本人对其进行翻译整理成下文，贴出此文，紧供分享。

dhtmlxGrid是一个拥有强大的数据绑定、优秀的大数据展示性能并支持ajax的JavaScript表格控件。该组件易于使用并通过富客户端的API提供了很大的扩展性。dhtmlxGrid支持不同的数据源（XML, JSON, CSV, JavaScript 数组和HTML表格），如果需要的话，还可以从自定义的xml中加载数据。
跨浏览器
使用JavaScript API进行控制
Ajax支持
简单的JavaScript 或者XM...</content>
  </entry>
  
  <entry>
    <title>网上收集的关于OpenStack的一些资源</title>
    <link href="/2011/07/07/some-resources-about-openstack"/>
    <updated>2011-07-07T00:00:00+08:00</updated>
    <id>/2011/07/07/some-resources-about-openstack</id>
    <content type="html">
OpenStack Nova code：https://bugs.launchpad.net/nova


OpenStack Blog：http://planet.openstack.org/

OpenStack 官方文档：http://docs.openstack.org/cactus/openstack-compute/admin/content/ch_getting-started-with-openstack.html

OpenStack 中国门户：http://blu001068.chinaw3.com/bbs/portal.php

在 Ubuntu 上安装和配置 O...</content>
  </entry>
  
  <entry>
    <title>Centos上安装 OpenNebula Management Console</title>
    <link href="/2011/06/29/install-opennebula-management-console-in-centos5-6"/>
    <updated>2011-06-29T00:00:00+08:00</updated>
    <id>/2011/06/29/install-opennebula-management-console-in-centos5-6</id>
    <content type="html">我们可以通过onehost/onevm/onevnet等等 这些命令行工具来管理 OpenNebula 云计算平台，也可以通过OpenNebula项目组开发的web控制台来访问OpenNebula。OpenNebula项目组提供了两个web程序来管理OpenNebula，一个即本文提到的OpenNebula Management Console，一个是The Cloud Operations Center，前者需要额外下载，后者内嵌与OpenNebula安装包内。

OpenNebula 2.2提供的文档相对较少并且零散，在网上可以找到一篇关于OpenNebula Management ...</content>
  </entry>
  
  <entry>
    <title>OpenNebula 2.2的特性</title>
    <link href="/2011/06/26/opennebula-2-2-features"/>
    <updated>2011-06-26T00:00:00+08:00</updated>
    <id>/2011/06/26/opennebula-2-2-features</id>
    <content type="html">以下这篇文章由OpenNebula 2.2 Features翻译而来。

OpenNebula是一款为云计算而打造的开源工具箱。它允许你与Xen，KVM或VMware ESX一起建立和管理私有云，同时还提供Deltacloud适配器与Amazon EC2相配合来管理混合云。除了像Amazon一样的商业云服务提供商，在不同OpenNebula实例上运行私有云的Amazon合作伙伴也同样可以作为远程云服务供应商。

目前版本，可支持XEN、KVM和VMware，以及实时存取EC2和 ElasticHosts，它也支持印象档的传输、复制和虚拟网络管理网络。
主要特点和优势
私有云计算 

为私...</content>
  </entry>
  
  <entry>
    <title>Eucalyptus使用的技术</title>
    <link href="/2011/06/22/the-technology-used-in-eucalyptus"/>
    <updated>2011-06-22T00:00:00+08:00</updated>
    <id>/2011/06/22/the-technology-used-in-eucalyptus</id>
    <content type="html">
libvirt


Libvirt 库是一种实现 Linux 虚拟化功能的 Linux® API，它支持各种虚拟机监控程序，包括 Xen 和 KVM，以及 QEMU 和用于其他操作系统的一些虚拟产品。


Netty


Netty 提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。


Axis2


Axis2是下一代 Apache Axis。Axis2 虽然由 Axis 1.x 处理程序模型提供支持，但它具有更强的灵活性并可扩展到新的体系结构。Axis2 基于新的体系结构进行了全新编写，而且没有采用 Axis 1.x 的常用代码。...</content>
  </entry>
  
  <entry>
    <title>Eucalyptus EE的介绍及功能说明</title>
    <link href="/2011/06/22/the-introduction-of-eucalyptus-ee-features-and-functions"/>
    <updated>2011-06-22T00:00:00+08:00</updated>
    <id>/2011/06/22/the-introduction-of-eucalyptus-ee-features-and-functions</id>
    <content type="html">Eucalyptus企业版2.0是一个基于Linux的软件架构，在企业现有的IT架构上实现一个可扩展的、提高效率的私有和混合云。Eucalyptus作为基础设施提供IaaS服务。这意味着用户可以通过Eucalyptus自助服务界面提供自己的资源（硬件、存储和网络）。一个Eucalyptus云是部署在企业的内部数据中心，由企业内部用户访问。因此，敏感数据可以在防火墙的保护下防止外部入侵。

Eucalyptus的设计目的是从根本上易于安装和尽可能没有侵扰。该软件高度模块化，具有行业标准，和语言无关。它提供了可以与EC2兼容的云计算平台和与S3兼容的云存储平台。
功能亮点

    无缝管理...</content>
  </entry>
  
  <entry>
    <title>接触云服务环境Eucalyptus</title>
    <link href="/2011/06/16/touch-cloud-environment-which-it-is-eucalyptus"/>
    <updated>2011-06-16T00:00:00+08:00</updated>
    <id>/2011/06/16/touch-cloud-environment-which-it-is-eucalyptus</id>
    <content type="html">最近在接触云计算平台，熟悉了Eucalyptus，并用其搭建云环境。通过网上的一些例子，逐渐的摸索出用Eucalyptus搭建云计算平台的方法。我所用的Eucalyptus是免费版，缺少很多企业版的功能。

Eucalyptus

Elastic Utility Computing Architecture for Linking Your Programs To Useful Systems （Eucalyptus） 是一种开源的软件基础结构，用来通过计算集群或工作站群实现弹性的、实用的云计算。它最初是美国加利福尼亚大学 Santa Barbara 计算机科学学院的一个研究项目，现在已...</content>
  </entry>
  
  <entry>
    <title>写给领导的一封信</title>
    <link href="/2010/08/06/a-letter-to-boss"/>
    <updated>2010-08-06T00:00:00+08:00</updated>
    <id>/2010/08/06/a-letter-to-boss</id>
    <content type="html">以下是关于汽车项目的一些思考与总结以及自己对进入公司以来的工作的回顾与总结。原来是想在项目结束的时候对这个项目进行总结，但是在发生昨天的事情之后，我就想在昨天晚上写这篇总结。后来想想如果在昨天就有以昨天的情绪写这篇文章的话，会有点对人不对事的感觉，所以就没有写了。在发生今天的事情之后，我觉得有必要把自己的想法写出来，所以有了下文。

等这个项目结束了，我想请假休息。

等这个项目结束了，我想请假休息，我想请假一天，睡一天觉也好，散散步也好，我想好好的放松一下疲惫的身躯、松弛一下紧张的大脑。

来公司四个月，除了有一次双休之外，每周都有6天在公司，每天都是电脑从早忙碌到晚，每天都是在想着工...</content>
  </entry>
  
  <entry>
    <title>还未发表的离职申请书</title>
    <link href="/2010/03/06/application-for-leave"/>
    <updated>2010-03-06T00:00:00+08:00</updated>
    <id>/2010/03/06/application-for-leave</id>
    <content type="html">敬爱的各位领导：

我很遗憾自己在这个时候向公司正式提出辞职。

自2009年9月16日入职以来，我一直都很珍视这个难得的工作机会，感谢部门各位领导对我的信任、栽培及包容，也感谢各位同事予的帮助和关心。

在过去的近4个月的时间里，利用公司给予的良好锻炼机会，在开发工作和社会交际等方面收获颇大，同时也丰富了阅览，锻炼了能力。工作上，技术上得到了提高。生活上，得到同事们的关照与帮助；思想上，得到领导与同事们的指导与帮助，有了更成熟与深刻的人生观。我对于公司提供的各方面关照表示真心的感谢！当然，我也自认为在工作当中已尽到自己的较大努力。

因为本人的离职而给公司造成的不便，本人深表歉意和遗憾...</content>
  </entry>
  
  <entry>
    <title>2010年工作计划</title>
    <link href="/2010/01/06/the-plan-of-2010"/>
    <updated>2010-01-06T00:00:00+08:00</updated>
    <id>/2010/01/06/the-plan-of-2010</id>
    <content type="html">2010年工作计划，这是2010年工作计划，不是2010年整年的人生计划。2010年，本命年，有太多的计划，太多的想法，只是不能一下子实现，变化总比计划快吧！人生的总总计划，就是在生活的纠结中，淡忘的。。。

2010年，难说什么时候会离开这个公司，不管怎样，在存在于这个公司的时间里，为公司、为同事、为自己总该做些什么，该为自己攒足钱还债，该为自己积累足够的跳槽经验，该慢慢的摸清以后事业发展的方向。

2010年工作计划：欢迎”大熊“拍砖，帮我截图，帮我顶贴，哈哈！

1：不管工作内容如何，工作心情如何，攒够万元，还清助学贷款费用。

2：熟悉BPM工作平台，摸清楚其底层以及应用层的实现...</content>
  </entry>
  
  <entry>
    <title>Extjs读取xml文件生成动态表格和表单(续)</title>
    <link href="/2009/10/31/ext_readxml_in_bjsasc_wuzi_continue"/>
    <updated>2009-10-31T00:00:00+08:00</updated>
    <id>/2009/10/31/ext_readxml_in_bjsasc_wuzi_continue</id>
    <content type="html">很多人向我要【Extjs读取xml文件生成动态表格和表单】一文的源代码，故花了些时间将源代码整理出来，并重新编写此文，分享当时的技术思路。

需要的文件有：


1.html文件，此处以SASC.search.MtrUse.html为例
2.Extjs相关文件,见SASC.search.MtrUse.html文件中的引用
3.工具类，DomUtils.js
4.核心js类:SASC.extjs.search.MtrUse.js
5.java代码


详细html和js代码见相关文件，这里先描述思路。

首先

通过一个事件打开一个弹出窗口，该窗口的url指向SASC.search.Mtr...</content>
  </entry>
  
  <entry>
    <title>Extjs读取xml文件生成动态表格和表单</title>
    <link href="/2009/10/22/ext_readxml_in_bjsasc_wuzi"/>
    <updated>2009-10-22T00:00:00+08:00</updated>
    <id>/2009/10/22/ext_readxml_in_bjsasc_wuzi</id>
    <content type="html">最近开发项目，需要动态读取xml文件，生成Extjs界面，xml文件通过前台页面的按钮事件传进来，可以在网上查找【javascript 弹出子窗口】的相关文章
获取弹出窗口url后的参数方法：
    // 获取url后的参数值
    function getQueryStringValue(name) {
        var url = window.location.search;
        if (url.indexOf(&amp;#39;?&amp;#39;) &amp;lt; 0) {
            return null
        }
        var index ...</content>
  </entry>
  
  <entry>
    <title>做最好的自己</title>
    <link href="/2009/09/26/be-best-of-myself"/>
    <updated>2009-09-26T00:00:00+08:00</updated>
    <id>/2009/09/26/be-best-of-myself</id>
    <content type="html">今天是传说中的万圣节？跟我有啥关系？没有关系，继续过自己的生活，follow my heart！

今天又看了看去年8月份写的个人简历记事本版，觉得胜似感慨！这份简历简单粗糙，真的没有脸面拿去应聘，也不那好意思贴出来。也许去年找工作，就受了简历的影响，而没有去跑过什么招聘会。按照当时的状态也实在是难以写出什么样的简历来，一没有炫耀的学历，二没有什么工作经验，学习一般，没参加什么社团活动；想参加国家电子设计大赛，来错了时间进错了班；想从事软件开发，起步晚不知从哪里入门！

谈到在公司的工作，现在主要是接触JavaScript和Extjs，用Extjs读取XML开发组件，替换公司系统中原有的...</content>
  </entry>
  
  <entry>
    <title>Hello Jekyll!</title>
    <link href="/2009/01/01/hello-jekyll"/>
    <updated>2009-01-01T00:00:00+08:00</updated>
    <id>/2009/01/01/hello-jekyll</id>
    <content type="html">1. 标题与文字格式

标题
# 测试 h1
## 测试 h2
### 测试 h3
#### 测试 h4
##### 测试 h5
###### 测试 h6

效果：

测试 h1

测试 h2

测试 h3

测试 h4

测试 h5

测试 h6

文字格式
**这是文字粗体格式**
*这是文字斜体格式*
~~在文字上添加删除线~~

2. 列表

无序列表
* 项目1
* 项目2
* 项目3

有序列表
1. 项目1
2. 项目2
3. 项目3
   * 项目1
   * 项目2

3. 代码

测试行代码： _post

测试段落代码：
/* hello world demo */...</content>
  </entry>
  
</feed>
