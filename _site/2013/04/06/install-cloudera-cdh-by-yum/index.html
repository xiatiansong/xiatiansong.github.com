<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>使用yum安装CDH Hadoop集群 - 夏天松的个人博客</title>
  <meta name="description" content="使用yum安装CDH Hadoop集群，包括hdfs、yarn、hive和hbase。">
  <meta name="keywords" content="java, hadoop, hive, hbase, spark, linux, scala, python, mysql">
  <meta name="author" content="夏天松">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="360-site-verification" content="9b7a87a1d52051c96644f0a9b8b79898" />
  <meta name="sogou_site_verification" content="ofwXWFdthV"/>

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" >

  <link rel="canonical" href="http://blog.xiatiansong.com/2013/04/06/install-cloudera-cdh-by-yum">
  <link rel="stylesheet" href="/static/css/bootstrap.min.css" media="all">
  <link rel="stylesheet" href="/static/css/style.css" media="all">
  <link rel="stylesheet" href="/static/css/pygments.css" media="all">
  <link rel="stylesheet" href="/static/css/font-awesome.css" media="all">

  <!-- atom & rss feed -->
  <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="夏天松的个人博客 RSS Feed">
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="夏天松的个人博客 ATOM Feed">
</head>


  <body>
    <div class="container">
      <!--[if lte IE 9]>
<div class="alert alert-warning">
  <p>Your Internet Explorer is not supported. Please upgrade your Internet Explorer to version 9+, or use latest <a href="http://www.google.com/chrome/" target="_blank" class="alert-link">Google chrome</a>、<a href="http://www.mozilla.org/firefox/" target="_blank" class="alert-link">Mozilla Firefox</a>.</p>
  <p>If you are using IE 9 or later, make sure you <a href="http://windows.microsoft.com/en-us/internet-explorer/use-compatibility-view#ie=ie-8" target="_blank" class="alert-link">turn off "Compatibility view"</a>.</p>
</div>
<![endif]-->
<header class="header">
  <div class="title"><a title="夏天松的个人博客" href="/">夏天松的个人博客</a></div>
  <ul class="nav navbar-nav navbar-right visible-lg visible-md">
    <li>
    <form id="search-form" class="form-group has-success visible-lg" role="form">
      <input type="text" class="form-control input-sm" placeholder="Search" id="query" style="width: 160px;">
    </form>
    </li>
    <li><a href="/archive.html" title="Archive"><span class='fa fa-archive fa-2x'></span></a></li>
    <li><a href="/categories.html" title="Categories"><span class='fa fa-navicon fa-2x'></span></a></li>
    <li><a href="/tags.html" title="Tags"><span class='fa fa-tags fa-2x'></span></a></li>
    <li><a href="/about.html" title="About"><span class='fa fa-user fa-2x'></span></a></li>
    
    <li><a href="https://github.com/xiatiansong" target="_blank" title="Github"><span class='fa fa-github fa-2x'></span></a></li>
    
    
    
    
    
    <li><a href="http://weibo.com/xiaotian120" target="_blank" title="Weibo"><span class="fa fa-weibo fa-2x"></span></a></li>
    

    <li><a href="/rss.xml" target="_blank" title="RSS"><span class='fa fa-rss fa-2x'></span></a></li>
  </ul>
</header>


      <div class="wrapper">
        <div class="row">
          <div id="search-loader" style="display:none;text-align:center">
            <img src="http://javachen-rs.qiniudn.com/images/loading.gif">
          </div>
          <div class="col-md-12">
  <article class="news-item">

      <h2  class="news-item"> 使用yum安装CDH Hadoop集群  
        <time class="small">2013.04.06</time>
      </h2>

      <div><p>Update:</p>

<ul>
<li><code class="prettyprint">2014.07.21</code> 添加 lzo 的安装</li>
<li><code class="prettyprint">2014.05.20</code> 修改cdh4为cdh5进行安装。</li>
<li><code class="prettyprint">2014.10.22</code> 添加安装 cdh5.2 注意事项。

<ul>
<li>1、<a href="http://blog.javachen.com/2014/10/20/cdh5.2-release/">cdh5.2</a> 发布了，其中 YARN 的一些配置参数做了修改，需要特别注意。</li>
<li>2、Hive 的元数据如果使用 PostgreSql9.X，需要设置 <code class="prettyprint">standard_conforming_strings</code> 为 off</li>
</ul></li>
</ul>

<h1 id="环境">环境</h1>

<ul>
<li>CentOS 6.4 x86_64</li>
<li>CDH 5.2.0</li>
<li>jdk1.6.0_31</li>
</ul>

<p>集群规划为3个节点，每个节点的ip、主机名和部署的组件分配如下：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">    192.168.56.121        cdh1     NameNode、Hive、ResourceManager、HBase
    192.168.56.122        cdh2     DataNode、SSNameNode、NodeManager、HBase
    192.168.56.123        cdh3     DataNode、HBase、NodeManager
</code></pre></div>
<h1 id="1.-准备工作">1. 准备工作</h1>

<p>安装 Hadoop 集群前先做好下面的准备工作，在修改配置文件的时候，建议在一个节点上修改，然后同步到其他节点，例如：对于 hdfs 和 yarn ，在 NameNode 节点上修改然后再同步，对于 HBase，选择一个节点再同步。因为要同步配置文件和在多个节点启动服务，建议配置 ssh 无密码登陆。</p>

<h2 id="1.1-配置hosts">1.1 配置hosts</h2>

<blockquote>
<p>CDH 要求使用 IPv4，IPv6 不支持。</p>
</blockquote>

<p><strong>禁用IPv6方法：</strong></p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>vim /etc/sysctl.conf
<span class="c">#disable ipv6</span>
net.ipv6.conf.all.disable_ipv6<span class="o">=</span>1
net.ipv6.conf.default.disable_ipv6<span class="o">=</span>1
net.ipv6.conf.lo.disable_ipv6<span class="o">=</span>1
</code></pre></div>
<p>使其生效：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sysctl -p
</code></pre></div>
<p>最后确认是否已禁用：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat /proc/sys/net/ipv6/conf/all/disable_ipv6
1
</code></pre></div>
<p>1、设置hostname，以cdh1为例</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hostname cdh1
</code></pre></div>
<p>2、确保<code class="prettyprint">/etc/hosts</code>中包含ip和FQDN，如果你在使用DNS，保存这些信息到<code class="prettyprint">/etc/hosts</code>不是必要的，却是最佳实践。</p>

<p>3、确保<code class="prettyprint">/etc/sysconfig/network</code>中包含<code class="prettyprint">hostname=cdh1</code></p>

<p>4、检查网络，运行下面命令检查是否配置了hostname以及其对应的ip是否正确。</p>

<p>运行<code class="prettyprint">uname -a</code>查看hostname是否匹配<code class="prettyprint">hostname</code>命令运行的结果：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>uname -a
Linux cdh1 2.6.32-358.23.2.el6.x86_64 <span class="c">#1 SMP Wed Oct 16 18:37:12 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux</span>
<span class="nv">$ </span>hostname
cdh1
</code></pre></div>
<p>运行<code class="prettyprint">/sbin/ifconfig</code>查看ip:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>ifconfig
eth1      Link encap:Ethernet  HWaddr 08:00:27:75:E0:95  
          inet addr:192.168.56.121  Bcast:192.168.56.255  Mask:255.255.255.0
......
</code></pre></div>
<p>先安装bind-utils，才能运行host命令：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install <span class="nb">bind</span>-utils -y
</code></pre></div>
<p>运行下面命令查看hostname和ip是否匹配:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>host -v -t A <span class="sb">`</span>hostname<span class="sb">`</span>
Trying <span class="s2">&quot;cdh1&quot;</span>
...
<span class="p">;;</span> ANSWER SECTION:
cdh1. 60 IN A   192.168.56.121
</code></pre></div>
<p>5、hadoop的所有配置文件中配置节点名称时，请使用hostname和不是ip</p>

<h2 id="1.2-关闭防火墙">1.2 关闭防火墙</h2>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>setenforce 0
<span class="nv">$ </span>vim /etc/sysconfig/selinux <span class="c">#修改SELINUX=disabled</span>
</code></pre></div>
<p>清空iptables</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>iptables -F
</code></pre></div>
<h2 id="1.3-时钟同步">1.3 时钟同步</h2>

<h2 id="搭建时钟同步服务器">搭建时钟同步服务器</h2>

<p>这里选择 cdh1 节点为时钟同步服务器，其他节点为客户端同步时间到该节点。、</p>

<p>安装ntp:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install ntp
</code></pre></div>
<p>修改 cdh1 上的配置文件 <code class="prettyprint">/etc/ntp.conf</code> :</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">restrict default ignore   //默认不允许修改或者查询ntp,并且不接收特殊封包
restrict 127.0.0.1        //给于本机所有权限
restrict 192.168.56.0 mask 255.255.255.0 notrap nomodify  //给于局域网机的机器有同步时间的权限
server  192.168.56.121     # local clock
driftfile /var/lib/ntp/drift
fudge   127.127.1.0 stratum 10
</code></pre></div>
<p>启动 ntp：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>service ntpd start
</code></pre></div>
<p>设置开机启动:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>chkconfig ntpd on
</code></pre></div>
<p>ntpq用来监视ntpd操作，使用标准的NTP模式6控制消息模式，并与NTP服务器通信。</p>

<p><code class="prettyprint">ntpq -p</code> 查询网络中的NTP服务器，同时显示客户端和每个服务器的关系。</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$ ntpq -p
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
*LOCAL(1)        .LOCL.           5 l    6   64    1    0.000    0.000   0.000
</code></pre></div>
<ul>
<li>&quot;* &quot;：响应的NTP服务器和最精确的服务器。</li>
<li>&quot;+&quot;：响应这个查询请求的NTP服务器。</li>
<li>&quot;blank（空格）&quot;：没有响应的NTP服务器。</li>
<li>&quot;remote&quot; ：响应这个请求的NTP服务器的名称。</li>
<li>&quot;refid &quot;：NTP服务器使用的更高一级服务器的名称。</li>
<li>&quot;st&quot;：正在响应请求的NTP服务器的级别。</li>
<li>&quot;when&quot;：上一次成功请求之后到现在的秒数。</li>
<li>&quot;poll&quot;：当前的请求的时钟间隔的秒数。</li>
<li>&quot;offset&quot;：主机通过NTP时钟同步与所同步时间源的时间偏移量，单位为毫秒（ms）。</li>
</ul>

<h2 id="客户端的配置">客户端的配置</h2>

<p>在cdh2和cdh3节点上执行下面操作：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>ntpdate cdh1
</code></pre></div>
<p>Ntpd启动的时候通常需要一段时间大概5分钟进行时间同步，所以在ntpd刚刚启动的时候还不能正常提供时钟服务，报错&quot;no server suitable for synchronization found&quot;。启动时候需要等待5分钟。</p>

<p>如果想定时进行时间校准，可以使用crond服务来定时执行。</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">00 1 * * * root /usr/sbin/ntpdate 192.168.56.121 &gt;&gt; /root/ntpdate.log 2&gt;&amp;1
</code></pre></div>
<p>这样，每天 1:00 Linux 系统就会自动的进行网络时间校准。</p>

<h2 id="1.4-安装jdk">1.4 安装jdk</h2>

<p>以下是手动安装jdk，你也可以通过yum方式安装，见下文。</p>

<p>检查jdk版本</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>java -version
</code></pre></div>
<p>如果其版本低于v1.6 update 31，则将其卸载</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>rpm -qa <span class="p">|</span> grep java
<span class="nv">$ </span>yum remove <span class="o">{</span>java-1.*<span class="o">}</span>
</code></pre></div>
<p>验证默认的jdk是否被卸载</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>which java
</code></pre></div>
<p>安装jdk，使用yum安装或者手动下载安装jdk-6u31-linux-x64.bin，下载地址：<a href="http://www.oracle.com/technetwork/java/javasebusiness/downloads/java-archive-downloads-javase6-419409.html#jdk-6u31-oth-JPR">这里</a></p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install jdk -y
</code></pre></div>
<p>创建符号连接</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>ln -s XXXXX/jdk1.6.0_31 /usr/java/latest
<span class="nv">$ </span>ln -s /usr/java/latest/bin/java /usr/bin/java
</code></pre></div>
<p>设置环境变量:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">echo</span> <span class="s2">&quot;export JAVA_HOME=/usr/java/latest&quot;</span> &gt;&gt;/root/.bashrc
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">&quot;export PATH=\$JAVA_HOME/bin:\$PATH&quot;</span> &gt;&gt; /root/.bashrc
<span class="nv">$ </span><span class="nb">source</span> /root/.bashrc
</code></pre></div>
<p>验证版本</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>java -version
    java version <span class="s2">&quot;1.6.0_31&quot;</span>
    Java<span class="o">(</span>TM<span class="o">)</span> SE Runtime Environment <span class="o">(</span>build 1.6.0_31-b04<span class="o">)</span>
    Java HotSpot<span class="o">(</span>TM<span class="o">)</span> 64-Bit Server VM <span class="o">(</span>build 20.6-b01, mixed mode<span class="o">)</span>
</code></pre></div>
<p>检查环境变量中是否有设置<code class="prettyprint">JAVA_HOME</code></p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>env <span class="p">|</span> grep JAVA_HOME
</code></pre></div>
<p>如果env中没有<code class="prettyprint">JAVA_HOM</code>E变量，则修改<code class="prettyprint">/etc/sudoers</code>文件</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>vi /etc/sudoers
    Defaults env_keep+<span class="o">=</span>JAVA_HOME
</code></pre></div>
<h2 id="1.5-设置本地yum源">1.5 设置本地yum源</h2>

<p>你可以从<a href="http://archive.cloudera.com/cdh4/repo-as-tarball/">这里</a>下载 cdh4 的仓库压缩包，或者从<a href="http://archive.cloudera.com/cdh5/repo-as-tarball/">这里</a> 下载 cdh5 的仓库压缩包。</p>

<p>因为我是使用的centos操作系统，故这里使用的cdh5的centos6仓库，将其下载之后解压配置cdh的yum源：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">[hadoop]
name=hadoop
baseurl=ftp://cdh1/cdh/5/
enabled=1
gpgcheck=0
</code></pre></div>
<p>这里使用的是 ftp 搭建 yum 源，需要安装 ftp 服务，并将解压后的目录拷贝到 ftp 存放文件的目录下。</p>

<p>操作系统的yum源，建议你通过下载 centos 的 dvd 然后配置一个本地的 yum 源。</p>

<p>其实，在配置了CDH 的 yum 源之后，可以通过 yum 来安装 jdk，然后，设置 JAVA HOME：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install jdk -y
</code></pre></div>
<h1 id="2.-安装和配置hdfs">2. 安装和配置HDFS</h1>

<p><strong>说明：</strong></p>

<ul>
<li>根据文章开头的节点规划，cdh1 为NameNode节点和SecondaryNameNode</li>
<li>根据文章开头的节点规划，cdh2 和 cdh3 为DataNode节点</li>
</ul>

<p>在 NameNode 节点安装 hadoop-hdfs-namenode</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop hadoop-hdfs hadoop-client hadoop-doc hadoop-debuginfo hadoop-hdfs-namenode
</code></pre></div>
<p>在 NameNode 节点中选择一个节点作为 secondarynamenode ，并安装 hadoop-hdfs-secondarynamenode</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop-hdfs-secondarynamenode -y
</code></pre></div>
<p>在DataNode节点安装 hadoop-hdfs-datanode</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop hadoop-hdfs hadoop-client hadoop-doc hadoop-debuginfo hadoop-hdfs-datanode -y
</code></pre></div>
<blockquote>
<p>配置 NameNode HA 请参考<a href="https://ccp.cloudera.com/display/CDH4DOC/Introduction+to+HDFS+High+Availability">Introduction to HDFS High Availability</a></p>
</blockquote>

<h2 id="2.1-修改hadoop配置文件">2.1 修改hadoop配置文件</h2>

<blockquote>
<p>更多的配置信息说明，请参考 <a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html">Apache Cluster Setup</a></p>
</blockquote>

<ol>
<li>在<code class="prettyprint">/etc/hadoop/conf/core-site.xml</code>中设置<code class="prettyprint">fs.defaultFS</code>属性值，该属性指定NameNode是哪一个节点以及使用的文件系统是file还是hdfs，格式：<code class="prettyprint">hdfs://&lt;namenode host&gt;:&lt;namenode port&gt;/</code>，默认的文件系统是<code class="prettyprint">file:///</code></li>
<li>在<code class="prettyprint">/etc/hadoop/conf/hdfs-site.xml</code>中设置<code class="prettyprint">dfs.permissions.superusergroup</code>属性，该属性指定hdfs的超级用户，默认为hdfs，你可以修改为hadoop</li>
</ol>

<p>配置如下：</p>

<p>/etc/hadoop/conf/core-site.xml:</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>hdfs://cdh1:8020<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>/etc/hadoop/conf/hdfs-site.xml:</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>dfs.permissions.superusergroup<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>hadoop<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<h2 id="2.2-指定本地文件目录">2.2 指定本地文件目录</h2>

<p>在hadoop中默认的文件路径以及权限要求如下：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">目录                                  所有者       权限      默认路径
hadoop.tmp.dir                      hdfs:hdfs   drwx------  /var/hadoop
dfs.namenode.name.dir               hdfs:hdfs   drwx------  file://${hadoop.tmp.dir}/dfs/name
dfs.datanode.data.dir               hdfs:hdfs   drwx------  file://${hadoop.tmp.dir}/dfs/data
dfs.namenode.checkpoint.dir         hdfs:hdfs   drwx------  file://${hadoop.tmp.dir}/dfs/namesecondary
</code></pre></div>
<p>说明你可以在hdfs-site.xml中只配置　｀hadoop.tmp.dir`,也可以分别配置上面的路径。</p>

<p>这里使用分别配置的方式，hdfs-site.xml中配置如下：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>file:///data/dfs/nn<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>file:///data/dfs/dn<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>在<strong>NameNode</strong>上手动创建 <code class="prettyprint">dfs.name.dir</code> 或 <code class="prettyprint">dfs.namenode.name.dir</code> 的本地目录：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mkdir -p /data/dfs/nn
</code></pre></div>
<p>在<strong>DataNode</strong>上手动创建 <code class="prettyprint">dfs.data.dir</code> 或 <code class="prettyprint">dfs.datanode.data.dir</code> 的本地目录：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mkdir -p /data/dfs/dn
</code></pre></div>
<p>修改上面目录所有者：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$ chown -R hdfs:hdfs /data/dfs/nn /data/dfs/dn
</code></pre></div>
<blockquote>
<p>hadoop的进程会自动设置 <code class="prettyprint">dfs.data.dir</code> 或 <code class="prettyprint">dfs.datanode.data.dir</code>，但是 <code class="prettyprint">dfs.name.dir</code> 或 <code class="prettyprint">dfs.namenode.name.dir</code> 的权限默认为755，需要手动设置为700。</p>
</blockquote>

<p>故，修改上面目录权限：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>chmod 700 /data/dfs/nn
</code></pre></div>
<p>或者：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>chmod go-rx /data/dfs/nn
</code></pre></div>
<p><strong>说明：</strong></p>

<p>DataNode的本地目录可以设置多个，你可以设置 <code class="prettyprint">dfs.datanode.failed.volumes.tolerated</code> 参数的值，表示能够容忍不超过该个数的目录失败。</p>

<h2 id="2.3-配置-secondarynamenode">2.3 配置 SecondaryNameNode</h2>

<p>在 <code class="prettyprint">/etc/hadoop/conf/hdfs-site.xml</code> 中可以配置以下参数：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">dfs.namenode.checkpoint.check.period
dfs.namenode.checkpoint.txns
dfs.namenode.checkpoint.dir
dfs.namenode.checkpoint.edits.dir
dfs.namenode.num.checkpoints.retained
</code></pre></div>
<p>如果想配置SecondaryNameNode节点，请从NameNode中单独选择一台机器，然后做以下设置：</p>

<ul>
<li>将运行SecondaryNameNode的机器名称加入到masters</li>
<li>在 <code class="prettyprint">/etc/hadoop/conf/hdfs-site.xml</code> 中加入如下配置：</li>
</ul>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.secondary.http.address<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>cdh1:50090<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>设置多个secondarynamenode，请参考<a href="http://blog.cloudera.com/blog/2009/02/multi-host-secondarynamenode-configuration/">multi-host-secondarynamenode-configuration</a>.</p>

<h2 id="2.4-开启回收站功能">2.4 开启回收站功能</h2>

<blockquote>
<p>回收站功能默认是关闭的，建议打开。</p>
</blockquote>

<p>在 <code class="prettyprint">/etc/hadoop/conf/core-site.xml</code> 中添加如下两个参数：</p>

<ul>
<li><code class="prettyprint">fs.trash.interval</code>,该参数值为时间间隔，单位为分钟，默认为0，表示回收站功能关闭。该值表示回收站中文件保存多长时间，如果服务端配置了该参数，则忽略客户端的配置；如果服务端关闭了该参数，则检查客户端是否有配置该参数；</li>
<li><code class="prettyprint">fs.trash.checkpoint.interval</code>，该参数值为时间间隔，单位为分钟，默认为0。该值表示检查回收站时间间隔，该值要小于<code class="prettyprint">fs.trash.interval</code>，该值在服务端配置。如果该值设置为0，则使用 <code class="prettyprint">fs.trash.interval</code> 的值。</li>
</ul>

<h2 id="2.5-(可选)配置datanode存储的负载均衡">2.5 (可选)配置DataNode存储的负载均衡</h2>

<p>在 <code class="prettyprint">/etc/hadoop/conf/hdfs-site.xml</code> 中配置以下三个参数（详细说明，请参考 <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_hdfs_cluster_deploy.html#concept_ncq_nnk_ck_unique_1">Optionally configure DataNode storage balancing</a>）：</p>

<ul>
<li>dfs.datanode.fsdataset. volume.choosing.policy</li>
<li>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</li>
<li>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</li>
</ul>

<h2 id="2.6-开启webhdfs">2.6 开启WebHDFS</h2>

<p>这里只在一个NameNode节点（ CDH1 ）上安装：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop-httpfs -y
</code></pre></div>
<p>然后配置代理用户，修改 /etc/hadoop/conf/core-site.xml，添加如下代码：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>  
<span class="nt">&lt;name&gt;</span>hadoop.proxyuser.httpfs.hosts<span class="nt">&lt;/name&gt;</span>  
<span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>  
<span class="nt">&lt;/property&gt;</span>  
<span class="nt">&lt;property&gt;</span>  
<span class="nt">&lt;name&gt;</span>hadoop.proxyuser.httpfs.groups<span class="nt">&lt;/name&gt;</span>  
<span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>  
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>然后重启 Hadoop 使配置生效。</p>

<p>接下来，启动 HttpFS 服务：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>service hadoop-httpfs start
</code></pre></div>
<blockquote>
<p>By default, HttpFS server runs on port 14000 and its URL is http://<HTTPFS_HOSTNAME>:14000/webhdfs/v1.</p>
</blockquote>

<p>简单测试，使用 curl 运行下面命令，并查看执行结果：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$ curl &quot;http://localhost:14000/webhdfs/v1?op=gethomedirectory&amp;user.name=hdfs&quot;
{&quot;Path&quot;:&quot;\/user\/hdfs&quot;}
</code></pre></div>
<p>更多的 API，请参考 <a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS REST API</a></p>

<h2 id="2.7-配置lzo">2.7 配置LZO</h2>

<p>下载repo文件到 <code class="prettyprint">/etc/yum.repos.d/</code>:</p>

<ul>
<li>如果你安装的是 CDH4，请下载<a href="http://archive.cloudera.com/gplextras/redhat/6/x86_64/gplextras/cloudera-gplextras4.repo">Red Hat/CentOS 6</a></li>
<li>如果你安装的是 CDH5，请下载<a href="http://archive.cloudera.com/gplextras5/redhat/6/x86_64/gplextras/cloudera-gplextras5.repo">Red Hat/CentOS 6</a></li>
</ul>

<p>然后，安装lzo:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop-lzo* impala-lzo  -y
</code></pre></div>
<p>最后，在 <code class="prettyprint">/etc/hadoop/conf/core-site.xml</code> 中添加如下配置：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>io.compression.codecs<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>io.compression.codec.lzo.class<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>更多关于LZO信息，请参考：<a href="http://wiki.apache.org/hadoop/UsingLzoCompression">Using LZO Compression</a></p>

<h2 id="2.8-(可选)配置snappy">2.8 (可选)配置Snappy</h2>

<p>cdh 的 rpm 源中默认已经包含了 snappy ，直接安装即可。</p>

<p>在每个节点安装Snappy：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install snappy snappy-devel  -y
</code></pre></div>
<p>然后，在 <code class="prettyprint">core-site.xml</code> 中修改<code class="prettyprint">io.compression.codecs</code>的值，添加 <code class="prettyprint">org.apache.hadoop.io.compress.SnappyCodec</code> ：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
<span class="nt">&lt;name&gt;</span>io.compression.codecs<span class="nt">&lt;/name&gt;</span>
<span class="nt">&lt;value&gt;</span>org.apache.hadoop.io.compress.DefaultCodec,
org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.BZip2Codec,
com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec,
org.apache.hadoop.io.compress.SnappyCodec<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>使 snappy 对 hadoop 可用：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>ln -sf /usr/lib64/libsnappy.so /usr/lib/hadoop/lib/native/
</code></pre></div>
<h2 id="2.9-启动hdfs">2.9 启动HDFS</h2>

<p>将配置文件同步到每一个节点：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>scp -r /etc/hadoop/conf root@cdh2:/etc/hadoop/
<span class="nv">$ </span>scp -r /etc/hadoop/conf root@cdh3:/etc/hadoop/
</code></pre></div>
<p>格式化NameNode：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop namenode -format
</code></pre></div>
<p>在每个节点运行下面命令启动hdfs：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">for </span>x in <span class="sb">`</span>ls /etc/init.d/<span class="p">|</span>grep  hadoop-hdfs<span class="sb">`</span> <span class="p">;</span> <span class="k">do </span>service <span class="nv">$x</span> start <span class="p">;</span> <span class="k">done</span>
</code></pre></div>
<p>在 hdfs 运行之后，创建 <code class="prettyprint">/tmp</code> 临时目录，并设置权限为 <code class="prettyprint">1777</code>：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir /tmp
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chmod -R 1777 /tmp
</code></pre></div>
<h2 id="2.10-访问web">2.10 访问web</h2>

<p>通过 <a href="http://cdh1:50070/">http://cdh1:50070/</a> 可以访问 NameNode 页面。</p>

<h1 id="3.-安装和配置yarn">3. 安装和配置YARN</h1>

<h2 id="节点规划">节点规划</h2>

<ul>
<li>根据文章开头的节点规划，cdh1 为resourcemanager节点</li>
<li>根据文章开头的节点规划，cdh2 和 cdh3 为nodemanager节点</li>
<li>为了简单，historyserver也装在 cdh1 节点上</li>
</ul>

<h2 id="安装服务">安装服务</h2>

<p>在 resourcemanager 节点安装:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop-yarn hadoop-yarn-resourcemanager -y
</code></pre></div>
<p>在 nodemanager 节点安装:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop-yarn hadoop-yarn-nodemanager hadoop-mapreduce -y
</code></pre></div>
<p>安装 historyserver：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hadoop-mapreduce-historyserver hadoop-yarn-proxyserver -y
</code></pre></div>
<h2 id="修改配置参数">修改配置参数</h2>

<p><strong>要想使用YARN</strong>，需要在 <code class="prettyprint">/etc/hadoop/conf/mapred-site.xml</code> 中做如下配置:</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p><strong>配置resourcemanager的节点名称以及一些服务的端口号</strong>，修改/etc/hadoop/conf/yarn-site.xml：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>cdh1:8031<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>cdh1:8032<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.scheduler.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>cdh1:8030<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.admin.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>cdh1:8033<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.webapp.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>cdh1:8088<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p><strong>配置YARN进程：</strong></p>

<ul>
<li><code class="prettyprint">yarn.nodemanager.aux-services</code>，在CDH4中该值设为 <code class="prettyprint">mapreduce.shuffle</code>，在CDH5中该值设为 <code class="prettyprint">mapreduce_shuffle</code></li>
<li><code class="prettyprint">yarn.nodemanager.aux-services.mapreduce.shuffle.class</code>，该值设为 <code class="prettyprint">org.apache.hadoop.mapred.ShuffleHandler</code></li>
<li><code class="prettyprint">yarn.resourcemanager.hostname</code>，该值设为 cdh1</li>
<li><code class="prettyprint">yarn.log.aggregation.enable</code>，该值设为 true</li>
<li><code class="prettyprint">yarn.application.classpath</code>，该值设为:</li>
</ul>
<div class="highlight"><pre><code class="language-text" data-lang="text">$HADOOP_CONF_DIR, $HADOOP_COMMON_HOME/*, $HADOOP_COMMON_HOME/lib/*, $HADOOP_HDFS_HOME/*, $HADOOP_HDFS_HOME/lib/*, $HADOOP_MAPRED_HOME/*, $HADOOP_MAPRED_HOME/lib/*, $HADOOP_YARN_HOME/*, $HADOOP_YARN_HOME/lib/*
</code></pre></div>
<p>即，在 <code class="prettyprint">/etc/hadoop/conf/yarn-site.xml</code> 中添加如下配置：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.log-aggregation-enable<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.application.classpath<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>
    $HADOOP_CONF_DIR,
    $HADOOP_COMMON_HOME/*,
    $HADOOP_COMMON_HOME/lib/*,
    $HADOOP_HDFS_HOME/*,
    $HADOOP_HDFS_HOME/lib/*,
    $HADOOP_MAPRED_HOME/*,
    $HADOOP_MAPRED_HOME/lib/*,
    $HADOOP_YARN_HOME/*,
    $HADOOP_YARN_HOME/lib/*
    <span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.log.aggregation.enable<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p><strong>注意：</strong></p>

<p>a. <code class="prettyprint">yarn.nodemanager.aux-services</code> 的值在 cdh4 中应该为 <code class="prettyprint">mapreduce.shuffle</code>，并配置参数<code class="prettyprint">yarn.nodemanager.aux-services.mapreduce.shuffle.class</code>值为 org.apache.hadoop.mapred.ShuffleHandler ，在cdh5中为<code class="prettyprint">mapreduce_shuffle</code>，这时候请配置<code class="prettyprint">yarn.nodemanager.aux-services.mapreduce_shuffle.class</code>参数</p>

<p>b. 这里配置了 <code class="prettyprint">yarn.application.classpath</code> ，需要设置一些喜欢环境变量：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/usr/lib/hadoop
<span class="nb">export </span><span class="nv">HIVE_HOME</span><span class="o">=</span>/usr/lib/hive
<span class="nb">export </span><span class="nv">HBASE_HOME</span><span class="o">=</span>/usr/lib/hbase
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span>/usr/lib/hadoop-hdfs
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span>/usr/lib/hadoop-mapreduce
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span>/usr/lib/hadoop-hdfs
<span class="nb">export </span><span class="nv">HADOOP_LIBEXEC_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/libexec
<span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/etc/hadoop
<span class="nb">export </span><span class="nv">HDFS_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/etc/hadoop
<span class="nb">export </span><span class="nv">HADOOP_YARN_HOME</span><span class="o">=</span>/usr/lib/hadoop-yarn
<span class="nb">export </span><span class="nv">YARN_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/etc/hadoop
</code></pre></div>
<p><strong>配置文件路径</strong></p>

<p>在hadoop中默认的文件路径以及权限要求如下：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">目录                                                     所有者         权限             默认路径
yarn.nodemanager.local-dirs               yarn:yarn   drwxr-xr-x    ${hadoop.tmp.dir}/nm-local-dir
yarn.nodemanager.log-dirs                   yarn:yarn     drwxr-xr-x      ${yarn.log.dir}/userlogs
yarn.nodemanager.remote-app-log-dir                                         hdfs://cdh1:8020/var/log/hadoop-yarn/apps
</code></pre></div>
<p>在 <code class="prettyprint">/etc/hadoop/conf/yarn-site.xml</code>文件中添加如下配置:</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.local-dirs<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>file:///data/yarn/local<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.log-dirs<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>file:///data/yarn/logs<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>/yarn/apps<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p><strong>创建本地目录</strong></p>

<p>创建 <code class="prettyprint">yarn.nodemanager.local-dirs</code> 和 <code class="prettyprint">yarn.nodemanager.log-dirs</code> 参数对应的目录：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mkdir -p /data/yarn/<span class="o">{</span><span class="nb">local</span>,logs<span class="o">}</span>
<span class="nv">$ </span>chown -R yarn:yarn /data/yarn
</code></pre></div>
<p><strong>创建Log目录</strong></p>

<p>在 hdfs 上创建 <code class="prettyprint">yarn.nodemanager.remote-app-log-dir</code> 对应的目录：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$ sudo -u hdfs hadoop fs -mkdir -p /yarn/apps
$ sudo -u hdfs hadoop fs -chown yarn:mapred /yarn/apps
$ sudo -u hdfs hadoop fs -chmod 1777 /yarn/apps
</code></pre></div>
<p><strong>配置History Server：</strong></p>

<p>在 <code class="prettyprint">/etc/hadoop/conf/mapred-site.xml</code> 中添加如下：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>mapreduce.jobhistory.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>cdh1:10020<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>mapreduce.jobhistory.webapp.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>cdh1:19888<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>此外，确保 mapred 用户能够使用代理，在 <code class="prettyprint">/etc/hadoop/conf/core-site.xml</code> 中添加如下参数：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.mapred.groups<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.mapred.hosts<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p><strong>配置 Staging 目录：</strong></p>

<p>在 <code class="prettyprint">/etc/hadoop/conf/mapred-site.xml</code> 中配置参数 <code class="prettyprint">yarn.app.mapreduce.am.staging-dir</code>（该值默认为：<code class="prettyprint">/tmp/hadoop-yarn/staging</code>，请参见 <a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>）：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>yarn.app.mapreduce.am.staging-dir<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>/user<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>并在 hdfs 上创建相应的目录：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir -p /user
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chmod 777 /user
</code></pre></div>
<p><strong>创建 history 子目录</strong></p>

<p>可选的，你可以在 <code class="prettyprint">/etc/hadoop/conf/mapred-site.xml</code> 设置以下两个参数：</p>

<ul>
<li><code class="prettyprint">mapreduce.jobhistory.intermediate-done-dir</code>，该目录权限应该为1777，默认值为 <code class="prettyprint">${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate</code></li>
<li><code class="prettyprint">mapreduce.jobhistory.done-dir</code>，该目录权限应该为750，默认值为 <code class="prettyprint">${yarn.app.mapreduce.am.staging-dir}/history/done</code></li>
</ul>

<p>在 hdfs 上创建目录并设置权限：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir -p /user/history
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chmod -R 1777 /user/history
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chown mapred:hadoop /user/history
</code></pre></div>
<h2 id="验证-hdfs-结构：">验证 HDFS 结构：</h2>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -ls -R /
</code></pre></div>
<p>你应该看到如下结构：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">drwxrwxrwt   - hdfs hadoop          0 2014-04-19 14:21 /tmp
drwxrwxrwx   - hdfs hadoop          0 2014-04-19 14:26 /user
drwxrwxrwt   - mapred hadoop        0 2014-04-19 14:31 /user/history
drwxr-x---   - mapred hadoop        0 2014-04-19 14:38 /user/history/done
drwxrwxrwt   - mapred hadoop        0 2014-04-19 14:48 /user/history/done_intermediate
drwxr-xr-x   - hdfs   hadoop        0 2014-04-19 15:31 /yarn
drwxrwxrwt   - yarn   mapred        0 2014-04-19 15:31 /yarn/apps
</code></pre></div>
<p>看到上面的目录结构，你就将NameNode上的配置文件同步到其他节点了，并且启动 yarn 的服务。</p>

<h2 id="同步配置文件">同步配置文件</h2>

<p>同步配置文件到整个集群:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>scp -r /etc/hadoop/conf root@cdh2:/etc/hadoop/
<span class="nv">$ </span>scp -r /etc/hadoop/conf root@cdh3:/etc/hadoop/
</code></pre></div>
<h2 id="启动服务">启动服务</h2>

<p>在 cdh1 节点启动 mapred-historyserver :</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>/etc/init.d/hadoop-mapreduce-historyserver start
</code></pre></div>
<p>在每个节点启动 YARN :</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">for </span>x in <span class="sb">`</span>ls /etc/init.d/<span class="p">|</span>grep hadoop-yarn<span class="sb">`</span> <span class="p">;</span> <span class="k">do </span>service <span class="nv">$x</span> start <span class="p">;</span> <span class="k">done</span>
</code></pre></div>
<p>为每个 MapReduce 用户创建主目录，比如说 hive 用户或者当前用户：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir /user/<span class="nv">$USER</span>
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chown <span class="nv">$USER</span> /user/<span class="nv">$USER</span>
</code></pre></div>
<p>设置 <code class="prettyprint">HADOOP_MAPRED_HOME</code> ,或者把其加入到 hadoop 的配置文件中</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span>/usr/lib/hadoop-mapreduce
</code></pre></div>
<h2 id="访问-web">访问 web</h2>

<p>通过 <a href="http://cdh1:8088/">http://cdh1:8088/</a> 可以访问 Yarn 的管理页面。</p>

<p>通过 <a href="http://cdh1:19888/">http://cdh1:19888/</a> 可以访问 JobHistory 的管理页面。</p>

<p>查看在线的节点：<a href="http://cdh1:8088/cluster/nodes">http://cdh1:8088/cluster/nodes</a>。</p>

<p>运行下面的测试程序，看是否报错：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Find how many jars name ending with examples you have inside location /usr/lib/</span>
<span class="nv">$ </span>find /usr/lib/ -name <span class="s2">&quot;*hadoop*examples*.jar&quot;</span>

<span class="c"># To list all the class name inside jar</span>
<span class="nv">$ </span>find /usr/lib/ -name <span class="s2">&quot;hadoop-examples.jar&quot;</span> <span class="p">|</span> xargs -0 -I <span class="s1">&#39;{}&#39;</span> sh -c <span class="s1">&#39;jar tf {}&#39;</span>

<span class="c"># To search for specific class name inside jar</span>
<span class="nv">$ </span>find /usr/lib/ -name <span class="s2">&quot;hadoop-examples.jar&quot;</span> <span class="p">|</span> xargs -0 -I <span class="s1">&#39;{}&#39;</span> sh -c <span class="s1">&#39;jar tf {}&#39;</span> <span class="p">|</span> grep -i wordcount.class

<span class="c"># 运行 randomwriter 例子</span>
<span class="nv">$ </span>sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomwriter out
</code></pre></div>
<h1 id="4.-安装-zookeeper">4. 安装 Zookeeper</h1>

<p>简单说明：</p>

<p>Zookeeper 至少需要3个节点，并且节点数要求是基数，这里在所有节点上都安装 Zookeeper。</p>

<h2 id="安装">安装</h2>

<p>在每个节点上安装zookeeper</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install zookeeper* -y
</code></pre></div>
<h2 id="修改配置文件">修改配置文件</h2>

<p>设置 zookeeper 配置 <code class="prettyprint">/etc/zookeeper/conf/zoo.cfg</code></p>
<div class="highlight"><pre><code class="language-text" data-lang="text">maxClientCnxns=50
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/var/lib/zookeeper
clientPort=2181
server.1=cdh1:2888:3888
server.2=cdh3:2888:3888
server.3=cdh3:2888:3888
</code></pre></div>
<h2 id="同步配置文件">同步配置文件</h2>

<p>将配置文件同步到其他节点：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>scp -r /etc/zookeeper/conf root@cdh2:/etc/zookeeper/
<span class="nv">$ </span>scp -r /etc/zookeeper/conf root@cdh3:/etc/zookeeper/
</code></pre></div>
<h2 id="初始化并启动服务">初始化并启动服务</h2>

<p>在每个节点上初始化并启动 zookeeper，注意 n 的值需要和 zoo.cfg 中的编号一致。</p>

<p>在 cdh1 节点运行</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>service zookeeper-server init --myid<span class="o">=</span>1
<span class="nv">$ </span>service zookeeper-server start
</code></pre></div>
<p>在 cdh2 节点运行</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>service zookeeper-server init --myid<span class="o">=</span>2
<span class="nv">$ </span>service zookeeper-server start
</code></pre></div>
<p>在 cdh3 节点运行</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$ service zookeeper-server init --myid=3
$ service zookeeper-server start
</code></pre></div>
<h2 id="测试">测试</h2>

<p>通过下面命令测试是否启动成功：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>zookeeper-client -server cdh1:2181
</code></pre></div>
<h1 id="5.-安装-hbase">5. 安装 HBase</h1>

<p>HBase 依赖 ntp 服务，故需要提前安装好 ntp。</p>

<h2 id="安装前设置">安装前设置</h2>

<p>1）修改系统 ulimit 参数:</p>

<p>在 <code class="prettyprint">/etc/security/limits.conf</code> 中添加下面两行并使其生效：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">hdfs  -       nofile  32768
hbase -       nofile  32768
</code></pre></div>
<p>2）修改 <code class="prettyprint">dfs.datanode.max.xcievers</code></p>

<p>在 <code class="prettyprint">hdfs-site.xml</code> 中修改该参数值，将该值调整到较大的值：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.max.xcievers<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>8192<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<h2 id="安装">安装</h2>

<p>在每个节点上安装 master 和 regionserver</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hbase hbase-master hbase-regionserver -y
</code></pre></div>
<p>如果需要你可以安装 hbase-rest、hbase-solr-indexer、hbase-thrift</p>

<h2 id="修改配置文件">修改配置文件</h2>

<p>修改 <code class="prettyprint">hbase-site.xml</code>文件，关键几个参数及含义如下：</p>

<ul>
<li><code class="prettyprint">hbase.distributed</code>：是否为分布式模式</li>
<li><code class="prettyprint">hbase.rootdir</code>：HBase在hdfs上的目录路径</li>
<li><code class="prettyprint">hbase.tmp.dir</code>：本地临时目录</li>
<li><code class="prettyprint">hbase.zookeeper.quorum</code>：zookeeper集群地址，逗号分隔</li>
<li><code class="prettyprint">hbase.hregion.max.filesize</code>：hregion文件最大大小</li>
<li><code class="prettyprint">hbase.hregion.memstore.flush.size</code>：memstore文件最大大小</li>
</ul>

<p>另外，在CDH5中建议<code class="prettyprint">关掉Checksums</code>（见<a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/cdh5ig_hbase_upgrade.html">Upgrading HBase</a>）以提高性能，修改为如下：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"> <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.regionserver.checksum.verify<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
   <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.hstore.checksum.algorithm<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>NULL<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>最后的配置如下，供参考：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
  <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>hbase.cluster.distributed<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>hbase.rootdir<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>hdfs://cdh1:8020/hbase<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>hbase.tmp.dir<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>/data/hbase<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>hbase.zookeeper.quorum<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>cdh1,cdh2,cdh3<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.hregion.max.filesize<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>536870912<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.hregion.memstore.flush.size<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>67108864<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.regionserver.lease.period<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>600000<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.client.retries.number<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.regionserver.handler.count<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.hstore.compactionThreshold<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.hstore.blockingStoreFiles<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>30<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>

  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.regionserver.checksum.verify<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.hstore.checksum.algorithm<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>NULL<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>
<p>在 hdfs 中创建 <code class="prettyprint">/hbase</code> 目录</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir /hbase
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chown hbase:hbase /hbase
</code></pre></div>
<p>设置 crontab 定时删除日志：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>crontab -e
* 10 * * * <span class="nb">cd</span> /var/log/hbase/<span class="p">;</span> rm -rf <span class="sb">`</span>ls /var/log/hbase/<span class="p">|</span>grep -P <span class="s1">&#39;hbase\-hbase\-.+\.log\.[0-9]&#39;</span><span class="se">\`</span>&gt;&gt; /dev/null <span class="p">&amp;</span>
</code></pre></div>
<h2 id="同步配置文件">同步配置文件</h2>

<p>将配置文件同步到其他节点：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>scp -r /etc/hbase/conf root@cdh2:/etc/hbase/
<span class="nv">$ </span>scp -r /etc/hbase/conf root@cdh3:/etc/hbase/
</code></pre></div>
<h2 id="创建本地目录">创建本地目录</h2>

<p>在 hbase-site.xml 配置文件中配置了 <code class="prettyprint">hbase.tmp.dir</code> 值为 <code class="prettyprint">/data/hbase</code>，现在需要在每个 hbase 节点创建该目录并设置权限：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mkdir /data/hbase
<span class="nv">$ </span>chown -R hbase:hbase /data/hbase/
</code></pre></div>
<h2 id="启动hbase">启动HBase</h2>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">for </span>x in <span class="sb">`</span>ls /etc/init.d/<span class="p">|</span>grep hbase<span class="sb">`</span> <span class="p">;</span> <span class="k">do </span>service <span class="nv">$x</span> start <span class="p">;</span> <span class="k">done</span>
</code></pre></div>
<h2 id="访问web">访问web</h2>

<p>通过 <a href="http://cdh1:60030/">http://cdh1:60030/</a> 可以访问 RegionServer 页面，然后通过该页面可以知道哪个节点为 Master，然后再通过 60010 端口访问 Master 管理界面。</p>

<h1 id="6.-安装hive">6. 安装hive</h1>

<p>在一个 NameNode 节点上安装 hive：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hive hive-metastore hive-server2 hive-jdbc hive-hbase  -y
</code></pre></div>
<p>在其他 DataNode 上安装：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hive hive-server2 hive-jdbc hive-hbase -y
</code></pre></div>
<h2 id="安装postgresql">安装postgresql</h2>

<p>这里使用 postgresq l数据库来存储元数据，如果你想使用 mysql 数据库，请参考下文。</p>

<p>手动安装、配置 postgresql 数据库，请参考 <a href="http://blog.javachen.com/hadoop/2013/03/24/manual-install-Cloudera-hive-CDH/">手动安装Cloudera Hive CDH</a></p>

<p>yum 方式安装：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$ yum install postgresql-server -y
</code></pre></div>
<p>初始化数据库：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>service postgresql initdb
</code></pre></div>
<p>修改配置文件postgresql.conf，修改完后内容如下：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cat /var/lib/pgsql/data/postgresql.conf  <span class="p">|</span> grep -e listen -e standard_conforming_strings
    <span class="nv">listen_addresses</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span>
    <span class="nv">standard_conforming_strings</span> <span class="o">=</span> off
</code></pre></div>
<p>修改 /var/lib/pgsql/data/pg_hba.conf，添加以下一行内容：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">    host    all         all         0.0.0.0/0                     trust
</code></pre></div>
<p>启动数据库</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#配置开启启动</span>
<span class="nv">$ </span>chkconfig postgresql on

<span class="nv">$ </span>service postgresql start
</code></pre></div>
<p>安装jdbc驱动</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install postgresql-jdbc -y
<span class="nv">$ </span>ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jar
</code></pre></div>
<p>创建数据库和用户</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash">    bash# su postgres
    bash<span class="nv">$ </span>psql
    <span class="nv">postgres</span><span class="o">=</span><span class="c"># CREATE USER hiveuser WITH PASSWORD &#39;redhat&#39;;</span>
    <span class="nv">postgres</span><span class="o">=</span><span class="c"># CREATE DATABASE metastore owner=hiveuser;</span>
    <span class="nv">postgres</span><span class="o">=</span><span class="c"># GRANT ALL privileges ON DATABASE metastore TO hiveuser;</span>
    <span class="nv">postgres</span><span class="o">=</span><span class="c"># \q;</span>
    bash<span class="nv">$ </span>psql  -U hiveuser -d metastore
    <span class="nv">postgres</span><span class="o">=</span><span class="c"># \i /usr/lib/hive/scripts/metastore/upgrade/postgres/hive-schema-0.13.0.postgres.sql</span>
    SET
    SET
    ..
</code></pre></div>
<blockquote>
<p>注意：<br>
创建的用户为hiveuser，密码为redhat，你可以按自己需要进行修改。<br>
初始化数据库的 sql 文件请根据 cdh 版本进行修改，这里我的 cdh 版本是5.2.0，对应的文件是 ive-schema-0.13.0.postgres.sql</p>
</blockquote>

<p>这时候的hive-site.xml文件内容如下：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionURL<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>jdbc:postgresql://localhost/metastore<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionDriverName<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>org.postgresql.Driver<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionUserName<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>hiveuser<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionPassword<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>redhat<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>datanucleus.autoCreateSchema<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>

    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>cdh1:8031<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>

    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.files.umask.value<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>0002<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.exec.reducers.max<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>999<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.auto.convert.join<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>

    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.metastore.schema.verification<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.metastore.warehouse.dir<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>/user/hive/warehouse<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.warehouse.subdir.inherit.perms<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.metastore.uris<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>thrift://cdh1:9083<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.metastore.server.min.threads<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>200<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.metastore.server.max.threads<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>100000<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.metastore.client.socket.timeout<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>3600<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>

    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.support.concurrency<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.zookeeper.quorum<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>cdh1,cdh2,cdh3<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.server2.thrift.min.worker.threads<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>5<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
        <span class="nt">&lt;name&gt;</span>hive.server2.thrift.max.worker.threads<span class="nt">&lt;/name&gt;</span>
        <span class="nt">&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>
<p>默认情况下，hive-server和 hive-server2 的 thrift 端口都未10000，如果要修改 hive-server2 thrift 端口，请添加：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hive.server2.thrift.port<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>10001<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>如果要设置运行 hive 的用户为连接的用户而不是启动用户，则添加：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hive.server2.enable.impersonation<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<p>并在 core-site.xml 中添加：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.hive.hosts<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.proxyuser.hive.groups<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>*<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<h2 id="安装mysql">安装mysql</h2>

<p>yum方式安装mysql：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install mysql mysql-devel mysql-server mysql-libs -y
</code></pre></div>
<p>启动数据库：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#配置开启启动</span>
<span class="nv">$ </span>chkconfig mysqld on

<span class="nv">$ </span>service mysqld start
</code></pre></div>
<p>安装jdbc驱动：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install mysql-connector-java
<span class="nv">$ </span>ln -s /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar
</code></pre></div>
<p>我是在 cdh1 节点上创建 mysql 数据库和用户：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mysql -e <span class="s2">&quot;</span>
<span class="s2">    CREATE DATABASE metastore;</span>
<span class="s2">    USE metastore;</span>
<span class="s2">    SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-0.13.0.mysql.sql;</span>
<span class="s2">    CREATE USER &#39;hiveuser&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;redhat&#39;;</span>
<span class="s2">    GRANT ALL PRIVILEGES ON metastore.* TO &#39;hiveuser&#39;@&#39;localhost&#39;;</span>
<span class="s2">    GRANT ALL PRIVILEGES ON metastore.* TO &#39;hiveuser&#39;@&#39;cdh1&#39;;</span>
<span class="s2">    FLUSH PRIVILEGES;</span>
<span class="s2">&quot;</span>
</code></pre></div>
<p>注意：创建的用户为 hiveuser，密码为 redhat ，你可以按自己需要进行修改。</p>

<p>修改 hive-site.xml 文件中以下内容：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml">    <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionURL<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>jdbc:mysql://cdh1:3306/metastore?useUnicode=true<span class="ni">&amp;amp;</span>characterEncoding=UTF-8<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;property&gt;</span>
      <span class="nt">&lt;name&gt;</span>javax.jdo.option.ConnectionDriverName<span class="nt">&lt;/name&gt;</span>
      <span class="nt">&lt;value&gt;</span>com.mysql.jdbc.Driver<span class="nt">&lt;/value&gt;</span>
    <span class="nt">&lt;/property&gt;</span>
</code></pre></div>
<h2 id="配置hive">配置hive</h2>

<p>修改<code class="prettyprint">/etc/hadoop/conf/hadoop-env.sh</code>，添加环境变量 <code class="prettyprint">HADOOP_MAPRED_HOME</code>，如果不添加，则当你使用 yarn 运行 mapreduce 时候会出现 <code class="prettyprint">UNKOWN RPC TYPE</code> 的异常</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span>/usr/lib/hadoop-mapreduce
</code></pre></div>
<p>在 hdfs 中创建 hive 数据仓库目录:</p>

<ul>
<li>hive 的数据仓库在 hdfs 中默认为 <code class="prettyprint">/user/hive/warehouse</code>,建议修改其访问权限为 <code class="prettyprint">1777</code>，以便其他所有用户都可以创建、访问表，但不能删除不属于他的表。</li>
<li>每一个查询 hive 的用户都必须有一个 hdfs 的 home 目录( <code class="prettyprint">/user</code> 目录下，如 root 用户的为 <code class="prettyprint">/user/root</code>)</li>
<li>hive 所在节点的 <code class="prettyprint">/tmp</code> 必须是 world-writable 权限的。</li>
</ul>

<p>创建目录并设置权限：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir /user/hive
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chown hive /user/hive

<span class="nv">$ </span>sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chmod 1777 /user/hive/warehouse
<span class="nv">$ </span>sudo -u hdfs hadoop fs -chown hive /user/hive/warehouse
</code></pre></div>
<p>启动hive-server和metastore:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>service hive-metastore start
<span class="nv">$ </span>service hive-server start
<span class="nv">$ </span>service hive-server2 start
</code></pre></div>
<p>测试：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>hive -e <span class="s1">&#39;create table t(id int);&#39;</span>
<span class="nv">$ </span>hive -e <span class="s1">&#39;select * from t limit 2;&#39;</span>
<span class="nv">$ </span>hive -e <span class="s1">&#39;select id from t;&#39;</span>
</code></pre></div>
<p>访问beeline:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>/usr/lib/hive/bin/beeline
    beeline&gt; !connect jdbc:hive2://localhost:10000 username password org.apache.hive.jdbc.HiveDriver
    0: jdbc:hive2://localhost:10000&gt; SHOW TABLES<span class="p">;</span>
    show tables<span class="p">;</span>
    +-----------+
    <span class="p">|</span> tab_name  <span class="p">|</span>
    +-----------+
    +-----------+
    No rows selected <span class="o">(</span>0.238 seconds<span class="o">)</span>
    0: jdbc:hive2://localhost:10000&gt;
</code></pre></div>
<p>其 sql语法参考<a href="http://sqlline.sourceforge.net/">SQLLine CLI</a>，在这里，你不能使用HiveServer的sql语句</p>

<h2 id="与hbase集成">与hbase集成</h2>

<p>先安装 hive-hbase:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>yum install hive-hbase -y
</code></pre></div>
<p>如果你是使用的 cdh4，则需要在 hive shell 里执行以下命令添加 jar：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>ADD JAR /usr/lib/hive/lib/zookeeper.jar<span class="p">;</span>
<span class="nv">$ </span>ADD JAR /usr/lib/hive/lib/hbase.jar<span class="p">;</span>
<span class="nv">$ </span>ADD JAR /usr/lib/hive/lib/hive-hbase-handler-&lt;hive_version&gt;.jar
<span class="nv">$ </span>ADD JAR /usr/lib/hive/lib/guava-11.0.2.jar<span class="p">;</span>
</code></pre></div>
<p><strong>说明：</strong> guava 包的版本以实际版本为准。</p>

<p>如果你是使用的 cdh5，则需要在 hive shell 里执行以下命令添加 jar：</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">ADD JAR /usr/lib/hive/lib/zookeeper.jar;
ADD JAR /usr/lib/hive/lib/hive-hbase-handler.jar;
ADD JAR /usr/lib/hbase/lib/guava-12.0.1.jar;
ADD JAR /usr/lib/hbase/hbase-client.jar;
ADD JAR /usr/lib/hbase/hbase-common.jar;
ADD JAR /usr/lib/hbase/hbase-hadoop-compat.jar;
ADD JAR /usr/lib/hbase/hbase-hadoop2-compat.jar;
ADD JAR /usr/lib/hbase/hbase-protocol.jar;
ADD JAR /usr/lib/hbase/hbase-server.jar;
</code></pre></div>
<p>以上你也可以在 hive-site.xml 中通过 <code class="prettyprint">hive.aux.jars.path</code> 参数来配置，或者你也可以在 hive-env.sh 中通过 <code class="prettyprint">export HIVE_AUX_JARS_PATH=</code> 来设置。</p>

<h1 id="8.-参考文章">8. 参考文章</h1>

<ul>
<li>[1] <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Installation-Guide/CDH5-Installation-Guide.html">CDH5-Installation-Guide</a></li>
<li>[2] <a href="http://roserouge.iteye.com/blog/1558498">hadoop cdh 安装笔记</a></li>
</ul>
</div>

      <!--
      <div id="pay" style="text-align:center;">
        ----EOF-----
        <br>
        <section>
  <h4>Sponsor</h4>
	<img src="http://javachen-rs.qiniudn.com/images/alipay.png" width="150/">
  <p class="small">觉得此博客对你有帮助，支付宝扫码赞助吧</p>
</section>

      </div>
      -->
      <p class="meta">
      	
            Categories:
      	    
          	<a class="btn btn-default btn-xs" href="/categories.html#hadoop">hadoop</a>
          
      	

      	
            Tags:
      	    
          	<a class="btn btn-default btn-xs" href="/tags.html#hadoop">hadoop</a>
          
          	<a class="btn btn-default btn-xs" href="/tags.html#hdfs">hdfs</a>
          
          	<a class="btn btn-default btn-xs" href="/tags.html#yarn">yarn</a>
          
          	<a class="btn btn-default btn-xs" href="/tags.html#hive">hive</a>
          
          	<a class="btn btn-default btn-xs" href="/tags.html#hbase">hbase</a>
          
      	
      </p>
	</article>

	<ul class="pager">
	  
	  <li class="previous"><a class="btn btn-xs" href="/2013/03/29/install-impala" title="安装Impala过程">&larr; Prev</a></li>
	  
	  
	  <li class="next"><a class="btn btn-xs" href="/2013/04/07/add-a-field-from-paramter-to-output" title="Kettle中添加一个参数字段到输出">Next &rarr;</a></li>
	  
	</ul>

  
<div id="comments">
  <div class="ds-thread" data-thread-key="/2013/04/06/install-cloudera-cdh-by-yum"  data-title="使用yum安装CDH Hadoop集群 - 夏天松的个人博客"></div>
</div>



</div>

        </div>
      </div>

      <footer class="footer text-center">
  <p>&copy; 2009-2015 <a href="/" target="_blank" title="86后，工作在深圳；Java程序员、Hadoop工程师；主要关注Java、Scala、Hadoop、Kettle、关注大数据处理技术。">夏天松</a>. With Help from <a href="//jekyllrb.com/" target="_blank" title="Transform your plain text into static websites and blogs.">Jekyll</a> and <a href="//getbootstrap.com/" target="_blank" title="Bootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.">Bootstrap</a>, theme from <a href="http://havee.me/" target="_blank" title="http://havee.me/">Havee</a>.

  <span style="float:right;"><a href="/about.html">About</a></span>

	
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




  </p>
  <div id="toTop" style="display: block;">
    <a href="#">▲</a><a href="#footer">▼</a>
  </div>
</footer>

    </div>

    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.min.js"></script>
    <script src="/static/js/core.js"></script>

    
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254098866'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1254098866' type='text/javascript'%3E%3C/script%3E"));</script>




    
    <!-- duoshuo Begin -->
    <script type="text/javascript">
      var duoshuoQuery = {short_name:"xiaotian120"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = 'http://static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] ||
        document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script>
    <!-- duoshuo End -->
    
  </body>
</html>
